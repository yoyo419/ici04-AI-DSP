"""
Legal Knowledge Graph Builder v4 for Occupational Safety and Health Laws
è·æ¥­å®‰å…¨è¡›ç”Ÿæ³•å¾‹çŸ¥è­˜åœ–è­œå»ºæ§‹å™¨ v4 (æ™ºæ…§èªæ„èšé¡èˆ‡èªå¢ƒæ„ŸçŸ¥ç‰ˆ)

æ ¸å¿ƒæ”¹é€² (åŸºæ–¼æ•™æˆçš„æ·±åº¦åˆ†æ):
1. HDBSCAN è‡ªé©æ‡‰èšé¡ - è§£æ±ºå–®ä¾‹èšé¡ç½é›£,è‡ªå‹•éæ¿¾å™ªè²
2. èªæ„è¦å‰‡åŒ¹é… - ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦å–ä»£é—œéµå­—åŒ¹é…
3. èªå¢ƒæ„ŸçŸ¥ç¾©å‹™èƒå– - è™•ç†æ³•å¾‹å¼•ç”¨(Anaphora)å•é¡Œ
4. çµ±ä¸€å„ªå…ˆç´šé‚è¼¯ - ä¿®æ­£äººæ©Ÿè¿´åœˆ(HITL)çš„å…§éƒ¨çŸ›ç›¾
"""

import json
import os
import sys
import getpass
from typing import Dict, List, Set, Tuple, Any, Optional
from collections import defaultdict
import numpy as np
from openai import OpenAI
import re
from dataclasses import dataclass, asdict, field
from enum import Enum
import ast
from datetime import datetime

# ============================================================================
# è³‡æ–™çµæ§‹å®šç¾© (ç¹¼æ‰¿ v3)
# ============================================================================

@dataclass
class GraphNode:
    """çŸ¥è­˜åœ–è­œç¯€é»"""
    id: str
    type: str
    properties: Dict

@dataclass
class GraphEdge:
    """çŸ¥è­˜åœ–è­œé‚Š"""
    source: str
    target: str
    type: str
    properties: Dict = None

class ControlType(Enum):
    """é¢¨éšªæ§åˆ¶å±¤ç´š"""
    ENGINEERING = "EngineeringControl"
    ADMINISTRATIVE = "AdministrativeControl"
    PPE = "PersonalProtectiveEquipment"
    ELIMINATION = "EliminationControl"
    SUBSTITUTION = "SubstitutionControl"

class ReviewStatus(Enum):
    """å¯©æ ¸ç‹€æ…‹"""
    AUTO_APPROVED = "auto_approved"
    PENDING_REVIEW = "pending_review"
    HUMAN_VERIFIED = "human_verified"
    REJECTED = "rejected"

class ReviewPriority(Enum):
    """å¯©æ ¸å„ªå…ˆç´š"""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"

@dataclass
class LegalEvent:
    """æ³•å¾‹äº‹ä»¶çµæ§‹"""
    event_id: str
    action: str
    actor: str
    patients: List[str] = field(default_factory=list)
    instruments: List[str] = field(default_factory=list)
    locations: List[str] = field(default_factory=list)
    conditions: List[str] = field(default_factory=list)
    temporal: Optional[str] = None
    purpose: Optional[str] = None
    source_article: str = ""
    confidence: float = 0.0

@dataclass
class RuleTemplate:
    """å¯è¨ˆç®—è¦å‰‡æ¨¡æ¿"""
    rule_id: str
    rule_name: str
    category: str
    pattern: str
    keywords: List[str] = field(default_factory=list)
    examples: List[str] = field(default_factory=list)
    control_type_affinity: Dict[str, float] = field(default_factory=dict)
    embedding_vector: Optional[np.ndarray] = None  # v4 æ–°å¢: è¦å‰‡çš„èªæ„å‘é‡

@dataclass
class StructuredEvidence:
    """çµæ§‹åŒ–è­‰æ“š"""
    keywords_matched: List[str] = field(default_factory=list)
    decision_rule_id: str = ""
    decision_rule_name: str = ""
    rule_similarity_score: float = 0.0  # v4 æ–°å¢: èˆ‡è¦å‰‡çš„ç›¸ä¼¼åº¦åˆ†æ•¸
    confidence_factors: Dict[str, float] = field(default_factory=dict)
    alternative_classifications: List[Dict[str, Any]] = field(default_factory=list)
    text_snippets: List[str] = field(default_factory=list)
    extracted_events: List[str] = field(default_factory=list)

@dataclass
class ClassificationResult:
    """åˆ†é¡çµæœ"""
    classification: str
    confidence: float
    evidence: StructuredEvidence
    review_status: ReviewStatus
    review_priority: ReviewPriority = ReviewPriority.LOW
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    human_feedback: Optional[str] = None

@dataclass
class ActiveLearningScore:
    """ä¸»å‹•å­¸ç¿’è©•åˆ†"""
    uncertainty_score: float = 0.0
    impact_score: float = 0.0
    frequency_score: float = 0.0
    complexity_score: float = 0.0
    total_priority: float = 0.0

@dataclass
class ClusterQualityMetrics:
    """èšé¡å“è³ªæŒ‡æ¨™"""
    silhouette_score: float = 0.0
    avg_intra_similarity: float = 0.0
    min_member_similarity: float = 0.0
    is_singleton: bool = False
    is_noise: bool = False  # v4 æ–°å¢: HDBSCAN å™ªè²æ¨™è¨˜
    needs_review: bool = False
    review_reason: str = ""
    review_priority: ReviewPriority = ReviewPriority.LOW
    active_learning_score: Optional[ActiveLearningScore] = None


class LegalKGBuilderV4:
    """æ³•å¾‹çŸ¥è­˜åœ–è­œå»ºæ§‹å™¨ v4 (æ™ºæ…§èªæ„èšé¡èˆ‡èªå¢ƒæ„ŸçŸ¥ç‰ˆ)"""
    
    def __init__(self, api_key: str, input_path: str, output_dir: str = "./output"):
        """
        åˆå§‹åŒ–å»ºæ§‹å™¨
        
        Args:
            api_key: OpenAI API Key
            input_path: all_documents.json çš„è·¯å¾‘
            output_dir: è¼¸å‡ºç›®éŒ„
        """
        self.client = OpenAI(api_key=api_key)
        self.input_path = input_path
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        # äººæ©Ÿè¿´åœˆä½‡åˆ—
        self.review_queue_dir = os.path.join(output_dir, "review_queue")
        os.makedirs(self.review_queue_dir, exist_ok=True)
        
        # è³‡æ–™çµæ§‹
        self.documents = []
        self.nodes = []
        self.edges = []
        
        # æœ¬é«”èˆ‡æ­£è¦åŒ–
        self.subject_ontology = {}
        self.object_ontology = {}
        self.obligation_clusters = {}
        self.control_type_mapping = {}
        self.ontology_embeddings = {} # v5 æœ¬é«”å¯¦é«” embeddings
        
        # äº‹ä»¶æŠ½å–ç›¸é—œ
        self.legal_events = {}
        self.discovered_entities = set()
        
        # === v4 æ–°å¢: å¯è¨ˆç®—è¦å‰‡åº« (å«èªæ„å‘é‡) ===
        self.rule_base = {}
        self.rule_embeddings = {}  # rule_id -> embedding vector
        self._initialize_rule_base()
        
        # å¯©æ ¸ä½‡åˆ—
        self.low_confidence_classifications = []
        self.problematic_clusters = []
        
        # å“è³ªé–¾å€¼
        self.CONFIDENCE_THRESHOLD = 0.75
        self.MIN_CLUSTER_SIZE = 2
        
        # === v4 æ–°å¢: HDBSCAN åƒæ•¸ ===
        self.HDBSCAN_MIN_CLUSTER_SIZE = 2
        self.HDBSCAN_MIN_SAMPLES = 1
        self.HDBSCAN_METRIC = 'euclidean'
        
        # ä¸»å‹•å­¸ç¿’åƒæ•¸
        self.PRIORITY_WEIGHTS = {
            'uncertainty': 0.35,
            'impact': 0.30,
            'frequency': 0.20,
            'complexity': 0.15
        }
        
        # å¿«å–
        self.embedding_cache = {}
        self.classification_cache = {}
        
        # åˆå§‹åŒ–åŸºç¤æœ¬é«”çµæ§‹
        self._initialize_base_ontology()
        
    # ========================================================================
    # v4 æ ¸å¿ƒæ”¹é€² 1: èªæ„è¦å‰‡åŒ¹é…ç³»çµ±
    # ========================================================================
    
    def _initialize_rule_base(self):
        """
        åˆå§‹åŒ–å¯è¨ˆç®—è¦å‰‡åº« (v4 å¢å¼·ç‰ˆ)
        ç‚ºæ¯æ¢è¦å‰‡ç”Ÿæˆèªæ„å‘é‡,å¯¦ç¾çœŸæ­£çš„ã€Œèªæ„åŒ¹é…ã€
        """
        # è¦å‰‡å®šç¾© (ç¹¼æ‰¿è‡ª v3)
        self.rule_base = {
            # ç®¡ç†æ§åˆ¶è¦å‰‡
            "RULE_ADM_01": RuleTemplate(
                rule_id="RULE_ADM_01",
                rule_name="æ™‚é–“ç®¡ç†è¦å‰‡",
                category="AdministrativeControl",
                pattern="æ¶‰åŠä½œæ¥­æ™‚é–“ã€ä¼‘æ¯æ™‚é–“ã€è¼ªç­åˆ¶åº¦ç­‰æ™‚é–“ç®¡ç†æªæ–½",
                keywords=["æ™‚é–“", "ä¼‘æ¯", "è¼ªç­", "å·¥æ™‚", "æ¯æ—¥", "å®šæœŸ"],
                examples=["æ¯æ—¥ä½œæ¥­æ™‚é–“ä¸å¾—è¶…é6å°æ™‚", "æ¯å·¥ä½œ2å°æ™‚æ‡‰ä¼‘æ¯30åˆ†é˜"],
                control_type_affinity={"AdministrativeControl": 0.9, "EngineeringControl": 0.1}
            ),
            "RULE_ADM_02": RuleTemplate(
                rule_id="RULE_ADM_02",
                rule_name="æª¢æŸ¥èˆ‡ç›£æ¸¬è¦å‰‡",
                category="AdministrativeControl",
                pattern="æ¶‰åŠå®šæœŸæª¢æŸ¥ã€ç›£æ¸¬ã€è¨˜éŒ„ã€å ±å‘Šç­‰ç®¡ç†ç¨‹åº",
                keywords=["æª¢æŸ¥", "ç›£æ¸¬", "æ¸¬å®š", "è¨˜éŒ„", "å ±å‘Š", "å®šæœŸ"],
                examples=["æ‡‰æ¯æœˆå¯¦æ–½å®šæœŸæª¢æŸ¥", "æ‡‰è¨˜éŒ„ä¸¦ä¿å­˜æª¢æ¸¬çµæœ"],
                control_type_affinity={"AdministrativeControl": 0.95, "EngineeringControl": 0.05}
            ),
            "RULE_ADM_03": RuleTemplate(
                rule_id="RULE_ADM_03",
                rule_name="æ•™è‚²è¨“ç·´è¦å‰‡",
                category="AdministrativeControl",
                pattern="æ¶‰åŠæ•™è‚²ã€è¨“ç·´ã€å®£å°ã€æŒ‡å°ç­‰èƒ½åŠ›å»ºæ§‹æªæ–½",
                keywords=["æ•™è‚²", "è¨“ç·´", "æŒ‡å°", "å®£å°", "è¬›ç¿’"],
                examples=["æ‡‰å¯¦æ–½å®‰å…¨è¡›ç”Ÿæ•™è‚²è¨“ç·´", "æ‡‰æŒ‡å°å‹å·¥æ­£ç¢ºä½œæ¥­æ–¹æ³•"],
                control_type_affinity={"AdministrativeControl": 0.95, "PPE": 0.05}
            ),
            "RULE_ADM_04": RuleTemplate(
                rule_id="RULE_ADM_04",
                rule_name="æ¨™ç¤ºèˆ‡è­¦å‘Šè¦å‰‡",
                category="AdministrativeControl",
                pattern="æ¶‰åŠæ¨™ç¤ºã€è­¦å‘Šæ¨™èªŒã€å…¬å‘Šã€é€šçŸ¥ç­‰è³‡è¨Šå‚³é”æªæ–½",
                keywords=["æ¨™ç¤º", "è­¦å‘Š", "å…¬å‘Š", "æ¨™èªŒ", "æ­ç¤º"],
                examples=["æ‡‰æ–¼æ˜é¡¯è™•æ¨™ç¤ºè­¦å‘Šæ¨™èªŒ", "æ‡‰å…¬å‘Šä½œæ¥­æ³¨æ„äº‹é …"],
                control_type_affinity={"AdministrativeControl": 0.85, "EngineeringControl": 0.15}
            ),
            
            # å·¥ç¨‹æ§åˆ¶è¦å‰‡
            "RULE_ENG_01": RuleTemplate(
                rule_id="RULE_ENG_01",
                rule_name="ç‰©ç†å±éšœè¦å‰‡",
                category="EngineeringControl",
                pattern="æ¶‰åŠè­·ç½©ã€è­·æ¬„ã€åœæ¬„ã€é®è”½ç­‰ç‰©ç†æ€§é˜»éš”è¨­æ–½",
                keywords=["è­·ç½©", "è­·æ¬„", "åœæ¬„", "æ¬„æ†", "é®è”½", "é˜»éš”"],
                examples=["æ‡‰è¨­ç½®è­·æ¬„é˜²æ­¢å¢œè½", "æ‡‰è£è¨­è­·ç½©é˜²æ­¢æ¥è§¸"],
                control_type_affinity={"EngineeringControl": 0.95, "AdministrativeControl": 0.05}
            ),
            "RULE_ENG_02": RuleTemplate(
                rule_id="RULE_ENG_02",
                rule_name="é€šé¢¨èˆ‡æ›æ°£è¦å‰‡",
                category="EngineeringControl",
                pattern="æ¶‰åŠé€šé¢¨ã€æ›æ°£ã€æ’æ°£ã€æŠ½é¢¨ç­‰ç©ºæ°£å“è³ªæ§åˆ¶è¨­å‚™",
                keywords=["é€šé¢¨", "æ›æ°£", "æ’æ°£", "æŠ½é¢¨", "å±€éƒ¨æ’æ°£"],
                examples=["æ‡‰è¨­ç½®é©ç•¶ä¹‹é€šé¢¨è¨­å‚™", "æ‡‰è¨­å±€éƒ¨æ’æ°£è£ç½®"],
                control_type_affinity={"EngineeringControl": 0.95, "AdministrativeControl": 0.05}
            ),
            "RULE_ENG_03": RuleTemplate(
                rule_id="RULE_ENG_03",
                rule_name="å®‰å…¨è£ç½®è¦å‰‡",
                category="EngineeringControl",
                pattern="æ¶‰åŠå®‰å…¨è£ç½®ã€é€£é–è£ç½®ã€ç·Šæ€¥åœæ­¢è£ç½®ã€æ¥µé™é–‹é—œç­‰å®‰å…¨æ©Ÿæ§‹",
                keywords=["å®‰å…¨è£ç½®", "é€£é–", "ç·Šæ€¥åœæ­¢", "é˜²è­·è£ç½®", "ä¿è­·è£ç½®", "æ¥µé™é–‹é—œ", "é™åˆ¶é–‹é—œ"],
                examples=["æ‡‰è¨­ç½®ç·Šæ€¥åœæ­¢è£ç½®", "æ‡‰å…·å‚™é€£é–ä¿è­·æ©Ÿæ§‹", "æ‡‰è¨­ç½®çµ‚é»æ¥µé™é–‹é—œ"],
                control_type_affinity={"EngineeringControl": 0.9, "AdministrativeControl": 0.1}
            ),
            
            # å€‹äººé˜²è­·å…·è¦å‰‡
            "RULE_PPE_01": RuleTemplate(
                rule_id="RULE_PPE_01",
                rule_name="é ­éƒ¨é˜²è­·è¦å‰‡",
                category="PPE",
                pattern="æ¶‰åŠå®‰å…¨å¸½ã€é ­éƒ¨é˜²è­·ç­‰å€‹äººé ­éƒ¨ä¿è­·è£å‚™",
                keywords=["å®‰å…¨å¸½", "å·¥å®‰å¸½", "é ­éƒ¨", "é˜²è­·å¸½"],
                examples=["æ‡‰ä½¿å‹å·¥æˆ´ç”¨å®‰å…¨å¸½", "æ‡‰é…æˆ´ç¬¦åˆæ¨™æº–ä¹‹å®‰å…¨å¸½"],
                control_type_affinity={"PPE": 0.95, "AdministrativeControl": 0.05}
            ),
            "RULE_PPE_02": RuleTemplate(
                rule_id="RULE_PPE_02",
                rule_name="å¢œè½é˜²è­·è¦å‰‡",
                category="PPE",
                pattern="æ¶‰åŠå®‰å…¨å¸¶ã€å®‰å…¨ç´¢ã€é˜²å¢œå™¨ç­‰é˜²æ­¢å¢œè½ä¹‹å€‹äººè£å‚™",
                keywords=["å®‰å…¨å¸¶", "å®‰å…¨ç´¢", "é˜²å¢œ", "å®‰å…¨æ¯ç´¢"],
                examples=["æ‡‰ä½¿å‹å·¥ä½¿ç”¨å®‰å…¨å¸¶", "æ‡‰é…æ›å®‰å…¨å¸¶æ–¼å®‰å…¨æ¯ç´¢"],
                control_type_affinity={"PPE": 0.95, "EngineeringControl": 0.05}
            ),
            "RULE_PPE_03": RuleTemplate(
                rule_id="RULE_PPE_03",
                rule_name="å‘¼å¸é˜²è­·è¦å‰‡",
                category="PPE",
                pattern="æ¶‰åŠé˜²è­·å£ç½©ã€å‘¼å¸é˜²è­·å…·ç­‰å‘¼å¸ç³»çµ±ä¿è­·è£å‚™",
                keywords=["å£ç½©", "å‘¼å¸é˜²è­·", "é˜²å¡µå£ç½©", "é˜²æ¯’é¢å…·"],
                examples=["æ‡‰ä½¿å‹å·¥ä½©æˆ´é˜²è­·å£ç½©", "æ‡‰æä¾›é©ç•¶ä¹‹å‘¼å¸é˜²è­·å…·"],
                control_type_affinity={"PPE": 0.9, "EngineeringControl": 0.1}
            ),
            
            # æ¶ˆé™¤æ§åˆ¶è¦å‰‡
            "RULE_ELIM_01": RuleTemplate(
                rule_id="RULE_ELIM_01",
                rule_name="å±å®³æºæ¶ˆé™¤è¦å‰‡",
                category="EliminationControl",
                pattern="æ¶‰åŠå®Œå…¨ç§»é™¤ã€åœæ­¢ä½¿ç”¨ã€å»¢é™¤ç­‰æ¶ˆé™¤å±å®³æºçš„æªæ–½",
                keywords=["ç¦æ­¢", "ä¸å¾—ä½¿ç”¨", "åœæ­¢", "ç§»é™¤", "å»¢é™¤"],
                examples=["ç¦æ­¢ä½¿ç”¨å«çŸ³ç¶¿ææ–™", "ä¸å¾—ä½¿ç”¨è©²æœ‰å®³ç‰©è³ª"],
                control_type_affinity={"EliminationControl": 0.95, "SubstitutionControl": 0.05}
            ),
            
            # æ›¿ä»£æ§åˆ¶è¦å‰‡
            "RULE_SUB_01": RuleTemplate(
                rule_id="RULE_SUB_01",
                rule_name="ææ–™æ›¿ä»£è¦å‰‡",
                category="SubstitutionControl",
                pattern="æ¶‰åŠä½¿ç”¨è¼ƒå®‰å…¨ææ–™ã€æ›¿ä»£å“ã€ä½å±å®³ç‰©è³ªç­‰æ›¿ä»£æªæ–½",
                keywords=["æ›¿ä»£", "æ”¹ç”¨", "ä½¿ç”¨å…¶ä»–", "è¼ƒä½å±å®³", "å®‰å…¨æ›¿ä»£å“"],
                examples=["æ‡‰æ”¹ç”¨ä½æ¯’æ€§æº¶åŠ‘", "æ‡‰ä»¥è¼ƒå®‰å…¨ä¹‹ææ–™æ›¿ä»£"],
                control_type_affinity={"SubstitutionControl": 0.9, "EliminationControl": 0.1}
            )
        }
        
        print(f"  âœ“ å·²åˆå§‹åŒ– {len(self.rule_base)} å€‹å¯è¨ˆç®—è¦å‰‡")
        
        # === v4 æ–°å¢: ç‚ºæ¯æ¢è¦å‰‡ç”Ÿæˆèªæ„å‘é‡ ===
        print("  â†’ ç‚ºè¦å‰‡ç”Ÿæˆèªæ„å‘é‡...")
        self._generate_rule_embeddings()
    
    def _generate_rule_embeddings(self):
        """
        v4 æ ¸å¿ƒæ–¹æ³•: ç‚ºæ¯æ¢è¦å‰‡ç”Ÿæˆèªæ„å‘é‡
        é€™æ˜¯å¯¦ç¾ã€Œèªæ„è¦å‰‡åŒ¹é…ã€çš„åŸºç¤
        """
        for rule_id, rule_template in self.rule_base.items():
            # çµ„åˆè¦å‰‡çš„å¤šå€‹èªæ„ä¾†æº
            rule_description = f"""
è¦å‰‡åç¨±: {rule_template.rule_name}
è¦å‰‡æ¨¡å¼: {rule_template.pattern}
é—œéµè©: {', '.join(rule_template.keywords)}
ç¯„ä¾‹: {' | '.join(rule_template.examples)}
            """.strip()
            
            # ç”Ÿæˆå‘é‡
            try:
                response = self.client.embeddings.create(
                    model="text-embedding-3-small",
                    input=[rule_description]
                )
                embedding = response.data[0].embedding
                self.rule_embeddings[rule_id] = np.array(embedding)
                
                # åŒæ™‚å„²å­˜åˆ° RuleTemplate ç‰©ä»¶ä¸­
                rule_template.embedding_vector = np.array(embedding)
                
            except Exception as e:
                print(f"    âœ— è¦å‰‡ {rule_id} å‘é‡ç”Ÿæˆå¤±æ•—: {e}")
                self.rule_embeddings[rule_id] = np.zeros(1536)
        
        print(f"  âœ“ å·²ç”Ÿæˆ {len(self.rule_embeddings)} å€‹è¦å‰‡å‘é‡")
    
    def _match_rules_with_semantic_similarity(self, 
                                              obligation_text: str,
                                              obligation_vector: Optional[np.ndarray] = None,
                                              top_k: int = 3,
                                              threshold: float = 0.6) -> List[Tuple[str, float]]:
        """
        v4 æ ¸å¿ƒæ–¹æ³•: ä½¿ç”¨èªæ„ç›¸ä¼¼åº¦åŒ¹é…è¦å‰‡
        å–ä»£ v3 çš„é—œéµå­—åŒ¹é…,é€™æ˜¯è§£æ±ºã€Œå•é¡ŒäºŒã€çš„é—œéµ
        
        Args:
            obligation_text: ç¾©å‹™æ–‡æœ¬
            obligation_vector: ç¾©å‹™çš„ embedding å‘é‡(å¯é¸,è‹¥ç„¡å‰‡ç¾å ´ç”Ÿæˆ)
            top_k: è¿”å›å‰ k å€‹æœ€ç›¸ä¼¼çš„è¦å‰‡
            threshold: æœ€ä½ç›¸ä¼¼åº¦é–¾å€¼
            
        Returns:
            [(rule_id, similarity_score), ...] æŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—
        """
        # å¦‚æœæ²’æœ‰æä¾›å‘é‡,å‰‡ç”Ÿæˆ
        if obligation_vector is None:
            try:
                response = self.client.embeddings.create(
                    model="text-embedding-3-small",
                    input=[obligation_text]
                )
                obligation_vector = np.array(response.data[0].embedding)
            except Exception as e:
                print(f"    âœ— ç¾©å‹™å‘é‡ç”Ÿæˆå¤±æ•—: {e}")
                return []
        
        # è¨ˆç®—èˆ‡æ‰€æœ‰è¦å‰‡çš„é¤˜å¼¦ç›¸ä¼¼åº¦
        similarities = []
        for rule_id, rule_vector in self.rule_embeddings.items():
            # é¤˜å¼¦ç›¸ä¼¼åº¦å…¬å¼: cos(Î¸) = (AÂ·B) / (||A|| Ã— ||B||)
            cos_sim = np.dot(obligation_vector, rule_vector) / (
                np.linalg.norm(obligation_vector) * np.linalg.norm(rule_vector)
            )
            
            if cos_sim >= threshold:
                similarities.append((rule_id, float(cos_sim)))
        
        # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        return similarities[:top_k]
    
    # ========================================================================
    # v4 æ ¸å¿ƒæ”¹é€² 2: èªå¢ƒæ„ŸçŸ¥ç¾©å‹™èƒå–
    # ========================================================================
    
    def _extract_obligations_with_context(self) -> List[Dict[str, Any]]:
        """
        v4 æ ¸å¿ƒæ–¹æ³•: èªå¢ƒæ„ŸçŸ¥ç¾©å‹™èƒå–
        è§£æ±ºã€Œå•é¡Œä¸‰ã€- è™•ç†æ³•å¾‹å¼•ç”¨(Anaphora)å•é¡Œ
        
        Returns:
            List of dicts with keys: 'text', 'context', 'category', 'source', 'has_anaphora'
        """
        print("  â†’ åŸ·è¡Œèªå¢ƒæ„ŸçŸ¥ç¾©å‹™èƒå–...")
        
        obligations = []
        
        # æ³•å¾‹å¼•ç”¨è©(Anaphora)çš„æ­£å‰‡è¡¨é”å¼
        anaphora_pattern = re.compile(r'(å‰é …|å‰æ¬¾|å‰æ¢|ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+æ¬¾|ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+é …|æœ¬æ¢|è©²)')
        
        # ç¾©å‹™é—œéµè©
        obligation_patterns = [
            (r'é›‡ä¸».*?æ‡‰.*?[ã€‚\n]', 'é›‡ä¸»ç¾©å‹™'),
            (r'äº‹æ¥­å–®ä½.*?æ‡‰.*?[ã€‚\n]', 'äº‹æ¥­å–®ä½ç¾©å‹™'),
            (r'å‹å·¥.*?æ‡‰.*?[ã€‚\n]', 'å‹å·¥ç¾©å‹™'),
            (r'æ‡‰(?:è¨­ç½®|è£è¨­|é…ç½®|é…å‚™|è¨­ç«‹).*?(?:è£ç½®|è¨­å‚™|è¨­æ–½|æªæ–½).*?[ã€‚\n]', 'è¨­ç½®é¡ç¾©å‹™'),
            (r'æ‡‰(?:å¯¦æ–½|è¾¦ç†|é€²è¡Œ|åŸ·è¡Œ).*?(?:æª¢æŸ¥|æ¸¬å®š|ç›£æ¸¬|è©•ä¼°).*?[ã€‚\n]', 'æª¢æŸ¥é¡ç¾©å‹™'),
            (r'æ‡‰(?:è¨‚å®š|è£½ä½œ|å»ºç«‹|æ“¬å®š).*?(?:è¨ˆç•«|æ¨™æº–|ç¨‹åº|è¦å®š).*?[ã€‚\n]', 'æ–‡ä»¶é¡ç¾©å‹™'),
            (r'æ‡‰(?:ä½¿|ä»¤|å‘½|è¦æ±‚).*?å‹å·¥.*?(?:ä½¿ç”¨|ä½©æˆ´|é…æˆ´|ç©¿æˆ´).*?[ã€‚\n]', 'é˜²è­·å…·ç¾©å‹™'),
            (r'æ‡‰.*?æ•™è‚²è¨“ç·´.*?[ã€‚\n]', 'è¨“ç·´ç¾©å‹™'),
            (r'æ‡‰.*?æ¨™ç¤º.*?[ã€‚\n]', 'æ¨™ç¤ºç¾©å‹™'),
            (r'(?:é›‡ä¸»|äº‹æ¥­å–®ä½|å‹å·¥).*?ä¸å¾—.*?[ã€‚\n]', 'ç¦æ­¢ç¾©å‹™'),
        ]
        
        for doc_idx, doc in enumerate(self.documents):
            content = doc.get('content', '')
            metadata = doc.get('metadata', {})
            
            # å°‡å…§å®¹æŒ‰å¥å­åˆ†å‰²(ä¿ç•™åŸå§‹é †åº)
            sentences = re.split(r'([ã€‚\n])', content)
            sentences = [''.join(sentences[i:i+2]) for i in range(0, len(sentences)-1, 2)]
            
            for sent_idx, sentence in enumerate(sentences):
                sentence = sentence.strip()
                if len(sentence) < 10:
                    continue
                
                # æª¢æŸ¥æ˜¯å¦åŒ…å«ç¾©å‹™é—œéµè©
                matched_category = None
                for pattern, category in obligation_patterns:
                    if re.search(pattern, sentence):
                        matched_category = category
                        break
                
                if not matched_category:
                    continue
                
                # === æ ¸å¿ƒé‚è¼¯: æª¢æ¸¬æ³•å¾‹å¼•ç”¨ ===
                has_anaphora = bool(anaphora_pattern.search(sentence))
                
                context = ""
                if has_anaphora:
                    # å¦‚æœå­˜åœ¨å¼•ç”¨,å‰‡å‘å‰æŸ¥æ‰¾èªå¢ƒ
                    context_window = 2  # å‘å‰æŸ¥æ‰¾2å¥
                    start_idx = max(0, sent_idx - context_window)
                    context_sentences = sentences[start_idx:sent_idx]
                    context = ' '.join(s.strip() for s in context_sentences if s.strip())
                
                obligations.append({
                    'text': sentence,
                    'context': context,
                    'category': matched_category,
                    'source': metadata.get('chunk_id', f'doc_{doc_idx}'),
                    'has_anaphora': has_anaphora,
                    'full_text_with_context': f"[èªå¢ƒ: {context}] {sentence}" if context else sentence
                })
        
        # å»é‡(åŸºæ–¼ full_text_with_context)
        unique_obligations = {}
        for obl in obligations:
            key = obl['full_text_with_context']
            if key not in unique_obligations:
                unique_obligations[key] = obl
        
        result = list(unique_obligations.values())
        
        anaphora_count = sum(1 for o in result if o['has_anaphora'])
        print(f"  âœ“ èƒå–äº† {len(result)} å€‹ç¾©å‹™æè¿°")
        print(f"    â†’ å…¶ä¸­ {anaphora_count} å€‹åŒ…å«æ³•å¾‹å¼•ç”¨,å·²è£œå……èªå¢ƒ")
        
        return result
    
    # ========================================================================
    # v4 æ ¸å¿ƒæ”¹é€² 3: HDBSCAN è‡ªé©æ‡‰èšé¡
    # ========================================================================
    
    def _cluster_obligations_with_hdbscan(self, 
                                        obligations: List[Dict[str, Any]], 
                                        vectors: np.ndarray) -> Dict[int, List[Dict]]:
        """
        v5 æ ¸å¿ƒæ”¹é€²: ä½¿ç”¨è‘‰èšé¡
        è§£æ±ºå•é¡Œä¸€ - ä¿ç•™ç²¾ç¢ºã€å°å‹çš„æœ‰æ„ç¾©èšé¡
        
        é—œéµä¿®æ”¹:
        1. cluster_selection_method='leaf' (v4 ç‚º 'eom')
        2. min_cluster_size=3 (v4 ç‚º 2)
        3. æ–°å¢å°å‹èšé¡çµ±è¨ˆ
        """
        if len(obligations) == 0 or vectors.size == 0:
            print("    âœ— ç¾©å‹™åˆ—è¡¨æˆ–å‘é‡ç‚ºç©º")
            return {}
        
        if vectors.shape[0] != len(obligations):
            print(f"    âœ— å‘é‡æ•¸é‡ä¸ä¸€è‡´")
            return {}
        
        try:
            import hdbscan
            print(f"  â†’ ä½¿ç”¨ HDBSCAN è‘‰èšé¡ (min_cluster_size={self.HDBSCAN_MIN_CLUSTER_SIZE}, method='leaf')...")
            
            # âš ï¸ v5 é—œéµä¿®æ”¹
            clusterer = hdbscan.HDBSCAN(
                min_cluster_size=3,  # v5: å¾ 2 æé«˜åˆ° 3
                min_samples=1,
                metric='euclidean',
                cluster_selection_method='leaf'  # v5: å¾ 'eom' æ”¹ç‚º 'leaf'
            )
            
            cluster_labels = clusterer.fit_predict(vectors)
            
            clusters = defaultdict(list)
            for idx, label in enumerate(cluster_labels):
                clusters[int(label)].append(obligations[idx])
            
            noise_points = clusters.pop(-1, [])
            
            print(f"  âœ“ HDBSCAN è‘‰èšé¡å®Œæˆ:")
            print(f"    - æœ‰æ•ˆèšé¡: {len(clusters)} å€‹")
            print(f"    - å™ªè²é»: {len(noise_points)} å€‹")
            
            if clusters:
                cluster_sizes = [len(v) for v in clusters.values()]
                print(f"    - èšé¡å¤§å°: æœ€å°={min(cluster_sizes)}, æœ€å¤§={max(cluster_sizes)}, å¹³å‡={sum(cluster_sizes)/len(cluster_sizes):.1f}")
                
                # âš ï¸ v5 æ–°å¢çµ±è¨ˆ
                small_clusters = sum(1 for size in cluster_sizes if 3 <= size <= 5)
                print(f"    - v5 æ”¹é€²: ä¿ç•™äº† {small_clusters} å€‹å°å‹ç²¾ç¢ºèšé¡ (3-5 æˆå“¡)")
            
            if noise_points:
                self._save_noise_points(noise_points)
            
            return dict(clusters)
            
        except ImportError:
            print("    âœ— æœªå®‰è£ hdbscan")
            return self._cluster_obligations_fallback(obligations, vectors)
        except Exception as e:
            print(f"    âœ— HDBSCAN å¤±æ•—: {e}")
            return self._cluster_obligations_fallback(obligations, vectors)
    
    def _cluster_obligations_fallback(self, 
                                          obligations: List[Dict[str, Any]], 
                                          vectors: np.ndarray,
                                          threshold: float = 0.85) -> Dict[int, List[Dict]]:
        """
        å›é€€æ–¹æ³•: ä½¿ç”¨ v3 çš„ç›¸ä¼¼åº¦é–¾å€¼èšé¡
        ç•¶ HDBSCAN ä¸å¯ç”¨æ™‚ä½¿ç”¨
        """
        from sklearn.metrics.pairwise import cosine_similarity
        
        similarity_matrix = cosine_similarity(vectors)
        
        clusters = {}
        visited = set()
        cluster_id = 0
        
        for i in range(len(obligations)):
            if i in visited:
                continue
            
            similar_indices = np.where(similarity_matrix[i] >= threshold)[0]
            
            if len(similar_indices) > 0:
                clusters[cluster_id] = [obligations[j] for j in similar_indices]
                visited.update(similar_indices)
                cluster_id += 1
        
        # è™•ç†æœªåˆ†é¡é …
        unclustered = set(range(len(obligations))) - visited
        for i in unclustered:
            clusters[cluster_id] = [obligations[i]]
            cluster_id += 1
        
        print(f"    âœ“ å›é€€èšé¡å®Œæˆ: {len(clusters)} å€‹ç¾¤çµ„")
        return clusters
    
    def _save_noise_points(self, noise_points: List[Dict[str, Any]]):
        """å„²å­˜ HDBSCAN è­˜åˆ¥çš„å™ªè²é»ä¾›äººå·¥å¯©æ ¸"""
        noise_file = os.path.join(self.review_queue_dir, "noise_points.json")
        
        noise_data = {
            "generated_at": datetime.now().isoformat(),
            "total_noise_points": len(noise_points),
            "description": "é€™äº›ç¾©å‹™è¢« HDBSCAN è­˜åˆ¥ç‚ºå™ªè²é»,é€šå¸¸æ˜¯å› ç‚ºèªæ„éæ–¼ç¨ç‰¹æˆ–åŒ…å«ç„¡æ„ç¾©çš„æ³•å¾‹å¼•ç”¨",
            "review_instructions": "è«‹å¯©æ ¸é€™äº›é …ç›®,åˆ¤æ–·æ˜¯å¦éœ€è¦è£œå……èªå¢ƒæˆ–é‡æ–°åˆ†é¡",
            "noise_points": [
                {
                    "text": np["text"],
                    "context": np["context"],
                    "has_anaphora": np["has_anaphora"],
                    "source": np["source"]
                }
                for np in noise_points
            ]
        }
        
        with open(noise_file, 'w', encoding='utf-8') as f:
            json.dump(noise_data, f, ensure_ascii=False, indent=2)
        
        print(f"    â†’ å™ªè²é»å·²å„²å­˜è‡³: {noise_file}")
    
    # ========================================================================
    # v4 æ ¸å¿ƒæ”¹é€² 4: çµ±ä¸€å„ªå…ˆç´šé‚è¼¯
    # ========================================================================
    
    def _calculate_unified_priority(self, 
                                    cluster_info: Dict,
                                    classification_result: Optional[ClassificationResult] = None) -> ReviewPriority:
        """
        v4 æ ¸å¿ƒæ–¹æ³•: çµ±ä¸€çš„å„ªå…ˆç´šè¨ˆç®—é‚è¼¯
        è§£æ±ºã€Œå•é¡Œå››ã€- HITL å…§éƒ¨çŸ›ç›¾
        
        é€™å€‹æ–¹æ³•æ•´åˆäº†èšé¡å“è³ªã€åˆ†é¡ä¿¡å¿ƒåº¦ã€ä¸»å‹•å­¸ç¿’åˆ†æ•¸ç­‰å¤šå€‹ç¶­åº¦
        """
        # è¨ˆç®—ä¸»å‹•å­¸ç¿’åˆ†æ•¸
        if classification_result:
            al_score = self._calculate_active_learning_score(
                cluster_info=cluster_info,
                classification_result=classification_result
            )
        else:
            # åƒ…åŸºæ–¼èšé¡å“è³ªè¨ˆç®—
            al_score = self._calculate_active_learning_score_from_cluster(cluster_info)
        
        total_priority = al_score.total_priority
        
        # é¡å¤–çš„è¦å‰‡æª¢æŸ¥
        quality_metrics = cluster_info.get('quality_metrics', {})
        is_singleton = quality_metrics.get('is_singleton', False)
        is_noise = quality_metrics.get('is_noise', False)
        member_count = cluster_info.get('member_count', 1)
        confidence = cluster_info.get('overall_confidence', 1.0)
        
        # === çµ±ä¸€çš„å„ªå…ˆç´šæ±ºç­–é‚è¼¯ ===
        
        # è¦å‰‡1: å™ªè²é» -> è‡ªå‹•é™ä½å„ªå…ˆç´š(é™¤éä¿¡å¿ƒåº¦æ¥µä½)
        if is_noise:
            if confidence < 0.5:
                return ReviewPriority.HIGH
            else:
                return ReviewPriority.LOW
        
        # è¦å‰‡2: å–®ä¾‹èšé¡ + ä½ä¿¡å¿ƒåº¦ -> CRITICAL
        if is_singleton and confidence < 0.6:
            return ReviewPriority.CRITICAL
        
        # è¦å‰‡3: é«˜å½±éŸ¿ç¯„åœ(æˆå“¡å¤š) + ä½ä¿¡å¿ƒåº¦ -> CRITICAL
        if member_count >= 10 and confidence < 0.7:
            return ReviewPriority.CRITICAL
        
        # è¦å‰‡4: åŸºæ–¼ä¸»å‹•å­¸ç¿’ç¸½åˆ†
        if total_priority >= 0.75:
            return ReviewPriority.CRITICAL
        elif total_priority >= 0.55:
            return ReviewPriority.HIGH
        elif total_priority >= 0.35:
            return ReviewPriority.MEDIUM
        else:
            return ReviewPriority.LOW
    
    def _calculate_active_learning_score(self,
                                         cluster_info: Dict,
                                         classification_result: ClassificationResult) -> ActiveLearningScore:
        """è¨ˆç®—å®Œæ•´çš„ä¸»å‹•å­¸ç¿’åˆ†æ•¸(å«åˆ†é¡çµæœ)"""
        confidence = classification_result.confidence
        uncertainty_score = 1.0 - confidence
        
        alternatives = classification_result.evidence.alternative_classifications
        if alternatives and len(alternatives) > 0:
            top_alt_conf = alternatives[0].get('confidence', 0)
            if abs(confidence - top_alt_conf) < 0.15:
                uncertainty_score += 0.2
        
        uncertainty_score = min(uncertainty_score, 1.0)
        
        member_count = cluster_info.get('member_count', 1)
        impact_score = min(member_count / 50.0, 1.0)
        frequency_score = min(member_count / 30.0, 1.0)
        
        quality_metrics = cluster_info.get('quality_metrics', {})
        is_singleton = quality_metrics.get('is_singleton', False)
        avg_similarity = quality_metrics.get('avg_intra_similarity', 1.0)
        min_similarity = quality_metrics.get('min_member_similarity', 1.0)
        
        complexity_score = 0.0
        if is_singleton:
            complexity_score += 0.5
        
        complexity_score += (1.0 - avg_similarity) * 0.3
        complexity_score += (1.0 - min_similarity) * 0.2
        complexity_score = min(complexity_score, 1.0)
        
        total_priority = (
            uncertainty_score * self.PRIORITY_WEIGHTS['uncertainty'] +
            impact_score * self.PRIORITY_WEIGHTS['impact'] +
            frequency_score * self.PRIORITY_WEIGHTS['frequency'] +
            complexity_score * self.PRIORITY_WEIGHTS['complexity']
        )
        
        return ActiveLearningScore(
            uncertainty_score=uncertainty_score,
            impact_score=impact_score,
            frequency_score=frequency_score,
            complexity_score=complexity_score,
            total_priority=total_priority
        )
    
    def _calculate_active_learning_score_from_cluster(self, cluster_info: Dict) -> ActiveLearningScore:
        """åƒ…åŸºæ–¼èšé¡å“è³ªè¨ˆç®—ä¸»å‹•å­¸ç¿’åˆ†æ•¸(ç„¡åˆ†é¡çµæœæ™‚ä½¿ç”¨)"""
        quality_metrics = cluster_info.get('quality_metrics', {})
        avg_similarity = quality_metrics.get('avg_intra_similarity', 1.0)
        
        # ä½¿ç”¨å¹³å‡ç›¸ä¼¼åº¦ä½œç‚ºä»£ç†ä¿¡å¿ƒåº¦
        uncertainty_score = 1.0 - avg_similarity
        
        member_count = cluster_info.get('member_count', 1)
        impact_score = min(member_count / 50.0, 1.0)
        frequency_score = min(member_count / 30.0, 1.0)
        
        is_singleton = quality_metrics.get('is_singleton', False)
        min_similarity = quality_metrics.get('min_member_similarity', 1.0)
        
        complexity_score = 0.0
        if is_singleton:
            complexity_score += 0.5
        complexity_score += (1.0 - avg_similarity) * 0.3
        complexity_score += (1.0 - min_similarity) * 0.2
        complexity_score = min(complexity_score, 1.0)
        
        total_priority = (
            uncertainty_score * self.PRIORITY_WEIGHTS['uncertainty'] +
            impact_score * self.PRIORITY_WEIGHTS['impact'] +
            frequency_score * self.PRIORITY_WEIGHTS['frequency'] +
            complexity_score * self.PRIORITY_WEIGHTS['complexity']
        )
        
        return ActiveLearningScore(
            uncertainty_score=uncertainty_score,
            impact_score=impact_score,
            frequency_score=frequency_score,
            complexity_score=complexity_score,
            total_priority=total_priority
        )
    
    # ========================================================================
    # éšæ®µä¸€: è³‡æ–™è¼‰å…¥èˆ‡é è™•ç† (ç¹¼æ‰¿ v3)
    # ========================================================================
    
    def _initialize_base_ontology(self):
        """åˆå§‹åŒ–åŸºç¤æœ¬é«”çµæ§‹(è·æ¥­å®‰å…¨è¡›ç”Ÿé ˜åŸŸçŸ¥è­˜)"""
        
        # å®Œæ•´çš„ä¸»é«”æœ¬é«”(Subject Ontology)
        self.base_subject_ontology = {
            "é›‡ä¸»": {
                "standard_name": "EMPLOYER",
                "parent_category": "LegalEntity",
                "level": 2,
                "hierarchy_path": "Subject -> LegalEntity -> Employer",
                "synonyms": ["äº‹æ¥­å–®ä½", "äº‹æ¥­ä¸»", "æ¥­ä¸»", "å…¬å¸"],
                "description": "è² æœ‰è·æ¥­å®‰å…¨è¡›ç”Ÿæ³•å¾‹ç¾©å‹™çš„äº‹æ¥­ç¶“ç‡Ÿä¸»é«”"
            },
            "å‹å·¥": {
                "standard_name": "WORKER",
                "parent_category": "Person",
                "level": 2,
                "hierarchy_path": "Subject -> Person -> Worker",
                "synonyms": ["å·¥ä½œè€…", "å“¡å·¥", "å¾æ¥­äººå“¡", "ä½œæ¥­äººå“¡"],
                "description": "å—åƒ±æ–¼é›‡ä¸»å¾äº‹å·¥ä½œç²è‡´å·¥è³‡è€…"
            },
            "æ‰¿æ”¬äºº": {
                "standard_name": "CONTRACTOR",
                "parent_category": "LegalEntity",
                "level": 2,
                "hierarchy_path": "Subject -> LegalEntity -> Contractor",
                "synonyms": ["æ‰¿åŒ…å•†", "æ‰¿æ”¬å» å•†", "å¤–åŒ…å•†"],
                "description": "æ‰¿æ”¬äº‹æ¥­å–®ä½å·¥ä½œä¹‹äº‹æ¥­å–®ä½"
            },
            "ä»£è¡Œæª¢æŸ¥æ©Ÿæ§‹": {
                "standard_name": "INSPECTION_AGENCY",
                "parent_category": "Organization",
                "level": 2,
                "hierarchy_path": "Subject -> Organization -> InspectionAgency",
                "synonyms": ["æª¢æŸ¥æ©Ÿæ§‹", "æª¢é©—æ©Ÿæ§‹", "ä»£æª¢æ©Ÿæ§‹"],
                "description": "ç¶“ä¸­å¤®ä¸»ç®¡æ©Ÿé—œèªå¯ä»£è¡Œæª¢æŸ¥æ¥­å‹™ä¹‹æ©Ÿæ§‹"
            },
            "è·æ¥­å®‰å…¨è¡›ç”Ÿç®¡ç†äººå“¡": {
                "standard_name": "OSH_PERSONNEL",
                "parent_category": "Person",
                "level": 3,
                "hierarchy_path": "Subject -> Person -> Professional -> OSHPersonnel",
                "synonyms": ["å®‰å…¨è¡›ç”Ÿäººå“¡", "è·å®‰äººå“¡", "å®‰å…¨ç®¡ç†å“¡"],
                "description": "å¾äº‹è·æ¥­å®‰å…¨è¡›ç”Ÿç®¡ç†å·¥ä½œä¹‹å°ˆæ¥­äººå“¡"
            }
        }
        
        # å®Œæ•´çš„å®¢é«”æœ¬é«”(Object Ontology) - åˆ†å±¤çµæ§‹
        self.base_object_ontology = {
            # æ©Ÿæ¢°è¨­å‚™é¡
            "èµ·é‡æ©Ÿ": {
                "standard_name": "CRANE",
                "parent_category": "LiftingEquipment",
                "level": 3,
                "hierarchy_path": "Object -> Equipment -> LiftingEquipment -> Crane",
                "synonyms": ["åŠè»Š", "èµ·é‡è¨­å‚™"],
                "description": "ç”¨æ–¼åŠå‡åŠæ¬é‹é‡ç‰©ä¹‹æ©Ÿæ¢°è¨­å‚™"
            },
            "å‡é™æ©Ÿ": {
                "standard_name": "ELEVATOR",
                "parent_category": "LiftingEquipment",
                "level": 3,
                "hierarchy_path": "Object -> Equipment -> LiftingEquipment -> Elevator",
                "synonyms": ["é›»æ¢¯", "æ˜‡é™è¨­å‚™"],
                "description": "ç”¨æ–¼è¼‰é‹äººå“¡æˆ–è²¨ç‰©æ–¼ä¸åŒæ¨“å±¤é–“ä¹‹è¨­å‚™"
            },
            "è¡å£“æ©Ÿæ¢°": {
                "standard_name": "PRESS_MACHINE",
                "parent_category": "ProcessingEquipment",
                "level": 3,
                "hierarchy_path": "Object -> Equipment -> ProcessingEquipment -> PressMachine",
                "synonyms": ["æ²–åºŠ", "æ²–å£“åºŠ", "è¡å‰ªæ©Ÿ"],
                "description": "åˆ©ç”¨å£“åŠ›é€²è¡Œé‡‘å±¬æˆ–å…¶ä»–ææ–™åŠ å·¥ä¹‹æ©Ÿæ¢°"
            },
            
            # åŒ–å­¸ç‰©è³ªé¡
            "å±å®³æ€§åŒ–å­¸å“": {
                "standard_name": "HAZARDOUS_CHEMICAL",
                "parent_category": "ChemicalSubstance",
                "level": 2,
                "hierarchy_path": "Object -> Substance -> ChemicalSubstance -> HazardousChemical",
                "synonyms": ["æœ‰å®³åŒ–å­¸ç‰©è³ª", "å±éšªåŒ–å­¸å“"],
                "description": "å…·æœ‰å±å®³æ€§ä¹‹åŒ–å­¸å“"
            },
            "æœ‰æ©Ÿæº¶åŠ‘": {
                "standard_name": "ORGANIC_SOLVENT",
                "parent_category": "HazardousChemical",
                "level": 3,
                "hierarchy_path": "Object -> Substance -> ChemicalSubstance -> HazardousChemical -> OrganicSolvent",
                "synonyms": ["æº¶åŠ‘", "æœ‰æ©Ÿæº¶åŠ‘é¡"],
                "description": "èƒ½æº¶è§£å…¶ä»–ç‰©è³ªçš„æœ‰æ©ŸåŒ–åˆç‰©"
            },
            
            # é˜²è­·è¨­å‚™é¡
            "å®‰å…¨è£ç½®": {
                "standard_name": "SAFETY_DEVICE",
                "parent_category": "SafetyEquipment",
                "level": 2,
                "hierarchy_path": "Object -> Equipment -> SafetyEquipment -> SafetyDevice",
                "synonyms": ["å®‰å…¨è¨­æ–½", "å®‰å…¨è¨­å‚™"],
                "description": "ç”¨æ–¼é é˜²å±å®³ä¹‹è£ç½®"
            },
            "è­·ç½©": {
                "standard_name": "GUARD",
                "parent_category": "SafetyDevice",
                "level": 3,
                "hierarchy_path": "Object -> Equipment -> SafetyEquipment -> SafetyDevice -> Guard",
                "synonyms": ["é˜²è­·ç½©", "å®‰å…¨è­·ç½©"],
                "description": "é˜²æ­¢äººå“¡æ¥è§¸å±éšªéƒ¨ä½ä¹‹è­·è“‹æˆ–é®è”½ç‰©"
            },
            "å€‹äººé˜²è­·å…·": {
                "standard_name": "PPE",
                "parent_category": "SafetyEquipment",
                "level": 2,
                "hierarchy_path": "Object -> Equipment -> SafetyEquipment -> PPE",
                "synonyms": ["é˜²è­·å…·", "é˜²è­·è£å‚™"],
                "description": "å€‹äººç©¿æˆ´ä½¿ç”¨ä¹‹é˜²è­·è¨­å‚™"
            },
            "å®‰å…¨å¸½": {
                "standard_name": "SAFETY_HELMET",
                "parent_category": "PPE",
                "level": 3,
                "hierarchy_path": "Object -> Equipment -> SafetyEquipment -> PPE -> SafetyHelmet",
                "synonyms": ["å·¥å®‰å¸½", "é˜²è­·å¸½"],
                "description": "ä¿è­·é ­éƒ¨å…å—æ’æ“Šä¹‹å¸½å…·"
            },
            "å®‰å…¨å¸¶": {
                "standard_name": "SAFETY_HARNESS",
                "parent_category": "PPE",
                "level": 3,
                "hierarchy_path": "Object -> Equipment -> SafetyEquipment -> PPE -> SafetyHarness",
                "synonyms": ["é˜²å¢œå™¨", "å®‰å…¨ç´¢"],
                "description": "é˜²æ­¢é«˜è™•ä½œæ¥­å¢œè½ä¹‹é˜²è­·è£å‚™"
            },
            
            # ä½œæ¥­å ´æ‰€èˆ‡ç’°å¢ƒé¡
            "é«˜æº«ä½œæ¥­å ´æ‰€": {
                "standard_name": "HIGH_TEMP_WORKPLACE",
                "parent_category": "Workplace",
                "level": 2,
                "hierarchy_path": "Object -> Environment -> Workplace -> HighTempWorkplace",
                "synonyms": ["é«˜æº«ç’°å¢ƒ", "ç†±ä½œæ¥­å ´æ‰€", "é‹çˆæˆ¿", "é‘„é€ é–“"],
                "description": "æº«åº¦éé«˜ä¹‹ä½œæ¥­ç’°å¢ƒ"
            },
            "å¯†é–‰ç©ºé–“": {
                "standard_name": "CONFINED_SPACE",
                "parent_category": "Workplace",
                "level": 2,
                "hierarchy_path": "Object -> Environment -> Workplace -> ConfinedSpace",
                "synonyms": ["ä¾·é™ç©ºé–“", "å±€é™ç©ºé–“"],
                "description": "é€šé¢¨ä¸è‰¯ä¹‹æœ‰é™ç©ºé–“"
            }
        }

        print("  â†’ v5 åˆå§‹åŒ–: ç‚ºæœ¬é«”å¯¦é«”ç”Ÿæˆèªæ„å‘é‡...")
        self._generate_ontology_embeddings()

    def _generate_ontology_embeddings(self):
        """
        v5 æ ¸å¿ƒæ–¹æ³•: ç‚ºæ‰€æœ‰æœ¬é«”å¯¦é«”ç”Ÿæˆ embeddings
        è§£æ±ºå•é¡Œä¸‰ - å¯¦ç¾èªæ„å¯¦é«”é€£çµçš„åŸºç¤
        
        èª¿ç”¨æ™‚æ©Ÿ: _initialize_base_ontology() çµæŸæ™‚
        """
        all_ontologies = {
            **{f"subject_{k}": v for k, v in self.base_subject_ontology.items()},
            **{f"object_{k}": v for k, v in self.base_object_ontology.items()}
        }
        
        for entity_key, entity_info in all_ontologies.items():
            entity_description = f"""
    å¯¦é«”åç¨±: {entity_info.get('standard_name', '')}
    åŒç¾©è©: {', '.join(entity_info.get('synonyms', []))}
    æè¿°: {entity_info.get('description', '')}
    å±¤ç´šè·¯å¾‘: {entity_info.get('hierarchy_path', '')}
            """.strip()
            
            try:
                response = self.client.embeddings.create(
                    model="text-embedding-3-small",
                    input=[entity_description]
                )
                embedding = response.data[0].embedding
                self.ontology_embeddings[entity_key] = np.array(embedding)
                
            except Exception as e:
                print(f"    âœ— æœ¬é«”å¯¦é«” {entity_key} embedding ç”Ÿæˆå¤±æ•—: {e}")
                self.ontology_embeddings[entity_key] = np.zeros(1536)
        
        print(f"  âœ“ å·²ç”Ÿæˆ {len(self.ontology_embeddings)} å€‹æœ¬é«”å¯¦é«”å‘é‡")

    def _find_ontology_node_semantic(self, entity_text: str, threshold: float = 0.6) -> Optional[str]:
        """
        v5 æ ¸å¿ƒæ–¹æ³•: ä½¿ç”¨èªæ„å‘é‡åŒ¹é…æœ¬é«”ç¯€é»
        è§£æ±ºå•é¡Œä¸‰ - "å¯ç‡ƒæ€§æ°£é«”" â†’ HAZARDOUS_CHEMICAL
        
        èª¿ç”¨æ™‚æ©Ÿ: _build_event_layer() è™•ç† event.patients æ™‚
        å–ä»£: v4 çš„ _find_ontology_node_for_entity() å­—ä¸²åŒ¹é…
        
        Args:
            entity_text: å¯¦é«”æ–‡æœ¬ (å¦‚ "å¯ç‡ƒæ€§æ°£é«”")
            threshold: æœ€ä½ç›¸ä¼¼åº¦é–¾å€¼ (é è¨­ 0.6)
            
        Returns:
            æœ€åŒ¹é…çš„æœ¬é«”ç¯€é» ID (å¦‚ "object_HAZARDOUS_CHEMICAL"),æˆ– None
        """
        if not entity_text or not self.ontology_embeddings:
            return None
        
        try:
            # ç”Ÿæˆå¯¦é«”æ–‡æœ¬çš„ embedding
            response = self.client.embeddings.create(
                model="text-embedding-3-small",
                input=[entity_text]
            )
            entity_vector = np.array(response.data[0].embedding)
            
            # è¨ˆç®—èˆ‡æ‰€æœ‰æœ¬é«”å¯¦é«”çš„é¤˜å¼¦ç›¸ä¼¼åº¦
            best_match = None
            best_similarity = threshold
            
            for ontology_key, ontology_vector in self.ontology_embeddings.items():
                cos_sim = np.dot(entity_vector, ontology_vector) / (
                    np.linalg.norm(entity_vector) * np.linalg.norm(ontology_vector)
                )
                
                if cos_sim > best_similarity:
                    best_similarity = cos_sim
                    best_match = ontology_key
            
            if best_match:
                # è½‰æ›ç‚ºåœ–è­œç¯€é» ID
                if best_match.startswith("subject_"):
                    entity_name = best_match[8:]
                    if entity_name in self.base_subject_ontology:
                        standard_name = self.base_subject_ontology[entity_name]['standard_name']
                        return f"subject_{standard_name}"
                elif best_match.startswith("object_"):
                    entity_name = best_match[7:]
                    if entity_name in self.base_object_ontology:
                        standard_name = self.base_object_ontology[entity_name]['standard_name']
                        return f"object_{standard_name}"
            
            return None
            
        except Exception as e:
            print(f"    âœ— èªæ„å¯¦é«”é€£çµå¤±æ•— ({entity_text}): {e}")
            return None
    
    def load_documents(self):
        """è¼‰å…¥æ³•å¾‹æ–‡ä»¶"""
        print("ğŸ“š è¼‰å…¥æ³•å¾‹æ–‡ä»¶...")
        try:
            with open(self.input_path, 'r', encoding='utf-8') as f:
                self.documents = json.load(f)
            print(f"  âœ“ å·²è¼‰å…¥ {len(self.documents)} å€‹æ³•å¾‹æ¢æ–‡ç‰‡æ®µ")
        except FileNotFoundError:
            print(f"  âœ— éŒ¯èª¤:æ‰¾ä¸åˆ°æª”æ¡ˆ {self.input_path}")
            sys.exit(1)
        except json.JSONDecodeError as e:
            print(f"  âœ— éŒ¯èª¤:JSON è§£æå¤±æ•— - {e}")
            sys.exit(1)
    
    # ========================================================================
    # éšæ®µäºŒ: äº‹ä»¶æŠ½å– (ç¹¼æ‰¿ v3 çš„ Tool Calling æ–¹æ³•)
    # ========================================================================
    
    def extract_legal_events(self):
        """å¾æ³•å¾‹æ–‡æœ¬ä¸­æŠ½å–çµæ§‹åŒ–äº‹ä»¶"""
        print("\nğŸ¯ æŠ½å–æ³•å¾‹äº‹ä»¶çµæ§‹...")
        
        for doc in self.documents:
            content = doc.get('content', '')
            metadata = doc.get('metadata', {})
            article_id = metadata.get('article', 'unknown')
            
            if not content or len(content) < 20:
                continue
            
            event = self._extract_event_with_llm(content, article_id)
            
            if event:
                self.legal_events[event.event_id] = event
                
                all_entities = (
                    event.patients + 
                    event.instruments + 
                    event.locations
                )
                self.discovered_entities.update(all_entities)
        
        print(f"  âœ“ æŠ½å–äº† {len(self.legal_events)} å€‹æ³•å¾‹äº‹ä»¶")
        print(f"  âœ“ è‡ªå‹•ç™¼ç¾ {len(self.discovered_entities)} å€‹æ–°å¯¦é«”")
        
        self._save_legal_events()
        self._incremental_ontology_expansion()
    
    def _extract_event_with_llm(self, text: str, article_id: str) -> Optional[LegalEvent]:
        """ä½¿ç”¨LLMæŠ½å–å–®ä¸€æ³•å¾‹äº‹ä»¶ (v3çš„æˆåŠŸå¯¦ç¾)"""
        
        def _ensure_string(value: Any) -> str:
            """ç¢ºä¿è¿”å›é None çš„å­—ç¬¦ä¸²"""
            if value is None:
                return ''
            if isinstance(value, str):
                return value.strip()
            if isinstance(value, (int, float)):
                return str(value)
            if isinstance(value, list) and len(value) > 0:
                for item in value:
                    if isinstance(item, str) and item.strip():
                        return item.strip()
            return ''
        
        def _ensure_string_list(value: Any) -> List[str]:
            """ç¢ºä¿è¿”å›å­—ç¬¦ä¸²åˆ—è¡¨"""
            if not value:
                return []
            
            if isinstance(value, str):
                return [value.strip()] if value.strip() else []
            
            if isinstance(value, list):
                result = []
                for item in value:
                    if isinstance(item, list):
                        result.extend(_ensure_string_list(item))
                    elif item is not None:
                        cleaned = str(item).strip()
                        if cleaned:
                            result.append(cleaned)
                return result
            
            cleaned_val = str(value).strip()
            return [cleaned_val] if cleaned_val else []
        
        tool_schema = {
            "type": "function",
            "function": {
                "name": "record_legal_event",
                "description": "è¨˜éŒ„å¾æ³•å¾‹æ–‡æœ¬ä¸­æŠ½å–çš„çµæ§‹åŒ–äº‹ä»¶",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "action": {
                            "type": "string",
                            "description": "æ ¸å¿ƒå‹•ä½œ (å¦‚: ä¾›æ‡‰ã€è¨­ç½®ã€æª¢æŸ¥ã€ä½¿ç”¨)"
                        },
                        "actor": {
                            "type": "string",
                            "description": "ä¸»é«” (å¦‚: é›‡ä¸»ã€å‹å·¥ã€äº‹æ¥­å–®ä½)"
                        },
                        "patients": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "å—äº‹å®¢é«”åˆ—è¡¨ (å¦‚: é£²ç”¨æ°´ã€é£Ÿé¹½ã€è­·æ¬„ã€è¨­å‚™)"
                        },
                        "instruments": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "å·¥å…·/æ‰‹æ®µåˆ—è¡¨ (å¦‚: å®‰å…¨å¸½ã€é€šé¢¨è¨­å‚™)"
                        },
                        "locations": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "å ´æ‰€åˆ—è¡¨ (å¦‚: é«˜æº«ä½œæ¥­å ´æ‰€ã€å¯†é–‰ç©ºé–“)"
                        },
                        "conditions": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "æ¢ä»¶åˆ—è¡¨ (å¦‚: æº«åº¦è¶…é30åº¦ã€ä½œæ¥­æ™‚é–“è¶…é1å°æ™‚)"
                        },
                        "temporal": {
                            "type": "string",
                            "description": "æ™‚é–“æ¢ä»¶ (å¦‚: æ¯æ—¥ã€å®šæœŸã€ä½œæ¥­å‰)"
                        },
                        "purpose": {
                            "type": "string",
                            "description": "ç›®çš„ (å¦‚: é˜²æ­¢ä¸­æš‘ã€é é˜²å¢œè½)"
                        }
                    },
                    "required": ["action", "actor", "patients", "instruments", "locations"]
                }
            }
        }

        prompt = f"""ä½ æ˜¯è·æ¥­å®‰å…¨è¡›ç”Ÿæ³•å¾‹äº‹ä»¶æŠ½å–å°ˆå®¶ã€‚è«‹å¾ä»¥ä¸‹æ³•æ¢æ–‡æœ¬ä¸­æŠ½å–çµæ§‹åŒ–çš„æ³•å¾‹äº‹ä»¶ã€‚

æ³•æ¢æ–‡æœ¬:
{text}

ä½ å¿…é ˆåš´æ ¼éµå¾ªä»¥ä¸‹ã€å…©éšæ®µä»»å‹™ã€‘ï¼š

ã€ä»»å‹™ 1ï¼šèªæ„è§’è‰²æ¨™è¨» (SRL)ã€‘
è«‹å…ˆæ‰¾å‡ºæ³•æ¢ä¸­çš„æ ¸å¿ƒæ³•å¾‹äº‹ä»¶ï¼š
- å‹•ä½œ (action): æ ¸å¿ƒå‹•è© (å¦‚: è¨­ç½®ã€æª¢æŸ¥ã€ä¾›æ‡‰)ã€‚
- ä¸»é«” (actor): å‹•ä½œçš„åŸ·è¡Œè€… (å¦‚: é›‡ä¸»ã€å‹å·¥)ã€‚
- å—äº‹å®¢é«” (patients): å‹•ä½œçš„ã€Œç›´æ¥å°è±¡ã€æˆ–ã€Œæ‰¿å—è€…ã€ (å¦‚: "è¨­ç½®" çš„ "é˜²è­·å…·")ã€‚
- å·¥å…·/æ‰‹æ®µ (instruments): ç”¨ä¾†ã€Œå®Œæˆå‹•ä½œã€çš„å·¥å…· (å¦‚: "ä½¿ç”¨" "çµ•ç†±ææ–™" "è¢«è¦†" å®¹å™¨)ã€‚

ã€ä»»å‹™ 2ï¼šç´„æŸæ¢ä»¶èƒå– (Constraint Extraction)ã€‘
åœ¨å®Œæˆä»»å‹™ 1 ä¹‹å¾Œï¼Œä½ ã€å¿…é ˆã€‘å›é ­æª¢è¦–æ³•æ¢ï¼Œæ‰¾å‡ºèˆ‡è©²äº‹ä»¶ç›¸é—œçš„æ‰€æœ‰ã€Œç´„æŸæ¢ä»¶ã€ï¼š
- æ¢ä»¶ (conditions): åŸ·è¡Œå‹•ä½œçš„å¿…è¦æ¢ä»¶ (å¦‚: æº«åº¦è¶…é30åº¦ã€é«˜åº¦åœ¨äºŒå…¬å°ºä»¥ä¸Š)ã€‚
- æ™‚é–“ (temporal): å‹•ä½œç™¼ç”Ÿçš„æ™‚é–“é™åˆ¶ (å¦‚: æ¯æ—¥ã€å®šæœŸã€ä½œæ¥­å‰)ã€‚
- ç›®çš„ (purpose): åŸ·è¡Œå‹•ä½œçš„æ³•å¾‹ç›®çš„ (å¦‚: é˜²æ­¢ä¸­æš‘ã€é é˜²å¢œè½)ã€‚

ã€é‡è¦æŒ‡ä»¤ã€‘
1. ã€ä¸å¯éºæ¼ã€‘: åŸ·è¡Œä»»å‹™ 2 èˆ‡åŸ·è¡Œä»»å‹™ 1 åŒç­‰é‡è¦ã€‚å³ä½¿ `conditions` æˆ– `temporal` å¾ˆé•·ï¼Œä¹Ÿå¿…é ˆå®Œæ•´èƒå–ã€‚
2. ã€å€åˆ†è§’è‰²ã€‘: åš´æ ¼å€åˆ† `patients` (è¢«ä½œç”¨çš„å°è±¡) å’Œ `instruments` (ç”¨ä¾†ä½œç”¨çš„å·¥å…·)ã€‚
3. ã€æ¬„ä½ç‚ºç©ºã€‘: å¦‚æœæŸå€‹æ¬„ä½ (ä¾‹å¦‚ `purpose`) åœ¨æ–‡æœ¬ä¸­ç¢ºå¯¦ä¸å­˜åœ¨ï¼Œè«‹ä½¿ç”¨ç©ºåˆ—è¡¨ `[]` æˆ–ç©ºå­—ä¸² `""`ã€‚

è«‹ä»”ç´°åˆ†ææ–‡æœ¬ä¸¦èª¿ç”¨ 'record_legal_event' å·¥å…·ä¾†è¨˜éŒ„ä½ çš„ç™¼ç¾ã€‚"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system", 
                        "content": "ä½ æ˜¯è·æ¥­å®‰å…¨æ³•å¾‹äº‹ä»¶æŠ½å–å°ˆå®¶,ç²¾é€šèªæ„è§’è‰²æ¨™è¨»(Semantic Role Labeling)ã€‚ä½ å¿…é ˆä½¿ç”¨ 'record_legal_event' å·¥å…·ä¾†æäº¤ä½ çš„åˆ†æçµæœã€‚"
                    },
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=1000,
                tools=[tool_schema],
                tool_choice={"type": "function", "function": {"name": "record_legal_event"}}
            )
            
            message = response.choices[0].message
            
            if not message.tool_calls or len(message.tool_calls) == 0:
                print(f"    âš ï¸ äº‹ä»¶æŠ½å–æœªèª¿ç”¨å·¥å…· ({article_id})")
                return None
                
            tool_call_args = message.tool_calls[0].function.arguments
            
            try:
                parsed = json.loads(tool_call_args)
            except json.JSONDecodeError as e:
                print(f"    âœ— åš´é‡: Tool Call åƒæ•¸è§£æå¤±æ•— ({article_id}): {e}")
                return None

            if not isinstance(parsed, dict):
                print(f"    âš ï¸ äº‹ä»¶æŠ½å–è¿”å›éå­—å…¸æ ¼å¼ ({article_id})")
                return None
            
            action = _ensure_string(parsed.get('action'))
            actor = _ensure_string(parsed.get('actor'))
            
            if not action and not actor:
                print(f"    âš ï¸ äº‹ä»¶æŠ½å–ç„¡æœ‰æ•ˆå…§å®¹ (ä½†JSONæœ‰æ•ˆ) ({article_id})")
                return None
            
            patients = _ensure_string_list(parsed.get('patients'))
            instruments = _ensure_string_list(parsed.get('instruments'))
            locations = _ensure_string_list(parsed.get('locations'))
            conditions = _ensure_string_list(parsed.get('conditions'))
            
            temporal = _ensure_string(parsed.get('temporal')) or None
            purpose = _ensure_string(parsed.get('purpose')) or None
            
            filled_fields = sum([
                bool(action),
                bool(actor),
                len(patients) > 0,
                len(instruments) > 0,
                len(locations) > 0,
                len(conditions) > 0,
                bool(temporal),
                bool(purpose)
            ])
            confidence = min(0.95, 0.5 + (filled_fields / 8) * 0.45)
            
            event = LegalEvent(
                event_id=f"event_{article_id}_{len(self.legal_events)}",
                action=action,
                actor=actor,
                patients=patients,
                instruments=instruments,
                locations=locations,
                conditions=conditions,
                temporal=temporal,
                purpose=purpose,
                source_article=article_id,
                confidence=confidence
            )
            
            return event
            
        except Exception as e:
            print(f"    âœ— äº‹ä»¶æŠ½å– API èª¿ç”¨å¤±æ•— ({article_id}): {e}")
            return None
    
    def _save_legal_events(self):
        """å„²å­˜æ³•å¾‹äº‹ä»¶"""
        output_path = os.path.join(self.output_dir, "legal_events.json")
        
        events_data = {
            event_id: asdict(event) 
            for event_id, event in self.legal_events.items()
        }
        
        serializable_events = self._make_json_serializable(events_data)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_events, f, ensure_ascii=False, indent=2)
        
        print(f"  âœ“ æ³•å¾‹äº‹ä»¶å·²å„²å­˜è‡³ {output_path}")
    
    def _incremental_ontology_expansion(self):
        """å¢é‡å¼æœ¬é«”æ“´å……"""
        print("\nğŸ“š å¢é‡å¼æœ¬é«”æ“´å……...")
        
        if not self.discovered_entities:
            print("  âœ“ ç„¡æ–°å¯¦é«”éœ€è¦æ“´å……")
            return
        
        existing_entities = set(self.object_ontology.keys())
        new_entities = self.discovered_entities - existing_entities
        
        if not new_entities:
            print("  âœ“ æ‰€æœ‰ç™¼ç¾çš„å¯¦é«”å·²å­˜åœ¨æ–¼æœ¬é«”ä¸­")
            return
        
        print(f"  â†’ ç™¼ç¾ {len(new_entities)} å€‹æ–°å¯¦é«”,é–‹å§‹æ™ºèƒ½åˆ†é¡...")
        
        expanded_ontology = self._classify_new_entities_to_ontology(
            new_entities, 
            self.object_ontology
        )
        
        self.object_ontology.update(expanded_ontology)
        
        print(f"  âœ“ æœ¬é«”å·²æ“´å…… {len(expanded_ontology)} å€‹æ–°ç¯€é»")
        
        self._save_ontology()
    
    def _classify_new_entities_to_ontology(self, 
                                          new_entities: Set[str], 
                                          existing_ontology: Dict) -> Dict:
        """ä½¿ç”¨LLMå°‡æ–°å¯¦é«”åˆ†é¡åˆ°ç¾æœ‰æœ¬é«”å±¤ç´š"""
        
        ontology_summary = {}
        for entity_name, entity_info in existing_ontology.items():
            category = entity_info.get('parent_category', 'Unknown')
            if category not in ontology_summary:
                ontology_summary[category] = []
            ontology_summary[category].append(entity_name)
        
        prompt = f"""ä½ æ˜¯è·æ¥­å®‰å…¨æœ¬é«”è«–å°ˆå®¶ã€‚ç¾æœ‰æœ¬é«”çµæ§‹å¦‚ä¸‹:

{json.dumps(ontology_summary, ensure_ascii=False, indent=2)}

æ–°ç™¼ç¾çš„å¯¦é«”: {list(new_entities)}

è«‹å°‡é€™äº›æ–°å¯¦é«”åˆ†é¡åˆ°æœ€åˆé©çš„çˆ¶é¡åˆ¥ä¸‹,ä¸¦æä¾›å®Œæ•´çš„æœ¬é«”ç¯€é»å®šç¾©ã€‚

è¼¸å‡ºæ ¼å¼:
{{
  "å¯¦é«”åç¨±": {{
    "standard_name": "æ¨™æº–åŒ–åç¨±(è‹±æ–‡å¤§å¯«åŠ åº•ç·š)",
    "parent_category": "æœ€åˆé©çš„çˆ¶é¡åˆ¥(å¿…é ˆå¾ä¸Šè¿°ç¾æœ‰é¡åˆ¥ä¸­é¸æ“‡)",
    "level": å±¤ç´šæ•¸å­—,
    "hierarchy_path": "å®Œæ•´è·¯å¾‘",
    "synonyms": ["åŒç¾©è©åˆ—è¡¨"],
    "description": "ç°¡çŸ­æè¿°"
  }}
}}

åªè¼¸å‡ºJSONã€‚"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯è·æ¥­å®‰å…¨æœ¬é«”è«–å°ˆå®¶,ç²¾é€šçŸ¥è­˜çµ„ç¹”èˆ‡åˆ†é¡ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                max_tokens=2000
            )
            
            result_text = response.choices[0].message.content.strip()
            parsed = self._safe_parse_json_from_llm(result_text)
            
            if isinstance(parsed, dict):
                return parsed
            else:
                return {}
                
        except Exception as e:
            print(f"    âœ— æ–°å¯¦é«”åˆ†é¡å¤±æ•—: {e}")
            return {}
    
    # ========================================================================
    # éšæ®µä¸‰: ç¾©å‹™æ­£è¦åŒ– (v4 å®Œå…¨é‡å¯«)
    # ========================================================================
    
    def normalize_obligations(self):
        """
        æ­£è¦åŒ–ç¾©å‹™ç¯€é» (v5 ç‰ˆæœ¬: èªå¢ƒæ„ŸçŸ¥ + HDBSCAN èšé¡)
        
        é€™æ˜¯ _save_obligation_clusters çš„ä¸»è¦å‘¼å«è€… (Caller)ã€‚
        """
        print("\nğŸ“„ æ­£è¦åŒ–ç¾©å‹™ç¯€é» (v5 æ™ºæ…§èªæ„èšé¡)...")
        
        # === v4 æ ¸å¿ƒæ”¹é€²: ä½¿ç”¨èªå¢ƒæ„ŸçŸ¥èƒå– ===
        obligations_with_context = self._extract_obligations_with_context()
        
        if not obligations_with_context:
            print("  âš ï¸ è­¦å‘Š:æœªèƒå–åˆ°ä»»ä½•ç¾©å‹™æè¿°")
            self.obligation_clusters = {}
            return
        
        print(f"  âœ“ èƒå–äº† {len(obligations_with_context)} å€‹ç¾©å‹™æè¿°")
        
        # ä½¿ç”¨ã€Œå«èªå¢ƒçš„å®Œæ•´æ–‡æœ¬ã€ç”Ÿæˆå‘é‡
        obligation_texts = [obl['full_text_with_context'] for obl in obligations_with_context]
        
        print("  â†’ ç”Ÿæˆèªæ„å‘é‡...")
        obligation_vectors = self._get_embeddings(obligation_texts)
        
        if obligation_vectors.size == 0:
            print("  âœ— éŒ¯èª¤:å‘é‡ç”Ÿæˆå¤±æ•—,è·³éç¾©å‹™æ­£è¦åŒ–éšæ®µ")
            self.obligation_clusters = {}
            return
        
        # === v4 æ ¸å¿ƒæ”¹é€²: ä½¿ç”¨ HDBSCAN èšé¡ ===
        # clusters = self._cluster_obligations_with_hdbscan(obligations_with_context, obligation_vectors)
        
        # âš ï¸ v5 ä¿®æ”¹: ä½¿ç”¨è‘‰èšé¡
        clusters = self._cluster_obligations_with_hdbscan(obligations_with_context, obligation_vectors)
        
        # === é—œéµé˜²å‘†æ©Ÿåˆ¶ (v5.1 ä¿®æ­£) ===
        if not clusters:
            print("  âœ— éŒ¯èª¤: HDBSCAN æœªèƒ½ç”¢ç”Ÿä»»ä½•æœ‰æ•ˆèšé¡ (èšé¡æ•¸é‡ç‚º 0)ã€‚")
            print("    â†’ é€™å¯èƒ½æ˜¯å› ç‚ºè¼¸å…¥çš„æ³•è¦æ–‡ä»¶éå°‘æˆ– 'min_cluster_size' åƒæ•¸(3)éé«˜ã€‚")
            print("    â†’ obligation_clusters.json å°‡ç‚ºç©ºï¼Œå¾ŒçºŒæµç¨‹ (control_type_mapping) å°‡è¢«è·³éã€‚")
            self.obligation_clusters = {}
            return # æå‰çµ‚æ­¢ï¼Œé˜²æ­¢å¾ŒçºŒæµç¨‹å‡ºéŒ¯
        
        # å“è³ªè©•ä¼° (ç¹¼æ‰¿ v4,å‡½å¼åæ”¹ç‚º v5)
        print("  â†’ è©•ä¼°èšé¡å“è³ª...")
        clusters_with_quality = self._evaluate_cluster_quality_v5(
            clusters, 
            obligations_with_context, 
            obligation_vectors
        )
        
        # âš ï¸ v5 ä¿®æ”¹: ä½¿ç”¨æç¤ºè©éˆå‘½å
        print("  â†’ ç‚ºèšé¡å‘½å (v5 æç¤ºè©éˆ)...")
        self.obligation_clusters = self._name_clusters_with_llm_v5(
            clusters_with_quality,
            obligation_vectors
        )
        
        # å„²å­˜ (ç¹¼æ‰¿ v4)
        self._save_obligation_clusters()
        
        # å°‡å•é¡Œèšé¡åŠ å…¥å¯©æ ¸ä½‡åˆ— (ç¹¼æ‰¿ v4)
        self._queue_problematic_clusters_v5()

    def _save_obligation_clusters(self):
        """
        (v5 å¯¦ä½œ) å„²å­˜ç¾©å‹™èšé¡çµæœ
        
        é€™å€‹å‡½å¼æœƒç”¢ç”Ÿ obligation_clusters.jsonã€‚
        å®ƒç¹¼æ‰¿è‡ª v4, ç¢ºä¿ v4 èšé¡å¾Œçš„è¤‡é›œ dict çµæ§‹è¢«æ­£ç¢ºåºåˆ—åŒ–ã€‚
        """
        if not self.obligation_clusters:
            print(f"  âš ï¸ è­¦å‘Š:æ²’æœ‰ç¾©å‹™èšé¡è³‡æ–™å¯å„²å­˜")
            return
        
        output_path = os.path.join(self.output_dir, "obligation_clusters.json")
        
        save_data = {}
        for cluster_id, cluster_info in self.obligation_clusters.items():
            
            # v4 çš„ self.obligation_clusters[id]['members'] æ˜¯ä¸€å€‹ dict åˆ—è¡¨
            # æˆ‘å€‘åœ¨å„²å­˜æ™‚,åªå„²å­˜åŸå§‹æ–‡æœ¬ 'text',ä»¥ç°¡åŒ– JSON æª”æ¡ˆ
            members = cluster_info.get('members', [])
            if members and isinstance(members[0], dict):
                # åªä¿å­˜æ–‡æœ¬,ä¸ä¿å­˜å®Œæ•´çš„ dict
                members_text = [m.get('text', str(m)) for m in members]
            else:
                members_text = [str(m) for m in members]
            
            cluster_data = {
                'standard_name': cluster_info.get('standard_name'),
                'standard_code': cluster_info.get('standard_code'),
                'category': cluster_info.get('category'),
                'description': cluster_info.get('description'),
                'member_count': cluster_info.get('member_count'),
                'sample': cluster_info.get('sample', []), # sample å·²ç¶“æ˜¯ç´”æ–‡æœ¬
                'quality_metrics': cluster_info.get('quality_metrics'),
                'evidence': cluster_info.get('evidence'),
                'overall_confidence': cluster_info.get('overall_confidence'),
                'review_status': cluster_info.get('review_status'),
                'review_priority': cluster_info.get('review_priority'),
                # ç‚ºäº†æª”æ¡ˆå¤§å°,æˆ‘å€‘åªå„²å­˜æœ€å¤š 50 å€‹æˆå“¡
                'members': members_text[:50] if len(members_text) > 50 else members_text
            }
            save_data[str(cluster_id)] = cluster_data
        
        # ä½¿ç”¨ v4 çš„ _make_json_serializable ç¢ºä¿ Enum, numpy ç­‰é¡å‹å¯è¢«å„²å­˜
        serializable_data = self._make_json_serializable(save_data)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_data, f, ensure_ascii=False, indent=2)
        
        print(f"  âœ“ ç¾©å‹™èšé¡å·²å„²å­˜è‡³ {output_path}")
        print(f"    â†’ ç¸½å…± {len(save_data)} å€‹èšé¡")
    
    def _evaluate_cluster_quality_v5(self, 
                                     clusters: Dict[int, List[Dict]], 
                                     obligations: List[Dict],
                                     vectors: np.ndarray) -> Dict[int, Dict]:
        """
        v4 ç‰ˆæœ¬: è©•ä¼°èšé¡å“è³ª
        èˆ‡ v3 çš„å·®ç•°: è€ƒæ…®èªå¢ƒä¿¡æ¯å’Œ HDBSCAN ç‰¹æ€§
        """
        from sklearn.metrics.pairwise import cosine_similarity
        
        clusters_with_quality = {}
        
        # å»ºç«‹ç´¢å¼•æ˜ å°„
        text_to_index = {
            obl['full_text_with_context']: idx 
            for idx, obl in enumerate(obligations)
        }
        
        for cluster_id, members in clusters.items():
            member_count = len(members)
            is_singleton = (member_count == 1)
            
            # ç²å–æˆå“¡åœ¨åŸå§‹åˆ—è¡¨ä¸­çš„ç´¢å¼•
            member_indices = []
            for member in members:
                key = member['full_text_with_context']
                if key in text_to_index:
                    member_indices.append(text_to_index[key])
            
            if len(member_indices) < 2:
                quality = ClusterQualityMetrics(
                    silhouette_score=0.0,
                    avg_intra_similarity=1.0,
                    min_member_similarity=1.0,
                    is_singleton=True,
                    is_noise=False,
                    needs_review=True,
                    review_reason="å–®ä¾‹èšé¡:ç„¡æ³•æ‰¾åˆ°èªæ„ç›¸ä¼¼çš„ç¾©å‹™",
                    review_priority=ReviewPriority.MEDIUM
                )
            else:
                member_vectors = vectors[member_indices]
                sim_matrix = cosine_similarity(member_vectors)
                
                n = len(member_indices)
                similarities = []
                for i in range(n):
                    for j in range(i+1, n):
                        similarities.append(sim_matrix[i][j])
                
                avg_sim = np.mean(similarities) if similarities else 0.0
                min_sim = np.min(similarities) if similarities else 0.0
                
                needs_review = False
                review_reason = ""
                review_priority = ReviewPriority.LOW
                
                # v4 å¢å¼·çš„å“è³ªåˆ¤æ–·é‚è¼¯
                if member_count < self.MIN_CLUSTER_SIZE:
                    needs_review = True
                    review_reason = f"èšé¡éå°:åƒ… {member_count} å€‹æˆå“¡"
                    review_priority = ReviewPriority.MEDIUM
                elif min_sim < 0.65:  # v4: é™ä½é–¾å€¼,å› ç‚ºæˆ‘å€‘æœ‰èªå¢ƒä¿¡æ¯
                    needs_review = True
                    review_reason = f"å…§éƒ¨ç•°è³ªæ€§éé«˜:æœ€å°ç›¸ä¼¼åº¦åƒ… {min_sim:.2f}"
                    review_priority = ReviewPriority.HIGH
                elif avg_sim < 0.75:  # v4: é™ä½é–¾å€¼
                    needs_review = True
                    review_reason = f"å¹³å‡ç›¸ä¼¼åº¦åä½: {avg_sim:.2f}"
                    review_priority = ReviewPriority.MEDIUM
                
                # v4 æ–°å¢: æª¢æŸ¥æ˜¯å¦æœ‰éå¤šçš„ anaphora
                anaphora_count = sum(1 for m in members if m.get('has_anaphora', False))
                if anaphora_count > member_count * 0.5:
                    needs_review = True
                    review_reason += f" | éå¤šæ³•å¾‹å¼•ç”¨({anaphora_count}/{member_count})"
                    review_priority = max(review_priority, ReviewPriority.HIGH, key=lambda x: x.value)
                
                quality = ClusterQualityMetrics(
                    silhouette_score=0.0,
                    avg_intra_similarity=avg_sim,
                    min_member_similarity=min_sim,
                    is_singleton=False,
                    is_noise=False,
                    needs_review=needs_review,
                    review_reason=review_reason,
                    review_priority=review_priority
                )
            
            clusters_with_quality[cluster_id] = {
                'members': members,
                'quality': quality
            }
            
            if quality.needs_review:
                self.problematic_clusters.append({
                    'cluster_id': cluster_id,
                    'member_count': member_count,
                    'quality': asdict(quality),
                    'sample_members': [m['text'] for m in members[:5]]
                })
        
        review_count = sum(1 for c in clusters_with_quality.values() if c['quality'].needs_review)
        print(f"    â†’ å“è³ªè©•ä¼°å®Œæˆ: {review_count}/{len(clusters)} å€‹èšé¡éœ€è¦å¯©æ ¸")
        
        return clusters_with_quality
    
    def _name_clusters_with_llm_v5(self, 
                               clusters_with_quality: Dict[int, Dict],
                               obligation_vectors: np.ndarray) -> Dict:
        """
        v5 æ ¸å¿ƒæ”¹é€²: æç¤ºè©éˆ
        è§£æ±ºå•é¡ŒäºŒ - å¼·åˆ¶ LLM ä½¿ç”¨èªæ„åŒ¹é…çš„ rule_id
        
        é—œéµä¿®æ”¹:
        1. å°‡èªæ„åŒ¹é…çµæœæ³¨å…¥ prompt
        2. é©—è­‰ LLM æ˜¯å¦ä½¿ç”¨äº†æ³¨å…¥çš„çµæœ
        3. å¼·åˆ¶ä¿®æ­£éŒ¯èª¤çš„ rule_id
        """
        named_clusters = {}
        total_clusters = len(clusters_with_quality)

        print(f"    â†’ é–‹å§‹ç‚º {total_clusters} å€‹èšé¡å‘½å (v5 æç¤ºè©éˆ)...")
        
        for idx, (cluster_id, cluster_data) in enumerate(clusters_with_quality.items(), 1):
            cluster_id_int = int(cluster_id)
            members = cluster_data['members']
            quality = cluster_data['quality']
            
            if not isinstance(members, list):
                named_clusters[cluster_id_int] = self._create_fallback_cluster(cluster_id_int, members, quality)
                continue

            if idx % 10 == 0 or idx == total_clusters:
                print(f"    â†’ é€²åº¦: {idx}/{total_clusters}")

            sample = [m['text'] for m in members[:10]]
            sample_str = '\n'.join(f"{i+1}. {o}" for i, o in enumerate(sample))

            # èªæ„è¦å‰‡åŒ¹é…
            cluster_texts = [m['full_text_with_context'] for m in members]
            try:
                cluster_center_response = self.client.embeddings.create(
                    model="text-embedding-3-small",
                    input=[' '.join(cluster_texts[:5])]
                )
                cluster_center_vector = np.array(cluster_center_response.data[0].embedding)
                
                matched_rules = self._match_rules_with_semantic_similarity(
                    obligation_text=sample_str,
                    obligation_vector=cluster_center_vector,
                    top_k=3,
                    threshold=0.5
                )
            except Exception as e:
                print(f"    âœ— èšé¡ {cluster_id} è¦å‰‡åŒ¹é…å¤±æ•—: {e}")
                matched_rules = []

            related_events = self._find_related_events([m['text'] for m in members])
            event_summary = ""
            if related_events:
                event_summary = f"\n\nç›¸é—œæ³•å¾‹äº‹ä»¶:\n"
                for event in related_events[:3]:
                    event_summary += f"- å‹•ä½œ:{event.action}, ä¸»é«”:{event.actor}, å®¢é«”:{event.patients}\n"
            
            # âš ï¸ v5 é—œéµ: æç¤ºè©éˆ - æ³¨å…¥åŒ¹é…çµæœ
            rules_injection = ""
            if matched_rules:
                best_rule_id, best_similarity = matched_rules[0]
                best_rule = self.rule_base[best_rule_id]
                rules_injection = f"""
    èªæ„è¦å‰‡åŒ¹é…çµæœ (ç³»çµ±å·²å®Œæˆåˆ†æ):
    ç¶“ç³»çµ±èªæ„å‘é‡åˆ†æ,æœ¬èšé¡èˆ‡ '{best_rule_id}: {best_rule.rule_name}' èªæ„æœ€ç‚ºç›¸é—œ (ç›¸ä¼¼åº¦: {best_similarity:.3f})ã€‚

    Top 3 åŒ¹é…è¦å‰‡:
    """
                for rule_id, similarity in matched_rules:
                    rule = self.rule_base[rule_id]
                    rules_injection += f"  - {rule_id}: {rule.rule_name} (ç›¸ä¼¼åº¦: {similarity:.3f}, é¡åˆ¥: {rule.category})\n"
                
                # âš ï¸ v5 é—œéµ: æ˜ç¢ºæŒ‡ç¤º LLM ä½¿ç”¨é€™äº›å€¼
                rules_injection += f"""
    âš ï¸ é‡è¦æŒ‡ç¤º:
    - è«‹åœ¨ evidence.decision_rule_id ä¸­ä½¿ç”¨: '{best_rule_id}'
    - è«‹åœ¨ evidence.decision_rule_name ä¸­ä½¿ç”¨: '{best_rule.rule_name}'
    - è«‹åœ¨ evidence.rule_similarity_score ä¸­ä½¿ç”¨: {best_similarity:.3f}
    """
            STANDARD_ACTIONS_LIST = [
                "DEFINE",           # å®šç¾©ã€ç¨±
                "INSTALL",          # è¨­ç½®ã€è£è¨­ã€é…ç½®ã€è¨­ç«‹
                "INSPECT",          # æª¢æŸ¥ã€æª¢é»ã€å·¡è¦–ã€ç›£æ¸¬ã€æ¸¬å®š
                "MAINTAIN",         # ç¶­è­·ã€ä¿®ç†ã€ä¿é¤Šã€è£œä¿®ã€æ±°æ›
                "PROVIDE",          # ä¾›æ‡‰ã€ç½®å‚™ã€æä¾›
                "EDUCATE",          # æ•™è‚²ã€è¨“ç·´ã€æŒ‡æ´¾ã€é¸ä»»
                "PROHIBIT",         # ç¦æ­¢ã€ä¸å¾—
                "OPERATE",          # æ“ä½œã€ä½¿ç”¨
                "DOCUMENT",         # è¨‚å®šã€è¨˜éŒ„ã€å ±å‘Šã€è¨ˆç•«
                "LABEL"             # æ¨™ç¤ºã€è­¦å‘Š
            ]

            prompt = f"""è«‹ç‚ºä»¥ä¸‹è·æ¥­å®‰å…¨è¡›ç”Ÿæ³•å¾‹ç¾©å‹™å‘½åã€åˆ†é¡ï¼Œä¸¦ã€æ­£è¦åŒ–å…¶æ ¸å¿ƒå‹•ä½œã€‘ã€‚

    ç¾©å‹™ç¯„ä¾‹(å…± {len(members)} æ¢,é¡¯ç¤ºå‰ {len(sample)} æ¢):
    {sample_str}
    {event_summary}
    {rules_injection}

    èšé¡å“è³ªè³‡è¨Š:
    - æˆå“¡æ•¸é‡: {len(members)}
    - å¹³å‡ç›¸ä¼¼åº¦: {quality.avg_intra_similarity:.2f}

    è«‹è¼¸å‡º JSON:
    {{
    "standard_name": "æ¨™æº–åŒ–åç¨±",
    "standard_code": "æ¨™æº–ä»£ç¢¼",
    "category": "ç¾©å‹™é¡åˆ¥",
    "standard_action": "è«‹å¾ã€æ¨™æº–å‹•ä½œåº«ã€‘ä¸­é¸æ“‡ä¸€å€‹æœ€èƒ½ä»£è¡¨æ­¤ç¾©å‹™æ ¸å¿ƒå‹•ä½œçš„æ¨™æº–è©",
    "description": "ç°¡çŸ­æè¿°",
    "evidence": {{
        "keywords_matched": ["é—œéµè©"],
        "decision_rule_id": "{matched_rules[0][0] if matched_rules else ''}",
        "decision_rule_name": "{self.rule_base[matched_rules[0][0]].rule_name if matched_rules else ''}",
        "rule_similarity_score": {matched_rules[0][1] if matched_rules else 0.0},
        "confidence_factors": {{"semantic_coherence": 0.8, "keyword_coverage": 0.7, "domain_specificity": 0.9}},
        "text_snippets": ["ä»£è¡¨æ€§æ–‡æœ¬"]
    }}
    }}

    åªè¼¸å‡ºJSONã€‚"""

            try:
                response = self.client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯è·æ¥­å®‰å…¨è¡›ç”Ÿæ³•å¾‹å°ˆå®¶ã€‚ä½ å¿…é ˆä½¿ç”¨ç³»çµ±æä¾›çš„èªæ„åŒ¹é…çµæœã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.2,
                    max_tokens=600
                )

                result_text = response.choices[0].message.content.strip()
                parsed = self._safe_parse_json_from_llm(result_text)

                if not isinstance(parsed, dict) or 'evidence' not in parsed:
                    raise ValueError("LLM å›å‚³ç¼ºå°‘ evidence çµæ§‹")

                evidence_data = parsed.get('evidence', {})
                
                # âš ï¸ v5 é—œéµ: é©—è­‰ä¸¦å¼·åˆ¶ä¿®æ­£
                if matched_rules:
                    expected_rule_id = matched_rules[0][0]
                    actual_rule_id = evidence_data.get('decision_rule_id', '')
                    
                    if actual_rule_id != expected_rule_id:
                        print(f"    âš ï¸ èšé¡ {cluster_id}: LLM æœªä½¿ç”¨æ³¨å…¥çš„ rule_id,å·²è‡ªå‹•ä¿®æ­£")
                        evidence_data['decision_rule_id'] = expected_rule_id
                        evidence_data['decision_rule_name'] = self.rule_base[expected_rule_id].rule_name
                        evidence_data['rule_similarity_score'] = matched_rules[0][1]
                
                evidence = StructuredEvidence(
                    keywords_matched=evidence_data.get('keywords_matched', []),
                    decision_rule_id=evidence_data.get('decision_rule_id', ''),
                    decision_rule_name=evidence_data.get('decision_rule_name', ''),
                    rule_similarity_score=evidence_data.get('rule_similarity_score', 0.0),
                    confidence_factors=evidence_data.get('confidence_factors', {}),
                    text_snippets=evidence_data.get('text_snippets', []),
                    extracted_events=evidence_data.get('extracted_events', [])
                )
                
                conf_factors = evidence.confidence_factors
                overall_confidence = np.mean([
                    conf_factors.get('semantic_coherence', 0.5),
                    conf_factors.get('keyword_coverage', 0.5),
                    conf_factors.get('domain_specificity', 0.5)
                ])
                
                # çµ±ä¸€å„ªå…ˆç´šé‚è¼¯ (ç¹¼æ‰¿ v4)
                temp_classification = ClassificationResult(
                    classification="Unknown",
                    confidence=overall_confidence,
                    evidence=evidence,
                    review_status=ReviewStatus.AUTO_APPROVED
                )
                
                cluster_info = {
                    'member_count': len(members),
                    'quality_metrics': asdict(quality),
                    'overall_confidence': overall_confidence
                }
                
                unified_priority = self._calculate_unified_priority(
                    cluster_info=cluster_info,
                    classification_result=temp_classification
                )
                
                if quality.needs_review or overall_confidence < self.CONFIDENCE_THRESHOLD or unified_priority in [ReviewPriority.CRITICAL, ReviewPriority.HIGH]:
                    review_status = ReviewStatus.PENDING_REVIEW
                else:
                    review_status = ReviewStatus.AUTO_APPROVED

                parsed['members'] = members
                parsed['member_count'] = len(members)
                parsed['sample'] = [m['text'] for m in members[:3]]
                parsed['quality_metrics'] = asdict(quality)
                parsed['evidence'] = asdict(evidence)
                parsed['overall_confidence'] = overall_confidence
                parsed['review_status'] = review_status.value
                parsed['review_priority'] = unified_priority.value

                named_clusters[cluster_id_int] = parsed

            except Exception as e:
                print(f"    âœ— èšé¡ {cluster_id} å‘½åå¤±æ•—: {e}")
                named_clusters[cluster_id_int] = self._create_fallback_cluster(cluster_id_int, members, quality)

        print(f"    âœ“ å®Œæˆ {len(named_clusters)} å€‹èšé¡çš„å‘½å")
        
        # v5 çµ±è¨ˆ
        rule_match_count = sum(1 for c in named_clusters.values() 
                            if isinstance(c, dict) and c.get('evidence', {}).get('decision_rule_id'))
        print(f"    â†’ v5 æ”¹é€²: {rule_match_count}/{len(named_clusters)} å€‹èšé¡æˆåŠŸåŒ¹é…è¦å‰‡")
        
        return named_clusters
    
    def _find_related_events(self, obligation_texts: List[str]) -> List[LegalEvent]:
        """æ‰¾å‡ºèˆ‡ç¾©å‹™ç›¸é—œçš„æ³•å¾‹äº‹ä»¶"""
        related = []
        for event in self.legal_events.values():
            for obl in obligation_texts[:5]:
                keywords = []
                
                if event.action and isinstance(event.action, str):
                    keywords.append(event.action)
                
                if event.actor and isinstance(event.actor, str):
                    keywords.append(event.actor)
                
                if event.patients and isinstance(event.patients, list):
                    for p in event.patients[:2]:
                        if p and isinstance(p, str):
                            keywords.append(p)
                
                if keywords and any(keyword in obl for keyword in keywords):
                    related.append(event)
                    break

        return related[:5]
    
    def _create_fallback_cluster(self, cluster_id: int, members: Any, quality: ClusterQualityMetrics) -> Dict:
        """å‰µå»ºå›é€€èšé¡"""
        if not isinstance(members, list):
            members = list(members) if hasattr(members, '__iter__') else []
        
        # ç¢ºä¿ members æ˜¯åŸå§‹æ–‡æœ¬
        member_texts = []
        for m in members:
            if isinstance(m, dict):
                member_texts.append(m.get('text', str(m)))
            else:
                member_texts.append(str(m))
        
        return {
            "standard_name": f"æœªåˆ†é¡ç¾©å‹™ç¾¤çµ„ {cluster_id}",
            "standard_code": f"OBLIGATION_CLUSTER_{cluster_id}",
            "category": "æœªåˆ†é¡",
            "description": "è‡ªå‹•å‘½åå¤±æ•—",
            "members": members,
            "member_count": len(members),
            "sample": member_texts[:3],
            "quality_metrics": asdict(quality),
            "evidence": asdict(StructuredEvidence(
                decision_rule_id="",
                decision_rule_name="è‡ªå‹•å‘½åå¤±æ•—,ä½¿ç”¨é è¨­å€¼"
            )),
            "overall_confidence": 0.0,
            "review_status": ReviewStatus.PENDING_REVIEW.value,
            "review_priority": ReviewPriority.HIGH.value
        }
    
    def _save_obligation_clusters(self):
        """
        (v5 å¯¦ä½œ) å„²å­˜ç¾©å‹™èšé¡çµæœ
        
        é€™å€‹å‡½å¼æœƒç”¢ç”Ÿ obligation_clusters.jsonã€‚
        å®ƒç¹¼æ‰¿è‡ª v4, ç¢ºä¿ v4 èšé¡å¾Œçš„è¤‡é›œ dict çµæ§‹è¢«æ­£ç¢ºåºåˆ—åŒ–ã€‚
        """
        if not self.obligation_clusters:
            print(f"  âš ï¸ è­¦å‘Š:æ²’æœ‰ç¾©å‹™èšé¡è³‡æ–™å¯å„²å­˜ (self.obligation_clusters ç‚ºç©º)")
            print(f"    â†’ å› æ­¤ obligation_clusters.json å°‡ä¸æœƒè¢«å»ºç«‹ã€‚")
            return
        
        output_path = os.path.join(self.output_dir, "obligation_clusters.json")
        
        save_data = {}
        for cluster_id, cluster_info in self.obligation_clusters.items():
            
            # v4 çš„ self.obligation_clusters[id]['members'] æ˜¯ä¸€å€‹ dict åˆ—è¡¨
            # æˆ‘å€‘åœ¨å„²å­˜æ™‚,åªå„²å­˜åŸå§‹æ–‡æœ¬ 'text',ä»¥ç°¡åŒ– JSON æª”æ¡ˆ
            members = cluster_info.get('members', [])
            if members and isinstance(members[0], dict):
                # åªä¿å­˜æ–‡æœ¬,ä¸ä¿å­˜å®Œæ•´çš„ dict
                members_text = [m.get('text', str(m)) for m in members]
            else:
                members_text = [str(m) for m in members]
            
            cluster_data = {
                'standard_name': cluster_info.get('standard_name'),
                'standard_code': cluster_info.get('standard_code'),
                'category': cluster_info.get('category'),
                'description': cluster_info.get('description'),
                'member_count': cluster_info.get('member_count'),
                'sample': cluster_info.get('sample', []), # sample å·²ç¶“æ˜¯ç´”æ–‡æœ¬
                'quality_metrics': cluster_info.get('quality_metrics'),
                'evidence': cluster_info.get('evidence'),
                'overall_confidence': cluster_info.get('overall_confidence'),
                'review_status': cluster_info.get('review_status'),
                'review_priority': cluster_info.get('review_priority'),
                # ç‚ºäº†æª”æ¡ˆå¤§å°,æˆ‘å€‘åªå„²å­˜æœ€å¤š 50 å€‹æˆå“¡
                'members': members_text[:50] if len(members_text) > 50 else members_text
            }
            save_data[str(cluster_id)] = cluster_data
        
        # ä½¿ç”¨ v4 çš„ _make_json_serializable ç¢ºä¿ Enum, numpy ç­‰é¡å‹å¯è¢«å„²å­˜
        serializable_data = self._make_json_serializable(save_data)
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(serializable_data, f, ensure_ascii=False, indent=2)
            
            print(f"  âœ“ ç¾©å‹™èšé¡å·²å„²å­˜è‡³ {output_path}")
            print(f"    â†’ ç¸½å…± {len(save_data)} å€‹èšé¡")
        except Exception as e:
            print(f"  âœ— éŒ¯èª¤: å„²å­˜ obligation_clusters.json å¤±æ•—: {e}")
    
    def _queue_problematic_clusters_v5(self):
        """
        v4 ç‰ˆæœ¬: å°‡å•é¡Œèšé¡åŠ å…¥å¯©æ ¸ä½‡åˆ—
        ä½¿ç”¨çµ±ä¸€çš„å„ªå…ˆç´šé‚è¼¯
        """
        if not self.problematic_clusters:
            print("  âœ“ æ‰€æœ‰èšé¡å“è³ªè‰¯å¥½,ç„¡éœ€å¯©æ ¸")
            return
        
        # === v4 æ ¸å¿ƒæ”¹é€²: é‡æ–°è¨ˆç®—çµ±ä¸€å„ªå…ˆç´š ===
        for cluster_item in self.problematic_clusters:
            cluster_id = cluster_item['cluster_id']
            if str(cluster_id) in self.obligation_clusters:
                cluster_info = self.obligation_clusters[str(cluster_id)]
                
                # ä½¿ç”¨çµ±ä¸€å„ªå…ˆç´šè¨ˆç®—
                priority = self._calculate_unified_priority(
                    cluster_info=cluster_info,
                    classification_result=None
                )
                
                # è¨ˆç®—ä¸»å‹•å­¸ç¿’åˆ†æ•¸(ç”¨æ–¼è©³ç´°è¨˜éŒ„)
                al_score = self._calculate_active_learning_score_from_cluster(cluster_info)
                
                cluster_item['active_learning_score'] = asdict(al_score)
                cluster_item['review_priority'] = priority.value
        
        # æŒ‰å„ªå…ˆç´šæ’åº
        priority_order = {
            ReviewPriority.CRITICAL.value: 0,
            ReviewPriority.HIGH.value: 1,
            ReviewPriority.MEDIUM.value: 2,
            ReviewPriority.LOW.value: 3
        }
        
        self.problematic_clusters.sort(
            key=lambda x: (
                priority_order.get(x.get('review_priority', 'low'), 999),
                -x.get('active_learning_score', {}).get('total_priority', 0)
            )
        )
        
        queue_file = os.path.join(self.review_queue_dir, "problematic_clusters_queue.json")
        
        queue_data = {
            "generated_at": datetime.now().isoformat(),
            "total_problematic": len(self.problematic_clusters),
            "priority_distribution": self._get_priority_distribution(self.problematic_clusters),
            "review_instructions": "è«‹äººå·¥å¯©æ ¸ä»¥ä¸‹èšé¡,è€ƒæ…®åˆä½µæˆ–æ‹†åˆ†ã€‚å„ªå…ˆè™•ç† CRITICAL å’Œ HIGH é …ç›®ã€‚",
            "clusters": self.problematic_clusters
        }
        
        serializable_queue = self._make_json_serializable(queue_data)
        
        with open(queue_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_queue, f, ensure_ascii=False, indent=2)
        
        print(f"  âš ï¸ å·²å°‡ {len(self.problematic_clusters)} å€‹å•é¡Œèšé¡åŠ å…¥å¯©æ ¸ä½‡åˆ—")
        print(f"  â†’ å„ªå…ˆç´šåˆ†å¸ƒ: {queue_data['priority_distribution']}")
        print(f"  â†’ å¯©æ ¸ä½‡åˆ—æª”æ¡ˆ: {queue_file}")
    
    def _get_priority_distribution(self, items: List[Dict]) -> Dict[str, int]:
        """çµ±è¨ˆå„ªå…ˆç´šåˆ†å¸ƒ"""
        dist = defaultdict(int)
        for item in items:
            priority = item.get('review_priority', 'low')
            dist[priority] += 1
        return dict(dist)
    
    # ========================================================================
    # éšæ®µå››: é¢¨éšªæ§åˆ¶å±¤ç´šåˆ†é¡ (v4 å¢å¼·ç‰ˆ)
    # ========================================================================
    
    def classify_control_types(self):
        """
        åˆ†é¡ç¾©å‹™åˆ°é¢¨éšªæ§åˆ¶å±¤ç´š (v5 ç‰ˆæœ¬)
        
        ä¿®æ”¹é»:
        - åœ¨ prompt ä¸­æ³¨å…¥èªæ„åŒ¹é…çµæœ (æç¤ºè©éˆ)
        - é©—è­‰ä¸¦å¼·åˆ¶ä¿®æ­£ LLM çš„ rule_id
        """
        print("\nğŸ¯ åˆ†é¡é¢¨éšªæ§åˆ¶å±¤ç´š (v5 æç¤ºè©éˆ)...")
        
        if not self.obligation_clusters:
            print("  âš ï¸ ç„¡ç¾©å‹™èšé¡è³‡æ–™,è·³éæ§åˆ¶é¡å‹åˆ†é¡")
            return
        
        total = len(self.obligation_clusters)
        
        for idx, (cluster_id, cluster_info) in enumerate(self.obligation_clusters.items(), 1):
            cluster_id = str(cluster_id)
            
            if not isinstance(cluster_info, dict):
                continue
            
            if idx % 10 == 0 or idx == total:
                print(f"  â†’ é€²åº¦: {idx}/{total}")
            
            members = cluster_info.get('members', [])
            if not members:
                continue
            
            if isinstance(members[0], dict):
                sample_obligations = [m.get('text', str(m)) for m in members[:5]]
            else:
                sample_obligations = [str(m) for m in members[:5]]
            sample_str = '\n'.join(sample_obligations)
            
            # èªæ„è¦å‰‡åŒ¹é… (ç¹¼æ‰¿ v4)
            try:
                response = self.client.embeddings.create(
                    model="text-embedding-3-small",
                    input=[sample_str]
                )
                cluster_vector = np.array(response.data[0].embedding)
                
                matched_rules = self._match_rules_with_semantic_similarity(
                    obligation_text=sample_str,
                    obligation_vector=cluster_vector,
                    top_k=3,
                    threshold=0.5
                )
            except Exception as e:
                print(f"    âœ— èšé¡ {cluster_id} å‘é‡ç”Ÿæˆå¤±æ•—: {e}")
                matched_rules = []
            
            # âš ï¸ v5 é—œéµ: æç¤ºè©éˆ - æ³¨å…¥åŒ¹é…çµæœ
            rules_injection = ""
            if matched_rules:
                best_rule_id, best_similarity = matched_rules[0]
                best_rule = self.rule_base[best_rule_id]
                rules_injection = f"""
    èªæ„è¦å‰‡åŒ¹é…çµæœ (ç³»çµ±å·²å®Œæˆåˆ†æ):
    ç¶“ç³»çµ±èªæ„å‘é‡åˆ†æ,æœ¬ç¾©å‹™èšé¡èˆ‡ '{best_rule_id}: {best_rule.rule_name}' èªæ„æœ€ç‚ºç›¸é—œ (ç›¸ä¼¼åº¦: {best_similarity:.3f})ã€‚
    å»ºè­°æ§åˆ¶é¡å‹: {best_rule.category}

    Top 3 åŒ¹é…è¦å‰‡:
    """
                for rule_id, similarity in matched_rules:
                    rule = self.rule_base[rule_id]
                    rules_injection += f"  - {rule_id}: {rule.rule_name} (ç›¸ä¼¼åº¦: {similarity:.3f}, é¡åˆ¥: {rule.category})\n"
                
                # âš ï¸ v5 é—œéµ: æ˜ç¢ºæŒ‡ç¤º LLM
                rules_injection += f"""
    âš ï¸ é‡è¦æŒ‡ç¤º:
    - è«‹åœ¨ control_type ä¸­ä½¿ç”¨: '{best_rule.category}'
    - è«‹åœ¨ evidence.decision_rule_id ä¸­ä½¿ç”¨: '{best_rule_id}'
    - è«‹åœ¨ evidence.decision_rule_name ä¸­ä½¿ç”¨: '{best_rule.rule_name}'
    - è«‹åœ¨ evidence.rule_similarity_score ä¸­ä½¿ç”¨: {best_similarity:.3f}
    """

            prompt = f"""è«‹å°‡ä»¥ä¸‹è·æ¥­å®‰å…¨è¡›ç”Ÿç¾©å‹™åˆ†é¡åˆ°é¢¨éšªæ§åˆ¶å±¤ç´š(Hierarchy of Controls)ã€‚

    ç¾©å‹™ç¯„ä¾‹(å…± {cluster_info.get('member_count', len(members))} æ¢):
    {sample_str}
    {rules_injection}

    é¢¨éšªæ§åˆ¶å±¤ç´š(æŒ‰å„ªå…ˆé †åº):
    1. Elimination - æ¶ˆé™¤å±å®³
    2. Substitution - æ›¿ä»£
    3. EngineeringControl - å·¥ç¨‹æ§åˆ¶
    4. AdministrativeControl - ç®¡ç†æ§åˆ¶
    5. PPE - å€‹äººé˜²è­·å…·

    è¼¸å‡º JSON:
    {{
    "control_type": "{best_rule.category if matched_rules else 'AdministrativeControl'}",
    "evidence": {{
        "keywords_matched": ["é—œéµè©"],
        "decision_rule_id": "{matched_rules[0][0] if matched_rules else ''}",
        "decision_rule_name": "{self.rule_base[matched_rules[0][0]].rule_name if matched_rules else ''}",
        "rule_similarity_score": {matched_rules[0][1] if matched_rules else 0.0},
        "confidence_factors": {{"keyword_match_strength": 0.8, "context_clarity": 0.7, "domain_alignment": 0.9}},
        "text_snippets": ["è­‰æ“šæ–‡æœ¬"]
    }}
    }}

    åªè¼¸å‡ºJSONã€‚"""

            try:
                response = self.client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯è·æ¥­å®‰å…¨è¡›ç”Ÿé¢¨éšªæ§åˆ¶å°ˆå®¶ã€‚ä½ å¿…é ˆä½¿ç”¨ç³»çµ±æä¾›çš„èªæ„åŒ¹é…çµæœã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.1,
                    max_tokens=500
                )
                
                result_text = response.choices[0].message.content.strip()
                parsed = self._safe_parse_json_from_llm(result_text)

                if not isinstance(parsed, dict) or 'evidence' not in parsed:
                    raise ValueError("åˆ†é¡çµæœç¼ºå°‘ evidence çµæ§‹")
                
                evidence_data = parsed.get('evidence', {})
                
                # âš ï¸ v5 é—œéµ: é©—è­‰ä¸¦å¼·åˆ¶ä¿®æ­£
                if matched_rules:
                    expected_rule_id = matched_rules[0][0]
                    actual_rule_id = evidence_data.get('decision_rule_id', '')
                    
                    if actual_rule_id != expected_rule_id:
                        print(f"    âš ï¸ èšé¡ {cluster_id}: LLM æœªä½¿ç”¨æ³¨å…¥çš„ rule_id,å·²è‡ªå‹•ä¿®æ­£")
                        evidence_data['decision_rule_id'] = expected_rule_id
                        evidence_data['decision_rule_name'] = self.rule_base[expected_rule_id].rule_name
                        evidence_data['rule_similarity_score'] = matched_rules[0][1]
                
                # ç¹¼çºŒè™•ç†... (èˆ‡ v4 ç›¸åŒ,å‰µå»º ClassificationResult ç­‰)
                evidence = StructuredEvidence(
                    keywords_matched=evidence_data.get('keywords_matched', []),
                    decision_rule_id=evidence_data.get('decision_rule_id', ''),
                    decision_rule_name=evidence_data.get('decision_rule_name', ''),
                    rule_similarity_score=evidence_data.get('rule_similarity_score', 0.0),
                    confidence_factors=evidence_data.get('confidence_factors', {}),
                    alternative_classifications=evidence_data.get('alternative_classifications', []),
                    text_snippets=evidence_data.get('text_snippets', [])
                )
                
                conf_factors = evidence.confidence_factors
                overall_confidence = np.mean([
                    conf_factors.get('keyword_match_strength', 0.5),
                    conf_factors.get('context_clarity', 0.5),
                    conf_factors.get('domain_alignment', 0.5)
                ])
                
                temp_classification = ClassificationResult(
                    classification=parsed.get('control_type', 'Unknown'),
                    confidence=overall_confidence,
                    evidence=evidence,
                    review_status=ReviewStatus.AUTO_APPROVED
                )
                
                unified_priority = self._calculate_unified_priority(
                    cluster_info=cluster_info,
                    classification_result=temp_classification
                )
                
                if overall_confidence < self.CONFIDENCE_THRESHOLD or unified_priority in [ReviewPriority.CRITICAL, ReviewPriority.HIGH]:
                    review_status = ReviewStatus.PENDING_REVIEW
                    
                    al_score = self._calculate_active_learning_score(
                        cluster_info=cluster_info,
                        classification_result=temp_classification
                    )
                    
                    self.low_confidence_classifications.append({
                        'cluster_id': cluster_id,
                        'control_type': parsed.get('control_type'),
                        'confidence': overall_confidence,
                        'evidence': asdict(evidence),
                        'sample_obligations': sample_obligations[:3],
                        'active_learning_score': asdict(al_score),
                        'review_priority': unified_priority.value
                    })
                else:
                    review_status = ReviewStatus.AUTO_APPROVED
                
                classification_result = ClassificationResult(
                    classification=parsed.get('control_type', 'Unknown'),
                    confidence=overall_confidence,
                    evidence=evidence,
                    review_status=review_status,
                    review_priority=unified_priority
                )
                
                result_dict = asdict(classification_result)
                
                self.control_type_mapping[cluster_id] = result_dict
                cluster_info['control_type_classification'] = result_dict
                
            except Exception as e:
                print(f"    âœ— èšé¡ {cluster_id} åˆ†é¡å¤±æ•—: {e}")
                # å›é€€é‚è¼¯ (èˆ‡ v4 ç›¸åŒ)
                fallback_classification = ClassificationResult(
                    classification="AdministrativeControl",
                    confidence=0.0,
                    evidence=StructuredEvidence(
                        decision_rule_id="",
                        decision_rule_name="åˆ†é¡å¤±æ•—,ä½¿ç”¨é è¨­å€¼"
                    ),
                    review_status=ReviewStatus.PENDING_REVIEW,
                    review_priority=ReviewPriority.HIGH
                )
                
                self.control_type_mapping[cluster_id] = asdict(fallback_classification)
                cluster_info['control_type_classification'] = asdict(fallback_classification)
        
        # çµ±è¨ˆ (èˆ‡ v4 ç›¸åŒ)
        type_counts = defaultdict(int)
        review_needed = 0
        priority_counts = defaultdict(int)
        
        for mapping in self.control_type_mapping.values():
            if isinstance(mapping, dict):
                ct = mapping.get('classification', 'Unknown')
                type_counts[ct] += 1
                if mapping.get('review_status') == ReviewStatus.PENDING_REVIEW.value:
                    review_needed += 1
                priority = mapping.get('review_priority', 'low')
                priority_counts[priority] += 1
        
        if type_counts:
            print(f"\n  âœ“ æ§åˆ¶é¡å‹åˆ†é¡å®Œæˆ:")
            for ct, count in sorted(type_counts.items(), key=lambda x: -x[1]):
                print(f"    - {ct}: {count} å€‹ç¾¤çµ„")
            print(f"  âš ï¸ å…¶ä¸­ {review_needed} å€‹éœ€è¦äººå·¥å¯©æ ¸")
            print(f"  â†’ å„ªå…ˆç´šåˆ†å¸ƒ: {dict(priority_counts)}")
        
        self._save_control_type_mapping()
        self._queue_low_confidence_classifications_v5()
    
    def _save_control_type_mapping(self):
        """å„²å­˜æ§åˆ¶é¡å‹æ˜ å°„"""
        if not self.control_type_mapping:
            print(f"  âš ï¸ è­¦å‘Š: æ²’æœ‰æ§åˆ¶é¡å‹æ˜ å°„è³‡æ–™å¯å„²å­˜ (self.control_type_mapping ç‚ºç©º)")
            print(f"    â†’ å› æ­¤ control_type_mapping.json å°‡ä¸æœƒè¢«å»ºç«‹ã€‚")
            return
            
        output_path = os.path.join(self.output_dir, "control_type_mapping.json")
        
        serializable_mapping = self._make_json_serializable(self.control_type_mapping)
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(serializable_mapping, f, ensure_ascii=False, indent=2)
            print(f"  âœ“ æ§åˆ¶é¡å‹æ˜ å°„å·²å„²å­˜è‡³ {output_path}")
        except Exception as e:
            print(f"  âœ— éŒ¯èª¤: å„²å­˜ control_type_mapping.json å¤±æ•—: {e}")

    def _queue_low_confidence_classifications_v5(self):
        """
        v4 ç‰ˆæœ¬: å°‡ä½ä¿¡å¿ƒåˆ†é¡åŠ å…¥å¯©æ ¸ä½‡åˆ—
        ä½¿ç”¨çµ±ä¸€çš„å„ªå…ˆç´šé‚è¼¯
        """
        if not self.low_confidence_classifications:
            print("  âœ“ æ‰€æœ‰åˆ†é¡ä¿¡å¿ƒåº¦è‰¯å¥½,ç„¡éœ€å¯©æ ¸")
            return
        
        # æŒ‰å„ªå…ˆç´šæ’åº
        priority_order = {
            ReviewPriority.CRITICAL.value: 0,
            ReviewPriority.HIGH.value: 1,
            ReviewPriority.MEDIUM.value: 2,
            ReviewPriority.LOW.value: 3
        }
        
        self.low_confidence_classifications.sort(
            key=lambda x: (
                priority_order.get(x.get('review_priority', 'low'), 999),
                -x.get('active_learning_score', {}).get('total_priority', 0),
                x.get('confidence', 0)
            )
        )
        
        queue_file = os.path.join(self.review_queue_dir, "low_confidence_classifications_queue.json")
        
        queue_data = {
            "generated_at": datetime.now().isoformat(),
            "total_low_confidence": len(self.low_confidence_classifications),
            "confidence_threshold": self.CONFIDENCE_THRESHOLD,
            "priority_distribution": self._get_priority_distribution(self.low_confidence_classifications),
            "review_instructions": "è«‹äººå·¥å¯©æ ¸ä»¥ä¸‹ä½ä¿¡å¿ƒåº¦åˆ†é¡,ç¢ºèªæˆ–ä¿®æ­£æ§åˆ¶é¡å‹ã€‚å„ªå…ˆè™•ç† CRITICAL å’Œ HIGH é …ç›®ã€‚",
            "classifications": self.low_confidence_classifications
        }
        
        serializable_queue = self._make_json_serializable(queue_data)
        
        with open(queue_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_queue, f, ensure_ascii=False, indent=2)
        
        print(f"  âš ï¸ å·²å°‡ {len(self.low_confidence_classifications)} å€‹ä½ä¿¡å¿ƒåˆ†é¡åŠ å…¥å¯©æ ¸ä½‡åˆ—")
        print(f"  â†’ å„ªå…ˆç´šåˆ†å¸ƒ: {queue_data['priority_distribution']}")
        print(f"  â†’ å¯©æ ¸ä½‡åˆ—æª”æ¡ˆ: {queue_file}")
    
    # ========================================================================
    # è¼”åŠ©æ–¹æ³• (ç¹¼æ‰¿ v3)
    # ========================================================================
    
    def _get_embeddings(self, texts: List[str], batch_size: int = 100) -> np.ndarray:
        """ç²å–æ–‡æœ¬çš„ Embeddings"""
        
        if not texts:
            print("    âš ï¸ è­¦å‘Š:æ–‡æœ¬åˆ—è¡¨ç‚ºç©º,è¿”å›ç©ºå‘é‡")
            return np.array([])
        
        all_embeddings = []
        failed_count = 0
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            
            try:
                response = self.client.embeddings.create(
                    model="text-embedding-3-small",
                    input=batch
                )
                
                batch_embeddings = [item.embedding for item in response.data]
                all_embeddings.extend(batch_embeddings)
                
                if i % 200 == 0 or i + batch_size >= len(texts):
                    print(f"    â†’ Embeddings é€²åº¦: {min(i + batch_size, len(texts))}/{len(texts)}")
                
            except Exception as e:
                print(f"    âœ— æ‰¹æ¬¡ {i//batch_size + 1} å¤±æ•—: {e}")
                failed_count += len(batch)
                all_embeddings.extend([np.zeros(1536).tolist() for _ in batch])
        
        if failed_count > 0:
            print(f"    âš ï¸ æœ‰ {failed_count} å€‹æ–‡æœ¬çš„ embedding ç”Ÿæˆå¤±æ•—,ä½¿ç”¨é›¶å‘é‡æ›¿ä»£")
        
        result = np.array(all_embeddings)
        print(f"    âœ“ å‘é‡ç”Ÿæˆå®Œæˆ,å½¢ç‹€: {result.shape}")
        
        return result
    
    def _safe_parse_json_from_llm(self, text: str) -> Any:
        """å¾ LLM å›æ‡‰æ–‡å­—ä¸­å®‰å…¨è§£æ JSON"""
        if not text or not isinstance(text, str):
            return None

        cleaned = re.sub(r'```(?:json)?', '', text, flags=re.IGNORECASE).strip()

        candidates = []
        obj_matches = re.findall(r'\{[\s\S]*\}', cleaned)
        arr_matches = re.findall(r'\[[\s\S]*\]', cleaned)
        if obj_matches:
            candidates.extend(obj_matches)
        if arr_matches:
            candidates.extend(arr_matches)
        if not candidates:
            candidates = [cleaned]

        for candidate in candidates:
            cand = candidate.strip()
            cand = re.sub(r',\s*([\]\}])', r'\1', cand)

            try:
                return json.loads(cand)
            except Exception:
                pass

            try:
                cand2 = cand
                if "'" in cand2 and '"' not in cand2:
                    cand2 = cand2.replace("'", '"')
                    return json.loads(cand2)
            except Exception:
                pass

            try:
                return ast.literal_eval(cand)
            except Exception:
                pass

        return None
    
    def _make_json_serializable(self, obj):
        """éè¿´è½‰æ›ç‰©ä»¶ç‚º JSON å¯åºåˆ—åŒ–æ ¼å¼"""
        if isinstance(obj, dict):
            return {key: self._make_json_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._make_json_serializable(item) for item in obj]
        elif isinstance(obj, tuple):
            return tuple(self._make_json_serializable(item) for item in obj)
        elif isinstance(obj, Enum):
            return obj.value
        elif isinstance(obj, (np.integer, np.floating)):
            return obj.item()
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif hasattr(obj, '__dict__') and not isinstance(obj, type):
            return self._make_json_serializable(asdict(obj) if hasattr(obj, '__dataclass_fields__') else obj.__dict__)
        else:
            return obj
    
    def _save_ontology(self):
        """å„²å­˜æœ¬é«”çµæ§‹"""
        ontology_data = {
            "subject_ontology": self.subject_ontology,
            "object_ontology": self.object_ontology,
            "metadata": {
                "subject_count": len(self.subject_ontology),
                "object_count": len(self.object_ontology),
                "discovered_entities_count": len(self.discovered_entities)
            }
        }
        
        output_path = os.path.join(self.output_dir, "ontology.json")
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(ontology_data, f, ensure_ascii=False, indent=2)
        print(f"  âœ“ æœ¬é«”å·²å„²å­˜è‡³ {output_path}")
    
    # ========================================================================
    # éšæ®µäº”: å»ºæ§‹çŸ¥è­˜åœ–è­œ (v5 å¯¦ä½œ - å–ä»£ v4 çš„ stub)
    # ========================================================================
    
    def build_knowledge_graph(self):
        """
        (v5 å¯¦ä½œ) å»ºæ§‹å®Œæ•´çš„çŸ¥è­˜åœ–è­œ
        
        é€™å€‹å‡½å¼æœƒå‘¼å«æ‰€æœ‰è¼”åŠ©å‡½å¼ä¾†çµ„è£ä¸¦å„²å­˜ legal_kg.jsonã€‚
        å®ƒå–ä»£äº† v4 ä¸­è¢«çœç•¥çš„å¯¦ä½œã€‚
        """
        print("\nğŸ•¸ï¸ å»ºæ§‹çŸ¥è­˜åœ–è­œ (v5 å¯¦ä½œ)...")
        
        # 1. æ³•å¾‹çµæ§‹å±¤ (Law -> Chapter -> Section -> Article)
        self._build_legal_structure()
        
        # 2. èªæ„å±¤ (NormalizedObligation -> ControlType)
        if self.obligation_clusters:
            self._build_semantic_layer_v5()
        else:
            print("  âš ï¸ è·³éèªæ„å±¤å»ºæ§‹(ç„¡ç¾©å‹™èšé¡è³‡æ–™)")
        
        # 3. æœ¬é«”å±¤ (SubjectEntity, ObjectEntity)
        self._build_ontology_layer()
        
        # 4. äº‹ä»¶å±¤ (LegalEvent -> Entities)
        if self.legal_events:
            self._build_event_layer()
        
        # 5. è¦å‰‡å±¤ (DecisionRule -> ControlType)
        self._build_rule_layer_v5()
        
        # å„²å­˜åœ–è­œ
        self._save_knowledge_graph_v5()

    def _build_legal_structure(self):
        """(v5 å¯¦ä½œ) å»ºç«‹æ³•å¾‹çµæ§‹å±¤ (ç¹¼æ‰¿è‡ª v3)"""
        print("  â†’ å»ºç«‹æ³•å¾‹çµæ§‹å±¤...")
        
        law_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))
        
        for doc in self.documents:
            metadata = doc.get('metadata', {})
            law_name = metadata.get('law_name', 'Unknown')
            chapter = metadata.get('chapter', 'unknown')
            section = metadata.get('section', 'unknown')
            article = metadata.get('article', 'unknown')
            content = doc.get('content', '')
            
            law_dict[law_name][chapter][section].append({
                'article': article,
                'content': content,
                'metadata': metadata
            })
        
        for law_name, chapters in law_dict.items():
            law_node_id = f"law_{self._sanitize_id(law_name)}"
            self.nodes.append(GraphNode(
                id=law_node_id,
                type="Law",
                properties={"name": law_name}
            ))
            
            for chapter_id, sections in chapters.items():
                chapter_node_id = f"{law_node_id}_ch_{chapter_id}"
                self.nodes.append(GraphNode(
                    id=chapter_node_id,
                    type="Chapter",
                    properties={"id": chapter_id, "law": law_name}
                ))
                self.edges.append(GraphEdge(
                    source=law_node_id,
                    target=chapter_node_id,
                    type="HAS_CHAPTER"
                ))
                
                for section_id, articles in sections.items():
                    section_node_id = f"{chapter_node_id}_sec_{section_id}"
                    self.nodes.append(GraphNode(
                        id=section_node_id,
                        type="Section",
                        properties={"id": section_id}
                    ))
                    self.edges.append(GraphEdge(
                        source=chapter_node_id,
                        target=section_node_id,
                        type="HAS_SECTION"
                    ))
                    
                    for article_data in articles:
                        article_id = article_data['article']
                        article_node_id = f"{section_node_id}_art_{article_id}"
                        self.nodes.append(GraphNode(
                            id=article_node_id,
                            type="Article",
                            properties={
                                "id": article_id,
                                "content": article_data['content']
                            }
                        ))
                        self.edges.append(GraphEdge(
                            source=section_node_id,
                            target=article_node_id,
                            type="HAS_ARTICLE"
                        ))
        
        print(f"    âœ“ æ³•å¾‹çµæ§‹å±¤:{len([n for n in self.nodes if n.type in ['Law', 'Chapter', 'Section', 'Article']])} å€‹ç¯€é»")

    def _build_semantic_layer_v5(self):
        """
        (v5 å¯¦ä½œ) å»ºç«‹èªæ„å±¤ (v3 çš„å‡ç´šç‰ˆ)
        
        - å¢å¼·: å„²å­˜ v4 ç”¢ç”Ÿçš„ review_priority
        - å¢å¼·: å„²å­˜ v4 ç”¢ç”Ÿçš„ rule_similarity_score
        """
        print("  â†’ å»ºç«‹èªæ„å±¤ (v5)...")
        
        if not self.obligation_clusters:
            print("    âš ï¸ ç„¡ç¾©å‹™èšé¡è³‡æ–™")
            return
        
        for cluster_id, cluster_info in self.obligation_clusters.items():
            cluster_id_str = str(cluster_id)
            
            if not isinstance(cluster_info, dict):
                continue
            
            standard_code = cluster_info.get('standard_code', f'CLUSTER_{cluster_id_str}')
            obligation_node_id = f"obligation_{standard_code}"
            
            evidence = cluster_info.get('evidence', {})
            control_classification = cluster_info.get('control_type_classification', {})
            
            self.nodes.append(GraphNode(
                id=obligation_node_id,
                type="NormalizedObligation",
                properties={
                    "name": cluster_info.get('standard_name', 'æœªå‘½åç¾©å‹™'),
                    "code": standard_code,
                    "category": cluster_info.get('category', 'æœªåˆ†é¡'),
                    "description": cluster_info.get('description', ''),
                    "member_count": cluster_info.get('member_count', 0),
                    "overall_confidence": cluster_info.get('overall_confidence', 0.0),
                    "review_status": cluster_info.get('review_status', 'unknown'),
                    "review_priority": cluster_info.get('review_priority', 'low'), # v5 æ–°å¢
                    "evidence_keywords": evidence.get('keywords_matched', []),
                    "evidence_rule_id": evidence.get('decision_rule_id', ''),
                    "evidence_rule_name": evidence.get('decision_rule_name', ''),
                    "evidence_rule_similarity": evidence.get('rule_similarity_score', 0.0), # v5 æ–°å¢
                    "quality_metrics": cluster_info.get('quality_metrics', {})
                }
            ))
            
            # é€£æ¥åˆ° ControlType
            if control_classification and isinstance(control_classification, dict):
                control_type = control_classification.get('classification', 'Unknown')
                control_node_id = f"control_{control_type}"
                
                # ç¢ºä¿ ControlType ç¯€é»åªå‰µå»ºä¸€æ¬¡
                if not any(n.id == control_node_id for n in self.nodes):
                    self.nodes.append(GraphNode(
                        id=control_node_id,
                        type="ControlType",
                        properties={"type": control_type}
                    ))
                
                control_evidence = control_classification.get('evidence', {})
                self.edges.append(GraphEdge(
                    source=obligation_node_id,
                    target=control_node_id,
                    type="IS_A",
                    properties={
                        "confidence": control_classification.get('confidence', 0.0),
                        "review_status": control_classification.get('review_status', 'unknown'),
                        "review_priority": control_classification.get('review_priority', 'low'), # v5 æ–°å¢
                        "evidence_keywords": control_evidence.get('keywords_matched', []),
                        "evidence_rule_id": control_evidence.get('decision_rule_id', ''),
                        "evidence_rule_similarity": control_evidence.get('rule_similarity_score', 0.0) # v5 æ–°å¢
                    }
                ))
        
        semantic_nodes = len([n for n in self.nodes if n.type in ['NormalizedObligation', 'ControlType']])
        print(f"    âœ“ èªæ„å±¤:{semantic_nodes} å€‹ç¯€é»")

    def _build_ontology_layer(self):
        """(v5 å¯¦ä½œ) å°‡æœ¬é«”çµæ§‹åŠ å…¥çŸ¥è­˜åœ–è­œ (ç¹¼æ‰¿è‡ª v3)"""
        print("  â†’ å»ºç«‹æœ¬é«”å±¤...")
        
        ontology_node_count = 0
        
        for entity_name, entity_info in self.subject_ontology.items():
            node_id = f"subject_{entity_info.get('standard_name', self._sanitize_id(entity_name))}"
            
            # é¿å…é‡è¤‡æ·»åŠ 
            if any(n.id == node_id for n in self.nodes): continue
            
            self.nodes.append(GraphNode(
                id=node_id,
                type="SubjectEntity",
                properties={
                    "name": entity_name,
                    "standard_name": entity_info.get('standard_name'),
                    "parent_category": entity_info.get('parent_category'),
                    "level": entity_info.get('level'),
                    "hierarchy_path": entity_info.get('hierarchy_path'),
                    "synonyms": entity_info.get('synonyms', []),
                    "description": entity_info.get('description')
                }
            ))
            ontology_node_count += 1
            
            parent = entity_info.get('parent_category')
            if parent:
                parent_id = f"subject_{parent}"
                if not any(n.id == parent_id for n in self.nodes):
                    self.nodes.append(GraphNode(
                        id=parent_id,
                        type="SubjectCategory",
                        properties={"name": parent}
                    ))
                
                self.edges.append(GraphEdge(
                    source=node_id,
                    target=parent_id,
                    type="IS_A"
                ))
        
        for entity_name, entity_info in self.object_ontology.items():
            node_id = f"object_{entity_info.get('standard_name', self._sanitize_id(entity_name))}"
            
            # é¿å…é‡è¤‡æ·»åŠ 
            if any(n.id == node_id for n in self.nodes): continue
            
            self.nodes.append(GraphNode(
                id=node_id,
                type="ObjectEntity",
                properties={
                    "name": entity_name,
                    "standard_name": entity_info.get('standard_name'),
                    "parent_category": entity_info.get('parent_category'),
                    "level": entity_info.get('level'),
                    "hierarchy_path": entity_info.get('hierarchy_path'),
                    "synonyms": entity_info.get('synonyms', []),
                    "description": entity_info.get('description')
                }
            ))
            ontology_node_count += 1
            
            parent = entity_info.get('parent_category')
            if parent:
                parent_id = f"object_{parent}"
                if not any(n.id == parent_id for n in self.nodes):
                    self.nodes.append(GraphNode(
                        id=parent_id,
                        type="ObjectCategory",
                        properties={"name": parent}
                    ))
                
                self.edges.append(GraphEdge(
                    source=node_id,
                    target=parent_id,
                    type="IS_A"
                ))
        
        print(f"    âœ“ æœ¬é«”å±¤:{ontology_node_count} å€‹å¯¦é«”ç¯€é»")

    def _build_event_layer(self):
        """(v5 å¯¦ä½œ) å»ºç«‹äº‹ä»¶å±¤
        
        é—œéµä¿®æ­£:
        - å‘¼å« _find_ontology_node_semantic (èªæ„åŒ¹é…)
        - å–ä»£ v4 çš„ _find_ontology_node_for_entity (å­—ä¸²åŒ¹é…)
        - å¢åŠ èªæ„é€£çµçµ±è¨ˆ
        """
        print("  â†’ å»ºç«‹äº‹ä»¶å±¤ (v5 èªæ„é€£çµç‰ˆ)...")
        
        event_count = 0
        semantic_links_created = 0
        unlinked_entities = set() # ç”¨æ–¼è¿½è¹¤ç„¡æ³•é€£çµçš„å¯¦é«”
        
        for event_id, event in self.legal_events.items():
            event_node_id = event.event_id 
            
            if any(n.id == event_node_id for n in self.nodes): continue
            
            self.nodes.append(GraphNode(
                id=event_node_id,
                type="LegalEvent",
                properties={
                    "event_id": event.event_id,
                    "action": event.action,
                    "actor": event.actor,
                    "patients": event.patients,
                    "instruments": event.instruments,
                    "locations": event.locations,
                    "conditions": event.conditions,
                    "temporal": event.temporal,
                    "purpose": event.purpose,
                    "source_article": event.source_article,
                    "confidence": event.confidence
                }
            ))
            event_count += 1
            
            # === v5 é—œéµä¿®æ­£: å‘¼å«èªæ„é€£çµ ===
            
            # 1. é€£çµ Actor
            if event.actor:
                actor_node_id = self._find_ontology_node_semantic(event.actor, threshold=0.7)
                if actor_node_id:
                    self.edges.append(GraphEdge(
                        source=event_node_id,
                        target=actor_node_id,
                        type="HAS_ACTOR",
                        properties={"source_text": event.actor}
                    ))
                    semantic_links_created += 1
                else:
                    unlinked_entities.add(event.actor)
            
            # 2. é€£çµ Patients
            for entity_text in event.patients:
                entity_node_id = self._find_ontology_node_semantic(entity_text, threshold=0.6)
                if entity_node_id:
                    self.edges.append(GraphEdge(
                        source=event_node_id,
                        target=entity_node_id,
                        type="HAS_PATIENT",
                        properties={"source_text": entity_text}
                    ))
                    semantic_links_created += 1
                else:
                    unlinked_entities.add(entity_text)
            
            # 3. é€£çµ Instruments
            for entity_text in event.instruments:
                entity_node_id = self._find_ontology_node_semantic(entity_text, threshold=0.6)
                if entity_node_id:
                    self.edges.append(GraphEdge(
                        source=event_node_id,
                        target=entity_node_id,
                        type="USES_INSTRUMENT",
                        properties={"source_text": entity_text}
                    ))
                    semantic_links_created += 1
                else:
                    unlinked_entities.add(entity_text)
            
            # 4. é€£çµ Locations
            for entity_text in event.locations:
                entity_node_id = self._find_ontology_node_semantic(entity_text, threshold=0.6)
                if entity_node_id:
                    self.edges.append(GraphEdge(
                        source=event_node_id,
                        target=entity_node_id,
                        type="AT_LOCATION",
                        properties={"source_text": entity_text}
                    ))
                    semantic_links_created += 1
                else:
                    unlinked_entities.add(entity_text)
        
        print(f"    âœ“ äº‹ä»¶å±¤:{event_count} å€‹äº‹ä»¶ç¯€é»")
        print(f"    âœ“ v5 æ”¹é€²: å»ºç«‹äº† {semantic_links_created} æ¢èªæ„å¯¦é«”é€£çµ (HAS_ACTOR, HAS_PATIENT, ...)")
        
        if unlinked_entities:
            print(f"    âš ï¸ v5 è­¦ç¤º: ç™¼ç¾ {len(unlinked_entities)} å€‹ç„¡æ³•é€£çµåˆ°æœ¬é«”çš„æ–°å¯¦é«”ã€‚")
            print(f"      (å»ºè­°: æ“´å…… ontology.json æˆ–å»ºç«‹ 'ontology_expansion_queue.json' è§£æ±ºã€Œå•é¡Œä¸‰ã€)")

    def _build_rule_layer_v5(self):
        """
        (v5 å¯¦ä½œ) å»ºç«‹è¦å‰‡å±¤ (v3 å‡ç´šç‰ˆ)
        
        - å¢å¼·: å¢åŠ  has_embedding å±¬æ€§,ä¸å„²å­˜å®Œæ•´å‘é‡
        """
        print("  â†’ å»ºç«‹è¦å‰‡å±¤ (v5)...")
        
        for rule_id, rule_template in self.rule_base.items():
            rule_node_id = f"rule_{rule_id}"
            
            if any(n.id == rule_node_id for n in self.nodes): continue
            
            self.nodes.append(GraphNode(
                id=rule_node_id,
                type="DecisionRule",
                properties={
                    "rule_id": rule_template.rule_id,
                    "rule_name": rule_template.rule_name,
                    "category": rule_template.category,
                    "pattern": rule_template.pattern,
                    "keywords": rule_template.keywords,
                    "examples": rule_template.examples,
                    "control_type_affinity": rule_template.control_type_affinity,
                    "has_embedding": rule_template.embedding_vector is not None # v5 æ–°å¢
                }
            ))
            
            # é€£æ¥è¦å‰‡åˆ°æ§åˆ¶é¡å‹
            control_node_id = f"control_{rule_template.category}"
            if any(n.id == control_node_id for n in self.nodes):
                self.edges.append(GraphEdge(
                    source=rule_node_id,
                    target=control_node_id,
                    type="SUPPORTS",
                    properties={
                        "affinity": rule_template.control_type_affinity.get(rule_template.category, 0.0)
                    }
                ))
        
        print(f"    âœ“ è¦å‰‡å±¤:{len(self.rule_base)} å€‹è¦å‰‡ç¯€é»")

    def _find_ontology_node_for_entity(self, entity_text: str) -> Optional[str]:
        """
        (v5 å¯¦ä½œ) ç‚ºå¯¦é«”æ–‡æœ¬æ‰¾åˆ°å°æ‡‰çš„æœ¬é«”ç¯€é» ID
        
        (é€™æ˜¯ v3 çš„ç°¡æ˜“åŒ¹é…ç‰ˆã€‚åœ¨ v6 ä¸­,æˆ‘å€‘æ‡‰å°‡æ­¤å‡ç´šç‚ºèªæ„å‘é‡åŒ¹é…)
        """
        # 1. å„ªå…ˆæª¢æŸ¥æ¨™æº–åç¨± (Standard Name)
        for entity_name, entity_info in self.object_ontology.items():
            if entity_info.get('standard_name') == entity_text:
                return f"object_{entity_info['standard_name']}"
        for entity_name, entity_info in self.subject_ontology.items():
            if entity_info.get('standard_name') == entity_text:
                return f"subject_{entity_info['standard_name']}"
        
        # 2. æª¢æŸ¥åç¨± (Name)
        if entity_text in self.object_ontology:
            return f"object_{self.object_ontology[entity_text].get('standard_name')}"
        if entity_text in self.subject_ontology:
            return f"subject_{self.subject_ontology[entity_text].get('standard_name')}"
        
        # 3. æª¢æŸ¥åŒç¾©è© (Synonyms)
        for entity_name, entity_info in self.object_ontology.items():
            if entity_text in entity_info.get('synonyms', []):
                return f"object_{entity_info['standard_name']}"
        for entity_name, entity_info in self.subject_ontology.items():
            if entity_text in entity_info.get('synonyms', []):
                return f"subject_{entity_info['standard_name']}"
        
        return None

    def _sanitize_id(self, text: str) -> str:
        """(v5 å¯¦ä½œ) æ¸…ç†æ–‡å­—ä»¥ç”Ÿæˆåˆæ³•çš„ ID (ç¹¼æ‰¿è‡ª v3)"""
        return re.sub(r'[^\w]', '_', text)

    def _save_knowledge_graph_v5(self):
        """
        (v5 å¯¦ä½œ) å„²å­˜çŸ¥è­˜åœ–è­œ
        
        - å¢å¼·: æ›´æ–° metadata ç‰ˆæœ¬è™Ÿ
        """
        kg_data = {
            "nodes": [asdict(n) for n in self.nodes],
            "edges": [asdict(e) for e in self.edges],
            "statistics": {
                "total_nodes": len(self.nodes),
                "total_edges": len(self.edges),
                "node_types": {
                    node_type: len([n for n in self.nodes if n.type == node_type])
                    for node_type in set(n.type for n in self.nodes)
                }
            },
            "metadata": {
                "version": "5.0", # v5 æ›´æ–°
                "features": [
                    "event_extraction",
                    "computable_rule_base_v2_semantic", # v5 æ›´æ–°
                    "active_learning_priority_v2_unified", # v5 æ›´æ–°
                    "incremental_ontology_expansion",
                    "hierarchical_ontology",
                    "structured_evidence_v2_semantic", # v5 æ›´æ–°
                    "context_aware_obligation_extraction", # v5 æ–°å¢
                    "hdbscan_clustering" # v5 æ–°å¢
                ],
                "confidence_threshold": self.CONFIDENCE_THRESHOLD,
                "min_cluster_size": self.MIN_CLUSTER_SIZE,
                "total_events_extracted": len(self.legal_events),
                "total_rules_defined": len(self.rule_base),
                "discovered_entities": len(self.discovered_entities)
            }
        }
        
        serializable_kg = self._make_json_serializable(kg_data)
        
        output_path = os.path.join(self.output_dir, "legal_kg.json")
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_kg, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… çŸ¥è­˜åœ–è­œå·²å®Œæˆ!")
        print(f"  â†’ ç¯€é»ç¸½æ•¸: {len(self.nodes)}")
        print(f"  â†’ é‚Šç¸½æ•¸: {len(self.edges)}")
        print(f"  â†’ è¼¸å‡ºè·¯å¾‘: {output_path}")   
    
    # ========================================================================
    # v4 å¢å¼·: æ‘˜è¦å ±å‘Šç”Ÿæˆ
    # ========================================================================
    
    def _generate_review_summary_v5(self):
        """
        v5 æ ¸å¿ƒæ”¹é€²: æ‘˜è¦å ±å‘Šé‚è¼¯ä¿®æ­£
        è§£æ±ºå•é¡Œå›› - æª¢æŸ¥æ‰€æœ‰ä½‡åˆ—,ç¢ºä¿å ±å‘Šä¸€è‡´æ€§
        
        é—œéµä¿®æ”¹:
        1. æª¢æŸ¥ problematic_clusters åˆ—è¡¨
        2. æª¢æŸ¥ low_confidence_classifications åˆ—è¡¨
        3. åŸºæ–¼å¯¦éš›å…§å®¹ç”Ÿæˆå»ºè­°
        """
        # âš ï¸ v5 é—œéµ: æª¢æŸ¥æ‰€æœ‰ä½‡åˆ—å…§å®¹
        all_review_items = []
        
        if self.problematic_clusters:
            all_review_items.extend(self.problematic_clusters)
        
        if self.low_confidence_classifications:
            all_review_items.extend(self.low_confidence_classifications)
        
        # çµ±è¨ˆå„å„ªå…ˆç´šæ•¸é‡
        priority_counts = defaultdict(int)
        for item in all_review_items:
            priority = item.get('review_priority', ReviewPriority.LOW.value)
            priority_counts[priority] += 1
        
        critical_count = priority_counts.get(ReviewPriority.CRITICAL.value, 0)
        high_count = priority_counts.get(ReviewPriority.HIGH.value, 0)
        medium_count = priority_counts.get(ReviewPriority.MEDIUM.value, 0)
        low_count = priority_counts.get(ReviewPriority.LOW.value, 0)
        
        summary = {
            "generated_at": datetime.now().isoformat(),
            "version": "v5.0",
            "improvements": [
                "è‘‰èšé¡ (Leaf Clustering)",
                "æç¤ºè©éˆ (Prompt Chaining)",
                "èªæ„å¯¦é«”é€£çµ (Semantic Entity Linking)",
                "æ‘˜è¦é‚è¼¯ä¿®æ­£ - æª¢æŸ¥æ‰€æœ‰ä½‡åˆ—"
            ],
            "review_queues": {
                "low_confidence_classifications": {
                    "count": len(self.low_confidence_classifications),
                    "file": "review_queue/low_confidence_classifications_queue.json",
                    "priority_distribution": self._get_priority_distribution(self.low_confidence_classifications)
                },
                "problematic_clusters": {
                    "count": len(self.problematic_clusters),
                    "file": "review_queue/problematic_clusters_queue.json",
                    "priority_distribution": self._get_priority_distribution(self.problematic_clusters)
                }
            },
            "thresholds": {
                "confidence_threshold": self.CONFIDENCE_THRESHOLD,
                "min_cluster_size": self.MIN_CLUSTER_SIZE,
                "hdbscan_min_cluster_size": 3,  # v5
                "hdbscan_cluster_selection": "leaf"  # v5
            },
            "recommendations": []
        }
        
        # âš ï¸ v5 é—œéµ: åŸºæ–¼å¯¦éš›ä½‡åˆ—å…§å®¹ç”Ÿæˆå»ºè­°
        if critical_count > 0:
            summary["recommendations"].append({
                "priority": ReviewPriority.CRITICAL.value,
                "action": "ç«‹å³å¯©æ ¸é—œéµé …ç›®",
                "description": f"æœ‰ {critical_count} å€‹é …ç›®è¢«æ¨™è¨˜ç‚º CRITICAL å„ªå…ˆç´š",
                "details": {
                    "from_clusters": sum(1 for x in self.problematic_clusters if x.get('review_priority') == ReviewPriority.CRITICAL.value),
                    "from_classifications": sum(1 for x in self.low_confidence_classifications if x.get('review_priority') == ReviewPriority.CRITICAL.value)
                },
                "next_steps": "é€™äº›é …ç›®å½±éŸ¿æ ¸å¿ƒæ³•è¦è§£é‡‹,å¿…é ˆå„ªå…ˆè™•ç†"
            })
        
        if high_count > 0:
            summary["recommendations"].append({
                "priority": ReviewPriority.HIGH.value,
                "action": "å¯©æ ¸é«˜å„ªå…ˆç´šé …ç›®",
                "description": f"æœ‰ {high_count} å€‹é …ç›®è¢«æ¨™è¨˜ç‚º HIGH å„ªå…ˆç´š",
                "details": {
                    "from_clusters": sum(1 for x in self.problematic_clusters if x.get('review_priority') == ReviewPriority.HIGH.value),
                    "from_classifications": sum(1 for x in self.low_confidence_classifications if x.get('review_priority') == ReviewPriority.HIGH.value)
                },
                "next_steps": "é€™äº›é …ç›®æ¶‰åŠé«˜é »ä½¿ç”¨æˆ–é«˜ä¸ç¢ºå®šæ€§,å»ºè­°ç›¡å¿«å¯©æ ¸"
            })
        
        if medium_count > 0:
            summary["recommendations"].append({
                "priority": ReviewPriority.MEDIUM.value,
                "action": "å®šæœŸå¯©æ ¸ä¸­ç­‰å„ªå…ˆç´šé …ç›®",
                "description": f"æœ‰ {medium_count} å€‹é …ç›®è¢«æ¨™è¨˜ç‚º MEDIUM å„ªå…ˆç´š",
                "next_steps": "å¯å®‰æ’å®šæœŸå¯©æ ¸é€±æœŸè™•ç†"
            })
        
        if not summary["recommendations"]:
            summary["recommendations"].append({
                "priority": ReviewPriority.LOW.value,
                "action": "ç„¡éœ€å¯©æ ¸",
                "description": "æ‰€æœ‰è‡ªå‹•åŒ–è™•ç†çµæœå“è³ªè‰¯å¥½",
                "next_steps": "å¯ç›´æ¥ä½¿ç”¨çŸ¥è­˜åœ–è­œ"
            })
        
        # v5 çµ±è¨ˆ
        summary["v5_statistics"] = {
            "total_rules_defined": len(self.rule_base),
            "rules_with_embeddings": len(self.rule_embeddings),
            "ontology_entities_with_embeddings": len(self.ontology_embeddings),
            "obligations_with_context": sum(1 for c in self.obligation_clusters.values() 
                                        if isinstance(c, dict) and 'members' in c),
            "semantic_matched_rules": sum(1 for c in self.obligation_clusters.values()
                                        if isinstance(c, dict) and c.get('evidence', {}).get('rule_similarity_score', 0) > 0),
            "cluster_selection_method": "leaf",
            "total_review_items": len(all_review_items),
            "priority_consistency": "å„ªå…ˆç´šé‚è¼¯å·²çµ±ä¸€,æ‘˜è¦èˆ‡ä½‡åˆ—100%ä¸€è‡´"
        }
        
        serializable_summary = self._make_json_serializable(summary)
        
        summary_file = os.path.join(self.review_queue_dir, "review_summary.json")
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_summary, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“‹ å¯©æ ¸æ‘˜è¦å ±å‘Š (v5):")
        print(f"  â†’ CRITICAL å„ªå…ˆç´š: {critical_count} å€‹")
        print(f"  â†’ HIGH å„ªå…ˆç´š: {high_count} å€‹")
        print(f"  â†’ MEDIUM å„ªå…ˆç´š: {medium_count} å€‹")
        print(f"  â†’ LOW å„ªå…ˆç´š: {low_count} å€‹")
        print(f"  âœ“ v5 æ”¹é€²: æ‘˜è¦èˆ‡ä½‡åˆ—å…§å®¹ 100% ä¸€è‡´")
        print(f"  â†’ è©³ç´°å ±å‘Š: {summary_file}")
    
    def _generate_rule_usage_report(self):
        """ç”Ÿæˆè¦å‰‡ä½¿ç”¨çµ±è¨ˆå ±å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆè¦å‰‡ä½¿ç”¨çµ±è¨ˆ...")
        
        rule_usage = defaultdict(int)
        
        for cluster_info in self.obligation_clusters.values():
            if isinstance(cluster_info, dict):
                evidence = cluster_info.get('evidence', {})
                rule_id = evidence.get('decision_rule_id', '')
                if rule_id:
                    rule_usage[rule_id] += 1
        
        for mapping in self.control_type_mapping.values():
            if isinstance(mapping, dict):
                evidence = mapping.get('evidence', {})
                rule_id = evidence.get('decision_rule_id', '')
                if rule_id:
                    rule_usage[rule_id] += 1
        
        report = {
            "generated_at": datetime.now().isoformat(),
            "version": "v4.0",
            "total_rules_defined": len(self.rule_base),
            "rules_used": len(rule_usage),
            "rules_unused": len(self.rule_base) - len(rule_usage),
            "semantic_matching_enabled": True,
            "rule_usage_details": {
                rule_id: {
                    "usage_count": count,
                    "rule_name": self.rule_base[rule_id].rule_name if rule_id in self.rule_base else "Unknown",
                    "category": self.rule_base[rule_id].category if rule_id in self.rule_base else "Unknown",
                    "has_embedding": rule_id in self.rule_embeddings
                }
                for rule_id, count in sorted(rule_usage.items(), key=lambda x: -x[1])
            },
            "unused_rules": [
                {
                    "rule_id": rule_id,
                    "rule_name": rule_template.rule_name,
                    "category": rule_template.category,
                    "possible_reason": "èªæ–™åº«ä¸­ç„¡ç›¸é—œç¾©å‹™" if rule_id in self.rule_embeddings else "å‘é‡ç”Ÿæˆå¤±æ•—"
                }
                for rule_id, rule_template in self.rule_base.items()
                if rule_id not in rule_usage
            ]
        }
        
        report_file = os.path.join(self.output_dir, "rule_usage_report.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"  âœ“ è¦å‰‡ä½¿ç”¨çµ±è¨ˆ:")
        print(f"    - å·²å®šç¾©è¦å‰‡: {len(self.rule_base)}")
        print(f"    - å·²ä½¿ç”¨è¦å‰‡: {len(rule_usage)}")
        print(f"    - æœªä½¿ç”¨è¦å‰‡: {len(self.rule_base) - len(rule_usage)}")
        print(f"    - è©³ç´°å ±å‘Š: {report_file}")
        
        if rule_usage:
            print(f"  â†’ æœ€å¸¸ç”¨è¦å‰‡ TOP 5:")
            for i, (rule_id, count) in enumerate(sorted(rule_usage.items(), key=lambda x: -x[1])[:5], 1):
                rule_name = self.rule_base.get(rule_id, type('obj', (), {'rule_name': 'Unknown'})).rule_name
                print(f"    {i}. {rule_id} ({rule_name}): {count} æ¬¡")
    
    # ========================================================================
    # ä¸»æµç¨‹
    # ========================================================================
    
    def build(self):
        """
        åŸ·è¡Œå®Œæ•´å»ºæ§‹æµç¨‹ (v5 ç‰ˆæœ¬)
        
        ä¿®æ”¹é»:
        - èª¿ç”¨ _generate_review_summary_v5 (å–ä»£ v4 çš„ _generate_review_summary_v4)
        - æ–°å¢ v5 ç‰ˆæœ¬æ¨™è­˜
        """
        print("=" * 70)
        print("Legal Knowledge Graph Builder v5")
        print("è·æ¥­å®‰å…¨è¡›ç”Ÿæ³•å¾‹çŸ¥è­˜åœ–è­œå»ºæ§‹å™¨ v5")
        print("(è‘‰èšé¡ã€æç¤ºè©éˆã€èªæ„å¯¦é«”é€£çµç‰ˆ)")
        print("=" * 70)
        
        # éšæ®µ 1: è¼‰å…¥ (ç¹¼æ‰¿ v4)
        self.load_documents()
        
        # éšæ®µ 2: äº‹ä»¶æŠ½å– (ç¹¼æ‰¿ v4)
        self.extract_legal_events()
        
        # éšæ®µ 2.5: æœ¬é«”å»ºæ§‹ (ç¹¼æ‰¿ v4)
        print("\nğŸ—‚ï¸ å»ºç«‹åˆ†å±¤æœ¬é«”...")
        self.subject_ontology = self.base_subject_ontology
        self.object_ontology.update(self.base_object_ontology)
        self._save_ontology()
        
        # âš ï¸ v5 ä¿®æ”¹: ç¾©å‹™æ­£è¦åŒ– (èª¿ç”¨ v5 ç‰ˆæœ¬)
        self.normalize_obligations()
        
        # âš ï¸ v5 ä¿®æ”¹: é¢¨éšªåˆ†é¡ (èª¿ç”¨ v5 ç‰ˆæœ¬,è¦‹ä¸‹æ–¹å‡½å¼ 8)
        self.classify_control_types()
        
        # éšæ®µ 5: åœ–è­œå»ºæ§‹ (ç¹¼æ‰¿ v4)
        self.build_knowledge_graph()
        
        # âš ï¸ v5 ä¿®æ”¹: å ±å‘Šç”Ÿæˆ (èª¿ç”¨ v5 ç‰ˆæœ¬)
        self._generate_review_summary_v5()
        self._generate_rule_usage_report()
        
        print("\n" + "=" * 70)
        print("âœ¨ æ‰€æœ‰è™•ç†å®Œæˆ (v5)!")
        print("=" * 70)

# ============================================================================
# ä½¿ç”¨ç¯„ä¾‹
# ============================================================================

def get_api_key():
    """å¾ç’°å¢ƒè®Šæ•¸æˆ–ä½¿ç”¨è€…è¼¸å…¥ç²å– API Key"""
    api_key = os.getenv('OPENAI_API_KEY')
    
    if api_key:
        print("âœ“ å¾ç’°å¢ƒè®Šæ•¸ OPENAI_API_KEY è¼‰å…¥ API Key")
        return api_key
    
    print("=" * 70)
    print("OpenAI API Key è¨­å®š")
    print("=" * 70)
    print("è«‹è¼¸å…¥æ‚¨çš„ OpenAI API Key")
    print("(æ‚¨å¯ä»¥å¾ https://platform.openai.com/api-keys ç²å–)")
    print()
    
    api_key = getpass.getpass("API Key: ").strip()
    
    if not api_key:
        print("âœ— éŒ¯èª¤:æœªæä¾› API Key")
        sys.exit(1)
    
    return api_key


if __name__ == "__main__":
    print("=" * 70)
    print("Legal Knowledge Graph Builder v4")
    print("è·æ¥­å®‰å…¨è¡›ç”Ÿæ³•å¾‹çŸ¥è­˜åœ–è­œå»ºæ§‹å™¨ v4 (æ™ºæ…§èªæ„èšé¡èˆ‡èªå¢ƒæ„ŸçŸ¥ç‰ˆ)")
    print("=" * 70)
    print()
    
    # ç²å– API Key
    api_key = get_api_key()
    
    # é…ç½®è·¯å¾‘
    # INPUT_PATH = "./processed_output/å‡é™æ©Ÿå®‰å…¨æª¢æŸ¥æ§‹é€ æ¨™æº–_processed.json" # ç”¨ä¾†å°è¦æ¨¡æ¸¬è©¦
    # INPUT_PATH = "./processed_output/é«˜å£“æ°£é«”å‹å·¥å®‰å…¨è¦å‰‡_processed.json" # ç”¨ä¾†å°è¦æ¨¡æ¸¬è©¦
    INPUT_PATH = "./processed_output/all_documents.json"
    OUTPUT_DIR = "./kg_output_v6"
    
    # æª¢æŸ¥è¼¸å…¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if not os.path.exists(INPUT_PATH):
        print(f"\nâœ— éŒ¯èª¤:æ‰¾ä¸åˆ°è¼¸å…¥æª”æ¡ˆ {INPUT_PATH}")
        print(f"è«‹ç¢ºèªæª”æ¡ˆè·¯å¾‘æ­£ç¢º")
        sys.exit(1)
    
    print(f"\né…ç½®è³‡è¨Š:")
    print(f"  è¼¸å…¥æª”æ¡ˆ: {INPUT_PATH}")
    print(f"  è¼¸å‡ºç›®éŒ„: {OUTPUT_DIR}")
    print()
    
    print("v4 æ ¸å¿ƒæ”¹é€²:")
    print("  1. HDBSCAN è‡ªé©æ‡‰èšé¡ - è‡ªå‹•éæ¿¾å™ªè²,è§£æ±ºå–®ä¾‹èšé¡å•é¡Œ")
    print("  2. èªæ„è¦å‰‡åŒ¹é… - ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦å–ä»£é—œéµå­—åŒ¹é…")
    print("  3. èªå¢ƒæ„ŸçŸ¥èƒå– - è‡ªå‹•è™•ç†æ³•å¾‹å¼•ç”¨(Anaphora)")
    print("  4. çµ±ä¸€å„ªå…ˆç´šé‚è¼¯ - ä¿®æ­£äººæ©Ÿè¿´åœˆå…§éƒ¨çŸ›ç›¾")
    print()
    
    # å»ºæ§‹çŸ¥è­˜åœ–è­œ
    try:
        builder = LegalKGBuilderV4(
            api_key=api_key,
            input_path=INPUT_PATH,
            output_dir=OUTPUT_DIR
        )
        
        builder.build()
        
        print("\n" + "=" * 70)
        print("âœ¨ å»ºæ§‹å®Œæˆ!è¼¸å‡ºæª”æ¡ˆ:")
        print(f"  1. {OUTPUT_DIR}/legal_events.json - æ³•å¾‹äº‹ä»¶çµæ§‹")
        print(f"  2. {OUTPUT_DIR}/ontology.json - åˆ†å±¤æœ¬é«”çµæ§‹(å«è‡ªå‹•ç™¼ç¾å¯¦é«”)")
        print(f"  3. {OUTPUT_DIR}/obligation_clusters.json - ç¾©å‹™èšé¡çµæœ(v4: HDBSCAN)")
        print(f"  4. {OUTPUT_DIR}/control_type_mapping.json - æ§åˆ¶é¡å‹æ˜ å°„(v4: èªæ„åŒ¹é…)")
        print(f"  5. {OUTPUT_DIR}/rule_usage_report.json - è¦å‰‡ä½¿ç”¨çµ±è¨ˆ(v4: èªæ„ç›¸ä¼¼åº¦)")
        print(f"  6. {OUTPUT_DIR}/review_queue/ - äººæ©Ÿè¿´åœˆå¯©æ ¸ä½‡åˆ—(v4: çµ±ä¸€å„ªå…ˆç´š)")
        print(f"     - review_summary.json - å¯©æ ¸æ‘˜è¦å ±å‘Š")
        print(f"     - problematic_clusters_queue.json - å•é¡Œèšé¡ä½‡åˆ—")
        print(f"     - low_confidence_classifications_queue.json - ä½ä¿¡å¿ƒåˆ†é¡ä½‡åˆ—")
        print(f"     - noise_points.json - HDBSCAN è­˜åˆ¥çš„å™ªè²é»")
        print("=" * 70)
        print("\nğŸ“ v4 ç‰ˆæœ¬ç›¸è¼ƒæ–¼ v3 çš„é—œéµæ”¹é€²:")
        print("  âœ“ è§£æ±ºäº†å–®ä¾‹èšé¡ç½é›£ (ä½¿ç”¨ HDBSCAN)")
        print("  âœ“ ä¿®æ­£äº†è¦å‰‡ä½¿ç”¨åå·® (èªæ„ç›¸ä¼¼åº¦åŒ¹é…)")
        print("  âœ“ è™•ç†äº†æ³•å¾‹å¼•ç”¨å•é¡Œ (èªå¢ƒæ„ŸçŸ¥èƒå–)")
        print("  âœ“ çµ±ä¸€äº†å„ªå…ˆç´šé‚è¼¯ (ä¸€è‡´çš„ HITL æŒ‡ä»¤)")
        print()
        print("ğŸ“ ä¸‹ä¸€æ­¥å»ºè­°:")
        print("  1. æª¢æŸ¥ review_summary.json äº†è§£å¯©æ ¸å„ªå…ˆç´š")
        print("  2. è™•ç† CRITICAL å’Œ HIGH å„ªå…ˆç´šé …ç›®")
        print("  3. æª¢è¦– noise_points.json ç¢ºèªè¢«éæ¿¾çš„ç¾©å‹™")
        print("  4. æ¯”è¼ƒ v3 å’Œ v4 çš„ rule_usage_report çœ‹æ”¹é€²æ•ˆæœ")
        
    except KeyboardInterrupt:
        print("\n\nâœ— ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
        sys.exit(0)
    except Exception as e:
        print(f"\nâœ— åŸ·è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)