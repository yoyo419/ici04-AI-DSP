#!/usr/bin/env python3
"""
ğŸŒ©ï¸ Google Colab å°ˆç”¨ LoRA åˆä½µè…³æœ¬
é©ç”¨æ–¼ Qwen2.5 Legal Delta æ¨¡å‹è¨“ç·´å¾Œçš„åˆä½µæ“ä½œ

ä½¿ç”¨æ–¹æ³•ï¼ˆåœ¨ Colab ä¸­ï¼‰:
    python merge_lora_colab.py
    python merge_lora_colab.py --device_map cpu  # è¨˜æ†¶é«”ä¸è¶³æ™‚ä½¿ç”¨
"""

import argparse
import json
import os
import gc
from datetime import datetime

import torch
from peft import PeftConfig, PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer


def log_print(message):
    """åŒæ™‚è¼¸å‡ºåˆ°æ§åˆ¶å°å’Œæ—¥èªŒæª”æ¡ˆ"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print(f"[{timestamp}] {message}")


def main():
    """ä¸»å‡½æ•¸"""
    # ============================================================
    # ç’°å¢ƒè¨­å®š
    # ============================================================
    print("=" * 70)
    print("ğŸŒ©ï¸ Colab LoRA åˆä½µå·¥å…·")
    print("=" * 70)
    print()
    
    # æª¢æŸ¥æ˜¯å¦åœ¨ Colab ç’°å¢ƒ
    IN_COLAB = False
    try:
        import google.colab
        IN_COLAB = True
    except ImportError:
        if 'COLAB_GPU' in os.environ or 'COLAB_TPU_ADDR' in os.environ:
            IN_COLAB = True
        elif os.path.exists('/content') and os.path.exists('/usr/local/lib/python3.10/dist-packages'):
            IN_COLAB = True
    
    if IN_COLAB:
        log_print("âœ“ åœ¨ Google Colab ç’°å¢ƒä¸­")
    else:
        log_print("âš ï¸  ä¸åœ¨ Colab ç’°å¢ƒï¼ˆæœ¬åœ°æ¸¬è©¦æ¨¡å¼ï¼‰")
    
    print()
    
    # è¨­å®šé›¢ç·šæ¨¡å¼ï¼ˆé¿å…ä¸å¿…è¦çš„ç¶²çµ¡è«‹æ±‚ï¼‰
    os.environ["HF_HUB_OFFLINE"] = "1"
    os.environ["TRANSFORMERS_OFFLINE"] = "1"
    os.environ["HF_HUB_DISABLE_TELEMETRY"] = "1"
    
    # ============================================================
    # è§£æåƒæ•¸
    # ============================================================
    parser = argparse.ArgumentParser(description="Colab LoRA Merge Tool")
    parser.add_argument(
        "--lora_adapter_path", 
        type=str, 
        default="/content/Colab/lora_adapter",
        help="Path to the trained LoRA adapter"
    )
    parser.add_argument(
        "--base_model_path", 
        type=str,
        default="/content/Colab/Qwen2.5-3B-Instruct", 
        help="Path to base model"
    )
    parser.add_argument(
        "--save_path", 
        type=str,
        default="/content/Colab/merged_model",
        help="Path to save the merged model"
    )
    parser.add_argument(
        "--device_map",
        type=str,
        default="auto",
        help="Device map (auto/cpu/cuda)"
    )
    parser.add_argument(
        "--save_to_drive",
        action="store_true",
        help="Save merged model to Google Drive"
    )
    parser.add_argument(
        "--drive_path",
        type=str,
        default="/content/drive/MyDrive/LegalDelta/merged_models",
        help="Google Drive path to save merged model"
    )
    
    args = parser.parse_args()
    
    # ============================================================
    # é¡¯ç¤ºé…ç½®
    # ============================================================
    log_print("ğŸ“‹ é…ç½®ä¿¡æ¯ï¼š")
    log_print(f"  LoRA adapter: {args.lora_adapter_path}")
    log_print(f"  Base model: {args.base_model_path}")
    log_print(f"  Save to: {args.save_path}")
    log_print(f"  Device: {args.device_map}")
    if args.save_to_drive:
        log_print(f"  Drive backup: {args.drive_path}")
    print()
    
    # ============================================================
    # é©—è­‰è·¯å¾‘
    # ============================================================
    log_print("ğŸ” æª¢æŸ¥æª”æ¡ˆ...")
    
    if not os.path.exists(args.lora_adapter_path):
        log_print(f"âŒ LoRA adapter ä¸å­˜åœ¨: {args.lora_adapter_path}")
        log_print("\nğŸ’¡ è«‹ç¢ºèªè¨“ç·´å·²å®Œæˆï¼Œä¸¦ä¸” LoRA adapter å·²ä¿å­˜ã€‚")
        return
    
    if not os.path.exists(args.base_model_path):
        log_print(f"âŒ Base model ä¸å­˜åœ¨: {args.base_model_path}")
        log_print("\nğŸ’¡ è«‹å…ˆä¸‹è¼‰åŸºç¤æ¨¡å‹ã€‚")
        return
    
    # æª¢æŸ¥å¿…è¦æª”æ¡ˆ
    adapter_config = os.path.join(args.lora_adapter_path, "adapter_config.json")
    if not os.path.exists(adapter_config):
        log_print(f"âŒ adapter_config.json ä¸å­˜åœ¨æ–¼ {args.lora_adapter_path}")
        return
    
    log_print("  âœ“ LoRA adapter æ‰¾åˆ°")
    log_print("  âœ“ Base model æ‰¾åˆ°")
    log_print("  âœ“ adapter_config.json æ‰¾åˆ°")
    print()
    
    # ============================================================
    # è¼‰å…¥é…ç½®
    # ============================================================
    log_print("ğŸ“‹ è¼‰å…¥ LoRA é…ç½®...")
    try:
        config = PeftConfig.from_pretrained(
            args.lora_adapter_path, 
            local_files_only=True
        )
        log_print("  âœ“ é…ç½®è¼‰å…¥æˆåŠŸ")
    except Exception as e:
        log_print(f"âš ï¸  å¾ PeftConfig è¼‰å…¥å¤±æ•—: {e}")
        log_print("  å˜—è©¦ç›´æ¥å¾ JSON è¼‰å…¥...")
        try:
            with open(adapter_config, 'r') as f:
                config_dict = json.load(f)
            from peft import LoraConfig
            config = LoraConfig(**config_dict)
            log_print("  âœ“ å¾ JSON è¼‰å…¥æˆåŠŸ")
        except Exception as e2:
            log_print(f"âŒ é…ç½®è¼‰å…¥å¤±æ•—: {e2}")
            return
    
    print()
    
    # ============================================================
    # è¼‰å…¥ Tokenizer
    # ============================================================
    log_print("ğŸ“ è¼‰å…¥ tokenizer...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            args.base_model_path,
            local_files_only=True,
            trust_remote_code=True
        )
        log_print("  âœ“ Tokenizer è¼‰å…¥æˆåŠŸ")
    except Exception as e:
        log_print(f"âŒ Tokenizer è¼‰å…¥å¤±æ•—: {e}")
        return
    
    print()
    
    # ============================================================
    # è¼‰å…¥åŸºç¤æ¨¡å‹
    # ============================================================
    log_print("ğŸ§  è¼‰å…¥åŸºç¤æ¨¡å‹...")
    log_print("  é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜...")
    
    try:
        model = AutoModelForCausalLM.from_pretrained(
            args.base_model_path,
            torch_dtype=torch.float16,  # ä½¿ç”¨ float16 ç¯€çœè¨˜æ†¶é«”
            device_map=args.device_map,
            trust_remote_code=True,
            local_files_only=True,
            low_cpu_mem_usage=True
        )
        log_print("  âœ“ åŸºç¤æ¨¡å‹è¼‰å…¥æˆåŠŸ")
    except Exception as e:
        log_print(f"âŒ åŸºç¤æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        log_print("\nğŸ’¡ å¦‚æœè¨˜æ†¶é«”ä¸è¶³ï¼Œè«‹å˜—è©¦ï¼š")
        log_print("   python merge_lora_colab.py --device_map cpu")
        return
    
    print()
    
    # ============================================================
    # è¼‰å…¥ LoRA Adapter
    # ============================================================
    log_print("ğŸ”§ è¼‰å…¥ LoRA adapter...")
    try:
        model = PeftModel.from_pretrained(
            model,
            args.lora_adapter_path,
            local_files_only=True,
            is_trainable=False
        )
        log_print("  âœ“ LoRA adapter è¼‰å…¥æˆåŠŸ")
    except Exception as e:
        log_print(f"âŒ LoRA adapter è¼‰å…¥å¤±æ•—: {e}")
        return
    
    print()
    
    # ============================================================
    # åˆä½µæ¨¡å‹
    # ============================================================
    log_print("ğŸ”„ åˆä½µ LoRA æ¬Šé‡åˆ°åŸºç¤æ¨¡å‹...")
    log_print("  é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜...")
    
    try:
        model = model.merge_and_unload()
        log_print("  âœ“ åˆä½µå®Œæˆ")
    except Exception as e:
        log_print(f"âŒ åˆä½µå¤±æ•—: {e}")
        log_print("\nğŸ’¡ å¦‚æœé‡åˆ° 'frozenset' éŒ¯èª¤ï¼Œé€™æ˜¯å·²çŸ¥å•é¡Œã€‚")
        log_print("   æ¨¡å‹å¯èƒ½éœ€è¦é‡æ–°è¼‰å…¥ç‚º float16ï¼ˆè€Œéé‡åŒ–æ ¼å¼ï¼‰å¾Œå†åˆä½µã€‚")
        return
    
    print()
    
    # ============================================================
    # æ¸…ç†è¨˜æ†¶é«”
    # ============================================================
    log_print("ğŸ§¹ æ¸…ç†è¨˜æ†¶é«”...")
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    log_print("  âœ“ è¨˜æ†¶é«”æ¸…ç†å®Œæˆ")
    
    print()
    
    # ============================================================
    # å„²å­˜åˆä½µå¾Œçš„æ¨¡å‹
    # ============================================================
    log_print(f"ğŸ’¾ å„²å­˜åˆä½µå¾Œçš„æ¨¡å‹åˆ° {args.save_path}...")
    log_print("  é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜...")
    
    os.makedirs(args.save_path, exist_ok=True)
    
    # ç§»å‹•åˆ° CPU ä»¥ç¯€çœè¨˜æ†¶é«”
    log_print("  ç§»å‹•æ¨¡å‹åˆ° CPU...")
    model = model.cpu()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # å„²å­˜æ¨¡å‹
    try:
        model.save_pretrained(
            args.save_path,
            max_shard_size="1GB",  # åˆ†ç‰‡å„²å­˜ï¼Œæ¯å€‹æª”æ¡ˆæœ€å¤§ 1GB
            safe_serialization=True
        )
        tokenizer.save_pretrained(args.save_path)
        log_print("  âœ“ æ¨¡å‹å„²å­˜æˆåŠŸ")
    except Exception as e:
        log_print(f"âŒ æ¨¡å‹å„²å­˜å¤±æ•—: {e}")
        return
    
    print()
    
    # ============================================================
    # å‚™ä»½åˆ° Google Driveï¼ˆå¯é¸ï¼‰
    # ============================================================
    if args.save_to_drive and IN_COLAB:
        log_print("ğŸ“¤ å‚™ä»½åˆ° Google Drive...")
        
        try:
            # æ›è¼‰ Google Drive
            from google.colab import drive
            drive.mount('/content/drive', force_remount=False)
            
            # å‰µå»ºç›®æ¨™ç›®éŒ„
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            drive_save_path = os.path.join(
                args.drive_path, 
                f"merged_model_{timestamp}"
            )
            os.makedirs(drive_save_path, exist_ok=True)
            
            # è¤‡è£½æª”æ¡ˆ
            log_print(f"  è¤‡è£½åˆ°: {drive_save_path}")
            import shutil
            shutil.copytree(args.save_path, drive_save_path, dirs_exist_ok=True)
            
            log_print("  âœ“ å·²å‚™ä»½åˆ° Google Drive")
            log_print(f"  ä½ç½®: {drive_save_path}")
        except Exception as e:
            log_print(f"âš ï¸  å‚™ä»½åˆ° Drive å¤±æ•—: {e}")
            log_print("  åˆä½µå¾Œçš„æ¨¡å‹ä»ä¿å­˜åœ¨ Colab è‡¨æ™‚å„²å­˜ä¸­")
    
    print()
    
    # ============================================================
    # æœ€çµ‚æ¸…ç†
    # ============================================================
    log_print("ğŸ§¹ æœ€çµ‚æ¸…ç†...")
    del model
    del tokenizer
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    print()
    
    # ============================================================
    # å®Œæˆ
    # ============================================================
    print("=" * 70)
    log_print("âœ… åˆä½µå®Œæˆï¼")
    print("=" * 70)
    print()
    log_print("ğŸ“‚ åˆä½µå¾Œçš„æ¨¡å‹ä½ç½®:")
    log_print(f"   {args.save_path}")
    
    if args.save_to_drive and IN_COLAB:
        log_print(f"\nğŸ“‚ Google Drive å‚™ä»½ä½ç½®:")
        log_print(f"   {drive_save_path}")
    
    print()
    log_print("ğŸš€ ä¸‹ä¸€æ­¥ï¼š")
    log_print("   1. ä½¿ç”¨åˆä½µå¾Œçš„æ¨¡å‹é€²è¡Œæ¨ç†")
    log_print("   2. å¦‚æœåœ¨ Colabï¼Œè¨˜å¾—å‚™ä»½åˆ° Google Drive")
    log_print("   3. æ¨ç†ç¯„ä¾‹ï¼š")
    print()
    print("```python")
    print("from transformers import AutoModelForCausalLM, AutoTokenizer")
    print()
    print(f"model = AutoModelForCausalLM.from_pretrained('{args.save_path}')")
    print(f"tokenizer = AutoTokenizer.from_pretrained('{args.save_path}')")
    print()
    print("prompt = 'ä¾å‹å‹•åŸºæº–æ³•è¦å®šï¼Œé›‡ä¸»å»¶é•·å‹å·¥ä¹‹å·¥ä½œæ™‚é–“...'")
    print("inputs = tokenizer(prompt, return_tensors='pt')")
    print("outputs = model.generate(**inputs, max_length=512)")
    print("print(tokenizer.decode(outputs[0], skip_special_tokens=True))")
    print("```")
    print()
    print("=" * 70)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  åˆä½µè¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        print("\nğŸ’¡ å¯èƒ½çš„è§£æ±ºæ–¹æ¡ˆ:")
        print("   1. è¨˜æ†¶é«”ä¸è¶³ â†’ ä½¿ç”¨ --device_map cpu")
        print("   2. æª”æ¡ˆä¸å­˜åœ¨ â†’ æª¢æŸ¥è·¯å¾‘æ˜¯å¦æ­£ç¢º")
        print("   3. æ¬Šé™å•é¡Œ â†’ ç¢ºèªæœ‰å¯«å…¥æ¬Šé™")
        import traceback
        traceback.print_exc()

