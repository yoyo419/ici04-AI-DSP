"""
ğŸŒ©ï¸ Google Colab å°ˆç”¨è¨“ç·´è…³æœ¬
é©ç”¨æ–¼ Qwen2.5-3B Legal Delta æ¨¡å‹è¨“ç·´

ç‰¹é»ï¼š
- å®Œå…¨åœ¨ Colab Linux ç’°å¢ƒé‹è¡Œ
- ä¸ä¾è³´ condaï¼ˆä½¿ç”¨ pipï¼‰
- è‡ªå‹•ä¸‹è¼‰æ¨¡å‹
- æ”¯æ´ GPU åŠ é€Ÿ
"""

import os
import sys
import time
from datetime import datetime

# æŠ‘åˆ¶ TensorFlow/CUDA è­¦å‘Šï¼ˆColab ç’°å¢ƒå¸¸è¦‹ï¼‰
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)

# ============================================================
# ç’°å¢ƒè¨­å®š
# ============================================================
print("=" * 70)
print("ğŸŒ©ï¸ LegalDelta Colab è¨“ç·´è…³æœ¬")
print("=" * 70)
print()

# æª¢æŸ¥ç’°å¢ƒï¼ˆä½¿ç”¨å¤šç¨®æ–¹å¼æª¢æ¸¬ï¼‰
IN_COLAB = False
try:
    import google.colab
    IN_COLAB = True
except ImportError:
    # æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
    if 'COLAB_GPU' in os.environ or 'COLAB_TPU_ADDR' in os.environ:
        IN_COLAB = True
    # æª¢æŸ¥æ˜¯å¦åœ¨ /content ç›®éŒ„ï¼ˆColab ç‰¹æœ‰ï¼‰
    elif os.path.exists('/content') and os.path.exists('/usr/local/lib/python3.10/dist-packages'):
        IN_COLAB = True

if IN_COLAB:
    print("âœ“ åœ¨ Google Colab ç’°å¢ƒä¸­")
else:
    print("âš ï¸  ä¸åœ¨ Colab ç’°å¢ƒï¼ˆæœ¬åœ°æ¸¬è©¦æ¨¡å¼ï¼‰")

# å›ºå®šéš¨æ©Ÿç¨®å­
import torch
import numpy as np
import random

def set_seed(seed=3407):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

set_seed(3407)

# æª¢æŸ¥ GPU
if torch.cuda.is_available():
    print(f"âœ“ GPU: {torch.cuda.get_device_name(0)}")
    print(f"âœ“ GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
else:
    print("âš ï¸  ç„¡ GPUï¼Œè¨“ç·´å°‡æœƒéå¸¸æ…¢")

print()

# ============================================================
# è¶…åƒæ•¸é…ç½®
# ============================================================
class Config:
    # æ¨¡å‹è¨­å®š
    model_size = "3B"  # å¯æ”¹ç‚º "14B" ä½†éœ€è¦æ›´å¤šè¨˜æ†¶é«”
    base_model_name = f"Qwen/Qwen2.5-{model_size}-Instruct"
    base_model_path = f"/content/Colab/Qwen2.5-{model_size}-Instruct"
    
    # è¨“ç·´æ•¸æ“š
    train_data = "/content/Colab/data/training_data.json"
    dev_data = "/content/Colab/data/valid_data.json"
    
    # è¼¸å‡ºè·¯å¾‘
    output_dir = "/content/Colab/outputs"
    lora_output = "/content/Colab/lora_adapter"
    log_dir = "/content/Colab/logs"
    
    # LoRA é…ç½®
    lora_rank = 32
    lora_alpha = 32
    lora_dropout = 0.0
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", 
                     "gate_proj", "up_proj", "down_proj"]
    
    # è¨“ç·´è¶…åƒæ•¸
    learning_rate = 5e-5
    num_train_epochs = 3  # Colab æ™‚é–“æœ‰é™ï¼Œæ¸›å°‘ epochs
    per_device_batch_size = 1
    gradient_accumulation_steps = 32
    max_seq_length = 4096  # æ¸›å°‘ä»¥ç¯€çœè¨˜æ†¶é«”
    max_prompt_length = 2048
    max_completion_length = 512
    
    # GRPO ç‰¹å®šåƒæ•¸
    num_generations = 4
    
    # å…¶ä»–è¨­å®š
    save_steps = 100
    logging_steps = 10
    eval_steps = 100
    save_total_limit = 2
    
    # å„ªåŒ–å™¨
    optim = "paged_adamw_8bit"  # ç¯€çœè¨˜æ†¶é«”
    warmup_ratio = 0.1
    lr_scheduler_type = "cosine"

config = Config()

# å‰µå»ºå¿…è¦ç›®éŒ„
os.makedirs(config.output_dir, exist_ok=True)
os.makedirs(config.log_dir, exist_ok=True)
os.makedirs(config.lora_output, exist_ok=True)

# è¨­å®šæ—¥èªŒ
experiment_name = f"qwen{config.model_size}_colab_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
log_file = os.path.join(config.log_dir, f"{experiment_name}.log")

def log_print(message):
    """åŒæ™‚è¼¸å‡ºåˆ°æ§åˆ¶å°å’Œæ—¥èªŒæª”æ¡ˆ"""
    print(message)
    with open(log_file, "a", encoding="utf-8") as f:
        f.write(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}\n")

log_print(f"å¯¦é©—åç¨±: {experiment_name}")
log_print(f"æ—¥èªŒæª”æ¡ˆ: {log_file}")
log_print("")

# ============================================================
# 1. æª¢æŸ¥/ä¸‹è¼‰åŸºç¤æ¨¡å‹
# ============================================================
log_print("=" * 70)
log_print("æ­¥é©Ÿ 1: æª¢æŸ¥åŸºç¤æ¨¡å‹")
log_print("=" * 70)

if not os.path.exists(config.base_model_path):
    log_print(f"â¬‡ï¸  ä¸‹è¼‰æ¨¡å‹: {config.base_model_name}")
    log_print("é€™å¯èƒ½éœ€è¦ 5-15 åˆ†é˜...")
    
    from huggingface_hub import snapshot_download
    
    try:
        model_path = snapshot_download(
            repo_id=config.base_model_name,
            local_dir=config.base_model_path,
            local_dir_use_symlinks=False,
            resume_download=True
        )
        log_print(f"âœ“ æ¨¡å‹ä¸‹è¼‰å®Œæˆ: {model_path}")
    except Exception as e:
        log_print(f"âŒ æ¨¡å‹ä¸‹è¼‰å¤±æ•—: {e}")
        sys.exit(1)
else:
    log_print(f"âœ“ æ¨¡å‹å·²å­˜åœ¨: {config.base_model_path}")

log_print("")

# ============================================================
# 2. æª¢æŸ¥è¨“ç·´æ•¸æ“š
# ============================================================
log_print("=" * 70)
log_print("æ­¥é©Ÿ 2: æª¢æŸ¥è¨“ç·´æ•¸æ“š")
log_print("=" * 70)

import json

if not os.path.exists(config.train_data):
    log_print(f"âŒ æ‰¾ä¸åˆ°è¨“ç·´æ•¸æ“š: {config.train_data}")
    log_print("è«‹ç¢ºèªæ•¸æ“šæª”æ¡ˆå·²ä¸Šå‚³")
    sys.exit(1)

with open(config.train_data, 'r', encoding='utf-8') as f:
    train_data = json.load(f)
log_print(f"âœ“ è¨“ç·´æ•¸æ“š: {len(train_data)} ç­†")

if os.path.exists(config.dev_data):
    with open(config.dev_data, 'r', encoding='utf-8') as f:
        dev_data = json.load(f)
    log_print(f"âœ“ é©—è­‰æ•¸æ“š: {len(dev_data)} ç­†")
else:
    log_print("âš ï¸  æ‰¾ä¸åˆ°é©—è­‰æ•¸æ“š")
    dev_data = None

log_print("")

# ============================================================
# 3. è¼‰å…¥æ¨¡å‹å’Œ Tokenizer
# ============================================================
log_print("=" * 70)
log_print("æ­¥é©Ÿ 3: è¼‰å…¥æ¨¡å‹")
log_print("=" * 70)

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model, TaskType

# è¼‰å…¥ Tokenizer
log_print("è¼‰å…¥ tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(
    config.base_model_path,
    trust_remote_code=True,
    local_files_only=True
)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

log_print("âœ“ Tokenizer è¼‰å…¥å®Œæˆ")

# è¼‰å…¥åŸºç¤æ¨¡å‹
log_print("è¼‰å…¥åŸºç¤æ¨¡å‹...")
model = AutoModelForCausalLM.from_pretrained(
    config.base_model_path,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True,
    local_files_only=True,
    low_cpu_mem_usage=True,
)

log_print("âœ“ åŸºç¤æ¨¡å‹è¼‰å…¥å®Œæˆ")

# æ‡‰ç”¨ LoRA
log_print("æ‡‰ç”¨ LoRA é…ç½®...")
lora_config = LoraConfig(
    r=config.lora_rank,
    lora_alpha=config.lora_alpha,
    lora_dropout=config.lora_dropout,
    target_modules=config.target_modules,
    bias="none",
    task_type=TaskType.CAUSAL_LM,
)

model = get_peft_model(model, lora_config)
log_print("âœ“ LoRA é…ç½®å®Œæˆ")

# é¡¯ç¤ºå¯è¨“ç·´åƒæ•¸
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
all_params = sum(p.numel() for p in model.parameters())
trainable_percent = 100 * trainable_params / all_params

log_print(f"å¯è¨“ç·´åƒæ•¸: {trainable_params:,} / {all_params:,} ({trainable_percent:.2f}%)")
log_print("")

# ============================================================
# 4. æº–å‚™æ•¸æ“šé›†
# ============================================================
log_print("=" * 70)
log_print("æ­¥é©Ÿ 4: æº–å‚™æ•¸æ“šé›†")
log_print("=" * 70)

from datasets import Dataset

SYSTEM_PROMPT = """ç”¨æˆ·ä¸åŠ©æ‰‹ä¹‹é—´çš„å¯¹è¯ã€‚ç”¨æˆ·æå‡ºé—®é¢˜ï¼ŒåŠ©æ‰‹è§£å†³å®ƒã€‚åŠ©æ‰‹é¦–å…ˆæ€è€ƒæ¨ç†è¿‡ç¨‹ï¼Œç„¶åæä¾›ç”¨æˆ·ç­”æ¡ˆã€‚æ¨ç†è¿‡ç¨‹å’Œç­”æ¡ˆåˆ†åˆ«åŒ…å«åœ¨ <reasoning> </reasoning> å’Œ <answer> </answer> æ ‡ç­¾ä¸­ã€‚

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”é—®é¢˜ï¼š
<reasoning>
åœ¨æ­¤è¯¦ç»†åˆ†æé—®é¢˜å¹¶å±•ç¤ºå®Œæ•´çš„æ¨ç†è¿‡ç¨‹ï¼ŒåŒ…æ‹¬æ€è€ƒæ­¥éª¤ã€ç›¸å…³çŸ¥è¯†å’Œé€»è¾‘åˆ†æã€‚
</reasoning>
<answer>
åœ¨æ­¤æä¾›ç®€æ´æ˜ç¡®çš„æœ€ç»ˆç­”æ¡ˆã€‚
</answer>"""

def format_dataset(data_list):
    """æ ¼å¼åŒ–ä¸¦ tokenize æ•¸æ“šé›†"""
    formatted_data = []
    
    for item in data_list:
        # æ§‹å»º prompt
        prompt_text = f"{SYSTEM_PROMPT}\n\n[QUERY_ID:{item['id']}]\nä½ æ˜¯ä¸€ä¸ªæ³•å¾‹ä¸“å®¶ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹è¦æ±‚å›ç­”ï¼š\n{item['instruction']}{item['question']}"
        
        # æ§‹å»ºå›ç­”
        answer_text = f"<answer>\n{item['answer']}\n</answer>"
        
        # å®Œæ•´æ–‡æœ¬ï¼ˆç”¨æ–¼è¨“ç·´ï¼‰
        full_text = f"{prompt_text}\n{answer_text}{tokenizer.eos_token}"
        
        # Tokenize æ–‡æœ¬
        tokenized = tokenizer(
            full_text,
            truncation=True,
            max_length=config.max_seq_length,
            padding=False,  # ç”± data_collator è™•ç† padding
        )
        
        formatted_data.append(tokenized)
    
    return Dataset.from_list(formatted_data)

train_dataset = format_dataset(train_data)
log_print(f"âœ“ è¨“ç·´é›†: {len(train_dataset)} ç­†")

if dev_data:
    eval_dataset = format_dataset(dev_data)
    log_print(f"âœ“ é©—è­‰é›†: {len(eval_dataset)} ç­†")
else:
    eval_dataset = None

log_print("")

# ============================================================
# 5. è¨­å®šè¨“ç·´åƒæ•¸
# ============================================================
log_print("=" * 70)
log_print("æ­¥é©Ÿ 5: é…ç½®è¨“ç·´")
log_print("=" * 70)

from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling

training_args = TrainingArguments(
    output_dir=os.path.join(config.output_dir, experiment_name),
    num_train_epochs=config.num_train_epochs,
    per_device_train_batch_size=config.per_device_batch_size,
    per_device_eval_batch_size=config.per_device_batch_size,
    gradient_accumulation_steps=config.gradient_accumulation_steps,
    learning_rate=config.learning_rate,
    warmup_ratio=config.warmup_ratio,
    lr_scheduler_type=config.lr_scheduler_type,
    logging_dir=os.path.join(config.log_dir, experiment_name),
    logging_steps=config.logging_steps,
    save_steps=config.save_steps,
    eval_steps=config.eval_steps if eval_dataset else None,
    save_total_limit=config.save_total_limit,
    eval_strategy="steps" if eval_dataset else "no",  # ä¿®å¾©ï¼šæ”¹ç‚º eval_strategy
    load_best_model_at_end=True if eval_dataset else False,
    metric_for_best_model="eval_loss" if eval_dataset else None,
    greater_is_better=False,
    fp16=torch.cuda.is_available(),
    bf16=False,
    optim=config.optim,
    max_grad_norm=1.0,
    report_to="tensorboard",
    remove_unused_columns=False,
    dataloader_num_workers=2,
)

# Data Collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
)

# è©•ä¼°æŒ‡æ¨™å‡½æ•¸
def compute_metrics(eval_preds):
    """
    è¨ˆç®—è©•ä¼°æŒ‡æ¨™
    
    è¿”å›çš„æŒ‡æ¨™ï¼š
    - accuracy: Token ç´šåˆ¥çš„æº–ç¢ºç‡
    - f1: Token ç´šåˆ¥çš„ F1 score (macro average)
    - precision: Token ç´šåˆ¥çš„ç²¾ç¢ºç‡
    - recall: Token ç´šåˆ¥çš„å¬å›ç‡
    """
    import numpy as np
    from sklearn.metrics import precision_recall_fscore_support
    
    predictions, labels = eval_preds
    
    # predictions æ˜¯ logitsï¼Œå– argmax å¾—åˆ°é æ¸¬çš„ token
    if isinstance(predictions, tuple):
        predictions = predictions[0]
    
    # å°‡ logits è½‰æ›ç‚ºé æ¸¬çš„ token ID
    predictions = np.argmax(predictions, axis=-1)
    
    # è¨ˆç®—æº–ç¢ºç‡ï¼ˆå¿½ç•¥ padding tokensï¼Œé€šå¸¸æ˜¯ -100ï¼‰
    mask = labels != -100
    
    if mask.sum() > 0:
        # æå–æœ‰æ•ˆçš„é æ¸¬å’Œæ¨™ç±¤ï¼ˆæ’é™¤ paddingï¼‰
        valid_predictions = predictions[mask]
        valid_labels = labels[mask]
        
        # 1. Accuracy
        correct = (valid_predictions == valid_labels)
        accuracy = correct.sum() / len(valid_labels)
        
        # 2. Precision, Recall, F1 Score
        # ä½¿ç”¨ macro averageï¼ˆå°æ¯å€‹é¡åˆ¥è¨ˆç®—æŒ‡æ¨™å¾Œå–å¹³å‡ï¼‰
        try:
            precision, recall, f1, _ = precision_recall_fscore_support(
                valid_labels,
                valid_predictions,
                average='macro',  # macro average: å°æ‰€æœ‰é¡åˆ¥å¹³ç­‰å°å¾…
                zero_division=0   # é¿å…é™¤é›¶éŒ¯èª¤
            )
        except Exception as e:
            # å¦‚æœè¨ˆç®—å¤±æ•—ï¼ˆä¾‹å¦‚åªæœ‰ä¸€å€‹é¡åˆ¥ï¼‰ï¼Œä½¿ç”¨ accuracy ä½œç‚ºå¾Œå‚™
            log_print(f"âš ï¸  F1 è¨ˆç®—è­¦å‘Š: {e}")
            precision = accuracy
            recall = accuracy
            f1 = accuracy
        
    else:
        accuracy = 0.0
        precision = 0.0
        recall = 0.0
        f1 = 0.0
    
    return {
        "accuracy": float(accuracy),
        "precision": float(precision),
        "recall": float(recall),
        "f1": float(f1),
    }

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics if eval_dataset else None,  # æ·»åŠ è©•ä¼°æŒ‡æ¨™
)

log_print("âœ“ è¨“ç·´é…ç½®å®Œæˆ")
log_print(f"  - Epochs: {config.num_train_epochs}")
log_print(f"  - Batch size: {config.per_device_batch_size}")
log_print(f"  - Gradient accumulation: {config.gradient_accumulation_steps}")
log_print(f"  - Effective batch size: {config.per_device_batch_size * config.gradient_accumulation_steps}")
log_print(f"  - Learning rate: {config.learning_rate}")
log_print("")

# ============================================================
# 6. é–‹å§‹è¨“ç·´
# ============================================================
log_print("=" * 70)
log_print("æ­¥é©Ÿ 6: é–‹å§‹è¨“ç·´")
log_print("=" * 70)
log_print("")

start_time = time.time()

try:
    log_print("ğŸš€ è¨“ç·´é–‹å§‹...")
    log_print("")
    
    # è¨“ç·´
    trainer.train()
    
    # ä¿å­˜æœ€çµ‚æ¨¡å‹
    log_print("")
    log_print("=" * 70)
    log_print("ä¿å­˜æ¨¡å‹...")
    
    trainer.save_model(config.lora_output)
    tokenizer.save_pretrained(config.lora_output)
    
    elapsed = time.time() - start_time
    hours = int(elapsed // 3600)
    minutes = int((elapsed % 3600) // 60)
    
    log_print(f"âœ“ è¨“ç·´å®Œæˆï¼")
    log_print(f"âœ“ è¨“ç·´æ™‚é–“: {hours} å°æ™‚ {minutes} åˆ†é˜")
    log_print(f"âœ“ LoRA adapter å·²ä¿å­˜åˆ°: {config.lora_output}")
    log_print("=" * 70)
    
    # å¦‚æœåœ¨ Colabï¼Œä¿å­˜åˆ° Google Drive
    if IN_COLAB:
        log_print("")
        log_print("å˜—è©¦ä¿å­˜åˆ° Google Drive...")
        
        try:
            from google.colab import drive
            import shutil
            
            # æª¢æŸ¥ Drive æ˜¯å¦å·²æ›è¼‰
            if not os.path.exists('/content/drive'):
                drive.mount('/content/drive')
            
            # ä¿å­˜è·¯å¾‘
            save_base = f'/content/drive/MyDrive/LegalDelta_Results/{experiment_name}'
            os.makedirs(save_base, exist_ok=True)
            
            # è¤‡è£½ LoRA adapter
            shutil.copytree(config.lora_output, f'{save_base}/lora_adapter')
            
            # è¤‡è£½æ—¥èªŒ
            if os.path.exists(config.log_dir):
                shutil.copytree(config.log_dir, f'{save_base}/logs', dirs_exist_ok=True)
            
            log_print(f"âœ“ çµæœå·²ä¿å­˜åˆ° Google Drive: {save_base}")
            
        except Exception as e:
            log_print(f"âš ï¸  ç„¡æ³•ä¿å­˜åˆ° Drive: {e}")
            log_print("è«‹æ‰‹å‹•è¤‡è£½çµæœæª”æ¡ˆ")
    
    log_print("")
    log_print("ğŸ‰ è¨“ç·´æµç¨‹å…¨éƒ¨å®Œæˆï¼")
    
except KeyboardInterrupt:
    log_print("")
    log_print("âš ï¸  è¨“ç·´è¢«ä¸­æ–·")
    log_print("éƒ¨åˆ†çµæœå¯èƒ½å·²ä¿å­˜åœ¨ checkpoint ä¸­")
    
except Exception as e:
    log_print("")
    log_print(f"âŒ è¨“ç·´å¤±æ•—: {e}")
    import traceback
    log_print(traceback.format_exc())
    raise

finally:
    # æ¸…ç†è¨˜æ†¶é«”
    import gc
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()

log_print("")
log_print("=" * 70)
log_print("è…³æœ¬åŸ·è¡ŒçµæŸ")
log_print("=" * 70)

