# ğŸ”„ Colab æ¨¡å‹åˆä½µæŒ‡å—

## ğŸ“‹ æ¦‚è¿°

è¨“ç·´å®Œæˆå¾Œï¼Œæ‚¨éœ€è¦å°‡ **LoRA adapter** èˆ‡ **åŸºç¤æ¨¡å‹** åˆä½µï¼Œæ‰èƒ½å¾—åˆ°å®Œæ•´çš„å¯ç”¨æ¨¡å‹ã€‚

---

## ğŸ¯ åˆä½µæµç¨‹

```
è¨“ç·´å®Œæˆ
   â†“
ç”Ÿæˆ LoRA adapter (/content/Colab/lora_adapter)
   â†“
åŸ·è¡Œ merge_lora_colab.py
   â†“
åˆä½µå¾Œçš„å®Œæ•´æ¨¡å‹ (/content/Colab/merged_model)
   â†“
å¯ç›´æ¥ç”¨æ–¼æ¨ç†
```

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### æ–¹æ³•ä¸€ï¼šåŸºæœ¬ä½¿ç”¨ï¼ˆè‡ªå‹•é…ç½®ï¼‰

```python
# åœ¨ Colab ä¸­åŸ·è¡Œ
!python merge_lora_colab.py
```

**è‡ªå‹•ä½¿ç”¨çš„è·¯å¾‘ï¼š**
- LoRA adapter: `/content/Colab/lora_adapter`
- Base model: `/content/Colab/Qwen2.5-3B-Instruct`
- è¼¸å‡º: `/content/Colab/merged_model`

---

### æ–¹æ³•äºŒï¼šè‡ªè¨‚è·¯å¾‘

```python
!python merge_lora_colab.py \
  --lora_adapter_path /content/Colab/lora_adapter \
  --base_model_path /content/Colab/Qwen2.5-3B-Instruct \
  --save_path /content/Colab/my_merged_model
```

---

### æ–¹æ³•ä¸‰ï¼šè¨˜æ†¶é«”ä¸è¶³æ™‚ä½¿ç”¨ CPU

```python
# å¦‚æœ GPU è¨˜æ†¶é«”ä¸è¶³
!python merge_lora_colab.py --device_map cpu
```

**æ³¨æ„ï¼š** CPU æ¨¡å¼æœƒè¼ƒæ…¢ï¼Œä½†å¯é¿å… OOMï¼ˆOut of Memoryï¼‰éŒ¯èª¤ã€‚

---

### æ–¹æ³•å››ï¼šè‡ªå‹•å‚™ä»½åˆ° Google Drive

```python
# åˆä½µå¾Œè‡ªå‹•å‚™ä»½åˆ° Google Drive
!python merge_lora_colab.py --save_to_drive
```

**å‚™ä»½ä½ç½®ï¼š** `/content/drive/MyDrive/LegalDelta/merged_models/merged_model_YYYYMMDD_HHMMSS`

---

## ğŸ“‚ æª”æ¡ˆçµæ§‹

### åˆä½µå‰

```
/content/Colab/
â”œâ”€â”€ Qwen2.5-3B-Instruct/          â† åŸºç¤æ¨¡å‹
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ model.safetensors
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ lora_adapter/                  â† è¨“ç·´ç”¢ç”Ÿçš„ LoRA adapter
    â”œâ”€â”€ adapter_config.json
    â”œâ”€â”€ adapter_model.safetensors
    â””â”€â”€ ...
```

### åˆä½µå¾Œ

```
/content/Colab/
â””â”€â”€ merged_model/                  â† åˆä½µå¾Œçš„å®Œæ•´æ¨¡å‹
    â”œâ”€â”€ config.json
    â”œâ”€â”€ model.safetensors
    â”œâ”€â”€ tokenizer.json
    â””â”€â”€ ...
```

---

## ğŸ¬ å®Œæ•´ç¯„ä¾‹ï¼ˆå¾è¨“ç·´åˆ°æ¨ç†ï¼‰

```python
# ===== æ­¥é©Ÿ 1: è¨“ç·´ =====
!python train_colab.py

# è¨“ç·´å®Œæˆå¾Œæœƒç”Ÿæˆ /content/Colab/lora_adapter

# ===== æ­¥é©Ÿ 2: åˆä½µ =====
!python merge_lora_colab.py --save_to_drive

# åˆä½µå®Œæˆå¾Œæœƒç”Ÿæˆ /content/Colab/merged_model

# ===== æ­¥é©Ÿ 3: æ¸¬è©¦æ¨ç† =====
from transformers import AutoModelForCausalLM, AutoTokenizer

# è¼‰å…¥åˆä½µå¾Œçš„æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "/content/Colab/merged_model",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("/content/Colab/merged_model")

# æº–å‚™æ³•å¾‹å•é¡Œ
system_prompt = """ç”¨æˆ·ä¸åŠ©æ‰‹ä¹‹é—´çš„å¯¹è¯ã€‚ç”¨æˆ·æå‡ºé—®é¢˜ï¼ŒåŠ©æ‰‹è§£å†³å®ƒã€‚åŠ©æ‰‹é¦–å…ˆæ€è€ƒæ¨ç†è¿‡ç¨‹ï¼Œç„¶åæä¾›ç”¨æˆ·ç­”æ¡ˆã€‚æ¨ç†è¿‡ç¨‹å’Œç­”æ¡ˆåˆ†åˆ«åŒ…å«åœ¨ <reasoning> </reasoning> å’Œ <answer> </answer> æ ‡ç­¾ä¸­ã€‚"""

question = "ä¾å‹å‹•åŸºæº–æ³•è¦å®šï¼Œé›‡ä¸»å»¶é•·å‹å·¥ä¹‹å·¥ä½œæ™‚é–“é€£åŒæ­£å¸¸å·¥ä½œæ™‚é–“ï¼Œæ¯æ—¥ä¸å¾—è¶…éå¤šå°‘å°æ™‚ï¼Ÿ"

# æ ¼å¼åŒ–è¼¸å…¥
messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": question}
]

# ç”Ÿæˆå›ç­”
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
inputs = tokenizer([text], return_tensors="pt").to(model.device)
outputs = model.generate(
    **inputs,
    max_new_tokens=512,
    temperature=0.7,
    do_sample=True
)

# è§£ç¢¼è¼¸å‡º
response = tokenizer.decode(outputs[0], skip_special_tokens=False)
print(response)
```

---

## âš™ï¸ åƒæ•¸èªªæ˜

| åƒæ•¸ | èªªæ˜ | é è¨­å€¼ |
|------|------|--------|
| `--lora_adapter_path` | LoRA adapter è·¯å¾‘ | `/content/Colab/lora_adapter` |
| `--base_model_path` | åŸºç¤æ¨¡å‹è·¯å¾‘ | `/content/Colab/Qwen2.5-3B-Instruct` |
| `--save_path` | åˆä½µå¾Œæ¨¡å‹å„²å­˜è·¯å¾‘ | `/content/Colab/merged_model` |
| `--device_map` | è£ç½®æ˜ å°„ (auto/cpu/cuda) | `auto` |
| `--save_to_drive` | æ˜¯å¦å‚™ä»½åˆ° Google Drive | `False` |
| `--drive_path` | Google Drive å‚™ä»½è·¯å¾‘ | `/content/drive/MyDrive/LegalDelta/merged_models` |

---

## ğŸ› ï¸ å¸¸è¦‹å•é¡Œ

### Q1: åˆä½µæ™‚é‡åˆ° OOMï¼ˆè¨˜æ†¶é«”ä¸è¶³ï¼‰

**è§£æ±ºæ–¹æ¡ˆï¼š**
```python
!python merge_lora_colab.py --device_map cpu
```

æˆ–è€…æ¸…ç†è¨˜æ†¶é«”å¾Œé‡è©¦ï¼š
```python
import gc
import torch

gc.collect()
torch.cuda.empty_cache()

!python merge_lora_colab.py
```

---

### Q2: `adapter_config.json` æ‰¾ä¸åˆ°

**åŸå› ï¼š** è¨“ç·´æœªå®Œæˆæˆ– LoRA adapter æœªæˆåŠŸä¿å­˜ã€‚

**è§£æ±ºæ–¹æ¡ˆï¼š**
1. æª¢æŸ¥è¨“ç·´æ—¥èªŒï¼Œç¢ºèªè¨“ç·´å·²å®Œæˆ
2. ç¢ºèª `/content/Colab/lora_adapter` ç›®éŒ„å­˜åœ¨ä¸”åŒ…å«ä»¥ä¸‹æª”æ¡ˆï¼š
   - `adapter_config.json`
   - `adapter_model.safetensors`

```python
# æª¢æŸ¥æª”æ¡ˆ
!ls -lh /content/Colab/lora_adapter
```

---

### Q3: åˆä½µå¾Œæ¨¡å‹ç„¡æ³•è¼‰å…¥

**è§£æ±ºæ–¹æ¡ˆï¼š**
æª¢æŸ¥åˆä½µå¾Œçš„æ¨¡å‹æª”æ¡ˆæ˜¯å¦å®Œæ•´ï¼š
```python
!ls -lh /content/Colab/merged_model

# æ‡‰è©²åŒ…å«ï¼š
# - config.json
# - model.safetensors (æˆ– model-00001-of-0000X.safetensors)
# - tokenizer.json
# - tokenizer_config.json
# - special_tokens_map.json
```

---

### Q4: `frozenset` éŒ¯èª¤

**éŒ¯èª¤è¨Šæ¯ï¼š**
```
AttributeError: 'frozenset' object has no attribute 'discard'
```

**åŸå› ï¼š** åœ¨é‡åŒ–æ¨¡å¼ä¸‹è¨“ç·´çš„æ¨¡å‹ï¼Œåˆä½µæ™‚å¯èƒ½é‡åˆ°æ­¤å•é¡Œã€‚

**è§£æ±ºæ–¹æ¡ˆï¼š**
1. ç¢ºä¿è¨“ç·´æ™‚ä½¿ç”¨ `load_in_4bit=True` ä½†åˆä½µæ™‚ä½¿ç”¨ `float16`
2. å¦‚æœå•é¡ŒæŒçºŒï¼Œè«‹åœ¨è¨“ç·´æ™‚æ”¹ç”¨ `torch_dtype=torch.float16` è€Œéé‡åŒ–

---

### Q5: åˆä½µéœ€è¦å¤šä¹…ï¼Ÿ

**æ™‚é–“ä¼°ç®—ï¼š**
- **3B æ¨¡å‹ï¼ˆColab T4 GPUï¼‰ï¼š** ç´„ 5-10 åˆ†é˜
- **3B æ¨¡å‹ï¼ˆCPU æ¨¡å¼ï¼‰ï¼š** ç´„ 15-30 åˆ†é˜
- **14B æ¨¡å‹ï¼š** ä¸å»ºè­°åœ¨ Colab å…è²»ç‰ˆåˆä½µï¼ˆè¨˜æ†¶é«”ä¸è¶³ï¼‰

---

## ğŸ“Š åˆä½µéç¨‹è¼¸å‡ºç¯„ä¾‹

```
======================================================================
ğŸŒ©ï¸ Colab LoRA åˆä½µå·¥å…·
======================================================================

[2025-10-26 14:30:00] âœ“ åœ¨ Google Colab ç’°å¢ƒä¸­

[2025-10-26 14:30:01] ğŸ“‹ é…ç½®ä¿¡æ¯ï¼š
[2025-10-26 14:30:01]   LoRA adapter: /content/Colab/lora_adapter
[2025-10-26 14:30:01]   Base model: /content/Colab/Qwen2.5-3B-Instruct
[2025-10-26 14:30:01]   Save to: /content/Colab/merged_model
[2025-10-26 14:30:01]   Device: auto

[2025-10-26 14:30:02] ğŸ” æª¢æŸ¥æª”æ¡ˆ...
[2025-10-26 14:30:02]   âœ“ LoRA adapter æ‰¾åˆ°
[2025-10-26 14:30:02]   âœ“ Base model æ‰¾åˆ°
[2025-10-26 14:30:02]   âœ“ adapter_config.json æ‰¾åˆ°

[2025-10-26 14:30:03] ğŸ“‹ è¼‰å…¥ LoRA é…ç½®...
[2025-10-26 14:30:03]   âœ“ é…ç½®è¼‰å…¥æˆåŠŸ

[2025-10-26 14:30:04] ğŸ“ è¼‰å…¥ tokenizer...
[2025-10-26 14:30:04]   âœ“ Tokenizer è¼‰å…¥æˆåŠŸ

[2025-10-26 14:30:05] ğŸ§  è¼‰å…¥åŸºç¤æ¨¡å‹...
[2025-10-26 14:30:05]   é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜...
[2025-10-26 14:32:15]   âœ“ åŸºç¤æ¨¡å‹è¼‰å…¥æˆåŠŸ

[2025-10-26 14:32:16] ğŸ”§ è¼‰å…¥ LoRA adapter...
[2025-10-26 14:32:30]   âœ“ LoRA adapter è¼‰å…¥æˆåŠŸ

[2025-10-26 14:32:31] ğŸ”„ åˆä½µ LoRA æ¬Šé‡åˆ°åŸºç¤æ¨¡å‹...
[2025-10-26 14:32:31]   é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜...
[2025-10-26 14:35:12]   âœ“ åˆä½µå®Œæˆ

[2025-10-26 14:35:13] ğŸ§¹ æ¸…ç†è¨˜æ†¶é«”...
[2025-10-26 14:35:14]   âœ“ è¨˜æ†¶é«”æ¸…ç†å®Œæˆ

[2025-10-26 14:35:15] ğŸ’¾ å„²å­˜åˆä½µå¾Œçš„æ¨¡å‹åˆ° /content/Colab/merged_model...
[2025-10-26 14:35:15]   é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜...
[2025-10-26 14:35:16]   ç§»å‹•æ¨¡å‹åˆ° CPU...
[2025-10-26 14:38:45]   âœ“ æ¨¡å‹å„²å­˜æˆåŠŸ

[2025-10-26 14:38:46] ğŸ§¹ æœ€çµ‚æ¸…ç†...

======================================================================
[2025-10-26 14:38:47] âœ… åˆä½µå®Œæˆï¼
======================================================================

[2025-10-26 14:38:47] ğŸ“‚ åˆä½µå¾Œçš„æ¨¡å‹ä½ç½®:
[2025-10-26 14:38:47]    /content/Colab/merged_model

[2025-10-26 14:38:47] ğŸš€ ä¸‹ä¸€æ­¥ï¼š
[2025-10-26 14:38:47]    1. ä½¿ç”¨åˆä½µå¾Œçš„æ¨¡å‹é€²è¡Œæ¨ç†
[2025-10-26 14:38:47]    2. å¦‚æœåœ¨ Colabï¼Œè¨˜å¾—å‚™ä»½åˆ° Google Drive
[2025-10-26 14:38:47]    3. æ¨ç†ç¯„ä¾‹ï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained('/content/Colab/merged_model')
tokenizer = AutoTokenizer.from_pretrained('/content/Colab/merged_model')

prompt = 'ä¾å‹å‹•åŸºæº–æ³•è¦å®šï¼Œé›‡ä¸»å»¶é•·å‹å·¥ä¹‹å·¥ä½œæ™‚é–“...'
inputs = tokenizer(prompt, return_tensors='pt')
outputs = model.generate(**inputs, max_length=512)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

======================================================================
```

---

## ğŸ’¡ æœ€ä½³å¯¦è¸

1. **è¨“ç·´å®Œæˆå¾Œç«‹å³åˆä½µ**
   - Colab æœƒè©±æœ‰æ™‚é™ï¼Œè¨“ç·´å®Œæˆå¾Œå„˜å¿«åˆä½µ

2. **å‚™ä»½åˆ° Google Drive**
   ```python
   !python merge_lora_colab.py --save_to_drive
   ```

3. **é©—è­‰åˆä½µçµæœ**
   ```python
   # æ¸¬è©¦è¼‰å…¥
   from transformers import AutoModelForCausalLM, AutoTokenizer
   
   model = AutoModelForCausalLM.from_pretrained("/content/Colab/merged_model")
   tokenizer = AutoTokenizer.from_pretrained("/content/Colab/merged_model")
   
   print("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸï¼")
   ```

4. **æ¸…ç†è‡¨æ™‚æª”æ¡ˆ**
   ```python
   # åˆä½µå®Œæˆå¾Œï¼Œå¦‚æœç©ºé–“ä¸è¶³ï¼Œå¯ä»¥åˆªé™¤ lora_adapter
   # ï¼ˆä½†å»ºè­°å…ˆå‚™ä»½åˆ° Driveï¼‰
   !rm -rf /content/Colab/lora_adapter
   ```

---

## ğŸ“š ç›¸é—œæ–‡ä»¶

- **`train_colab.py`** - Colab è¨“ç·´è…³æœ¬
- **`COLAB_QUICKSTART.md`** - Colab å¿«é€Ÿé–‹å§‹æŒ‡å—
- **`è¨“ç·´æŒ‡æ¨™èªªæ˜.md`** - è¨“ç·´æŒ‡æ¨™è©³è§£

---

## ğŸ†˜ éœ€è¦å¹«åŠ©ï¼Ÿ

å¦‚æœé‡åˆ°å•é¡Œï¼Œè«‹æª¢æŸ¥ï¼š
1. è¨“ç·´æ˜¯å¦æˆåŠŸå®Œæˆ
2. LoRA adapter æª”æ¡ˆæ˜¯å¦å®Œæ•´
3. GPU/CPU è¨˜æ†¶é«”æ˜¯å¦è¶³å¤ 
4. è·¯å¾‘æ˜¯å¦æ­£ç¢º

**å¸¸ç”¨é™¤éŒ¯æŒ‡ä»¤ï¼š**
```python
# æª¢æŸ¥ LoRA adapter
!ls -lh /content/Colab/lora_adapter

# æª¢æŸ¥ GPU è¨˜æ†¶é«”
!nvidia-smi

# æª¢æŸ¥å¯ç”¨ç©ºé–“
!df -h /content
```

---

**ç¥æ‚¨åˆä½µé †åˆ©ï¼** ğŸ‰

