#!/usr/bin/env python3
"""
åˆä½µ LoRA adapter èˆ‡åŸºç¤æ¨¡å‹

ä½¿ç”¨æ–¹æ³•:
    python src/merge_lora.py
    python src/merge_lora.py --device_map cpu  # è¨˜æ†¶é«”ä¸è¶³æ™‚ä½¿ç”¨
"""

import argparse
import json
import os
import gc

import torch
from peft import PeftConfig, PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer


def main():
    """ä¸»å‡½æ•¸"""
    # è¨­å®šé›¢ç·šæ¨¡å¼
    os.environ["HF_HUB_OFFLINE"] = "1"
    os.environ["TRANSFORMERS_OFFLINE"] = "1"
    
    # è§£æåƒæ•¸
    parser = argparse.ArgumentParser(description="Merge LoRA adapter with base model")
    parser.add_argument(
        "--model_name_or_path", 
        type=str, 
        default="./scripts/grpo_trainer_lora_model",
        help="Path to the LoRA adapter"
    )
    parser.add_argument(
        "--save_path", 
        type=str,
        default="./LegalDelta/Qwen2.5-3B-merge",
        help="Path to save the merged model"
    )
    parser.add_argument(
        "--base_model_path", 
        type=str,
        default="./Qwen2.5-3B-Instruct", 
        help="Path to base model"
    )
    parser.add_argument(
        "--device_map",
        type=str,
        default="auto",
        help="Device map (auto/cpu/cuda)"
    )
    
    args = parser.parse_args()
    
    # æ¨™æº–åŒ–è·¯å¾‘
    if args.model_name_or_path.startswith('./'):
        args.model_name_or_path = args.model_name_or_path[2:]
    
    print("=" * 60)
    print("LoRA æ¨¡å‹åˆä½µå·¥å…·")
    print("=" * 60)
    print(f"\nLoRA adapter: {args.model_name_or_path}")
    print(f"Base model: {args.base_model_path}")
    print(f"Save to: {args.save_path}")
    print(f"Device: {args.device_map}\n")
    
    # é©—è­‰è·¯å¾‘
    if not os.path.exists(args.model_name_or_path):
        raise FileNotFoundError(f"LoRA adapter not found: {args.model_name_or_path}")
    
    # è¼‰å…¥é…ç½®
    print("ğŸ“‹ Loading LoRA config...")
    try:
        config = PeftConfig.from_pretrained(args.model_name_or_path, local_files_only=True)
    except Exception:
        # Fallback: ç›´æ¥å¾ JSON è¼‰å…¥
        config_path = os.path.join(args.model_name_or_path, "adapter_config.json")
        with open(config_path, 'r') as f:
            config_dict = json.load(f)
        from peft import LoraConfig
        config = LoraConfig(**config_dict)
    
    # ç¢ºå®šåŸºç¤æ¨¡å‹è·¯å¾‘
    base_model_path = args.base_model_path
    if not os.path.exists(base_model_path):
        # å˜—è©¦å¾é…ç½®ä¸­ç²å–
        base_model_path = config.base_model_name_or_path
        if not os.path.exists(base_model_path):
            raise FileNotFoundError(f"Base model not found: {base_model_path}")
    
    # è¼‰å…¥ tokenizer
    print("ğŸ“ Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        base_model_path,
        local_files_only=True,
        trust_remote_code=True
    )
    
    # è¼‰å…¥åŸºç¤æ¨¡å‹
    print("ğŸ§  Loading base model...")
    model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.float16,
        device_map=args.device_map,
        trust_remote_code=True,
        local_files_only=True,
        low_cpu_mem_usage=True
    )
    print("   âœ“ Base model loaded")
    
    # è¼‰å…¥ LoRA adapter
    print("ğŸ”§ Loading LoRA adapter...")
    model = PeftModel.from_pretrained(
        model,
        args.model_name_or_path,
        local_files_only=True,
        is_trainable=False
    )
    print("   âœ“ LoRA adapter loaded")
    
    # åˆä½µ
    print("ğŸ”„ Merging LoRA with base model...")
    model = model.merge_and_unload()
    print("   âœ“ Merge completed")
    
    # æ¸…ç†è¨˜æ†¶é«”
    print("ğŸ§¹ Cleaning memory...")
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # å„²å­˜
    print(f"ğŸ’¾ Saving merged model to {args.save_path}...")
    os.makedirs(args.save_path, exist_ok=True)
    
    # ç§»å‹•åˆ° CPU ä»¥ç¯€çœè¨˜æ†¶é«”
    model = model.cpu()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # å„²å­˜æ¨¡å‹
    model.save_pretrained(
        args.save_path,
        max_shard_size="1GB",
        safe_serialization=True
    )
    tokenizer.save_pretrained(args.save_path)
    
    # æœ€çµ‚æ¸…ç†
    del model
    del tokenizer
    gc.collect()
    
    print("\n" + "=" * 60)
    print("âœ… Model merged successfully!")
    print(f"âœ… Saved to: {args.save_path}")
    print("=" * 60)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Interrupted by user")
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        print("\nğŸ’¡ Tips:")
        print("   - If out of memory: python src/merge_lora.py --device_map cpu")
        print("   - Check paths are correct")
        raise
