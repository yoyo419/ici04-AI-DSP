import os
import json
import re
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass, asdict
import PyPDF2
from openai import OpenAI
import tiktoken
from pathlib import Path
# pip install PyPDF2 openai tiktoken

# note: 
    # åŒ…å«å…¨éƒ¨pdfæª”çš„è™•ç†ä¸€æ¬¡ç´„è¦700000tokensï¼Œä¸çŸ¥é“è€å¸«æ˜¯ç”¨å“ªå€‹æ¨¡åž‹ä¸éŽç®—é«˜ä¸€é»žå¯èƒ½è¦400å°å¹£
    # å¦‚æžœè¦æ¸¬è©¦å¯ä»¥æŠŠä¸è¦ä¸€æ¬¡è™•ç†å…¨éƒ¨çš„æª”æ¡ˆï¼Œæˆ–æ˜¯æ¸›å°‘ä½¿ç”¨tokenï¼Œctrl+f "user-changable" æ‡‰è©²å¯ä»¥æ‰¾åˆ°å¯ä»¥èª¿æ•´çš„åœ°æ–¹

@dataclass
class ChunkMetadata:
    """Metadata for each content chunk"""
    chapter: str
    section: str
    article: str
    paragraph: int
    chunk_id: str
    parent_id: str
    level: int
    content_type: str  # 'text', 'table', 'formula'
    
@dataclass
class ProcessedChunk:
    """Processed content chunk with metadata"""
    content: str
    metadata: ChunkMetadata
    original_content: str  # original text before processing
    tokens: int

class LegalDocumentPreprocessor:
    """
    Preprocessing system for occupational safety legal norms
    Handles PDF loading, hierarchical chunking, table/formula detection and transformation
    """
    
    def __init__(self, openai_api_key: str, model: str = "gpt-4"):
        self.client = OpenAI(api_key=openai_api_key)
        self.model = model
        self.encoding = tiktoken.encoding_for_model("gpt-4")
        
        # Hierarchy patterns for legal documents
        self.hierarchy_patterns = {
            'chapter': r'ç¬¬\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒè¬]+)\s*ç« \s+(.+)',
            'section': r'ç¬¬\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒè¬]+)\s*ç¯€\s+(.+)',
            'article': r'ç¬¬\s*(\d+)\s*æ¢',
            'paragraph': r'^(\d+)\s+',
        }
        
        # Table detection pattern
        self.table_pattern = r'â”Œ[\s\S]*?â””'
        
        # Formula patterns
        self.formula_patterns = [
            r'[A-Za-zÎ±-Ï‰Î‘-Î©]+\s*[ï¼=]\s*[^ã€‚\n]+',
            r'å¼ä¸­.*?[ï¼š:].+',
        ]
        
        # Mandarin number conversion
        self.mandarin_numbers = {
            'â—‹': '0', 'ä¸€': '1', 'äºŒ': '2', 'ä¸‰': '3', 'å››': '4',
            'äº”': '5', 'å…­': '6', 'ä¸ƒ': '7', 'å…«': '8', 'ä¹': '9',
            'å': '10', 'ç™¾': '100', 'åƒ': '1000', 'è¬': '10000'
        }
        
    def load_pdfs_from_folder(self, folder_path: str) -> List[Dict[str, Any]]:
        # Load all PDFs from a folder and extract text
        documents = []
        folder = Path(folder_path)
        
        for pdf_file in folder.glob('*.pdf'):
            try:
                with open(pdf_file, 'rb') as file:
                    pdf_reader = PyPDF2.PdfReader(file)
                    text = ""
                    for page in pdf_reader.pages:
                        text += page.extract_text() + "\n"
                    
                    documents.append({
                        'filename': pdf_file.name,
                        'text': text,
                        'num_pages': len(pdf_reader.pages)
                    })
                    print(f"âœ“ Loaded: {pdf_file.name} ({len(pdf_reader.pages)} pages)")
            except Exception as e:
                print(f"âœ— Error loading {pdf_file.name}: {e}")
        
        return documents
    
    def convert_mandarin_numbers(self, text: str) -> str:
        # Convert Mandarin number representations to Arabic numerals
        pattern = r'([â—‹ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)[ï¼Ž\.]([â—‹ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)'
        
        def replace_number(match):
            integer_part = match.group(1)
            decimal_part = match.group(2)
            
            # Convert integer part
            integer_str = ''.join(self.mandarin_numbers.get(c, c) for c in integer_part)
            # Convert decimal part
            decimal_str = ''.join(self.mandarin_numbers.get(c, c) for c in decimal_part)
            
            return f"{integer_str}.{decimal_str}"
        
        text = re.sub(pattern, replace_number, text)
        
        # Convert standalone Mandarin numbers
        for mandarin, arabic in self.mandarin_numbers.items():
            text = text.replace(mandarin, arabic)
        
        return text
    
    def detect_hierarchy(self, text: str) -> Dict[str, Any]:
        # Detect and return hierarchical structure in legal document
        # This can be improved with more detailed patterns, for example, adding "ç¯€"
        hierarchy = {
            'chapter': None,
            'chapter_title': None,
            'section': None,
            'section_title': None,
            'article': None,
            'paragraph': None
        }
        
        # Check for chapter
        chapter_match = re.search(self.hierarchy_patterns['chapter'], text)
        if chapter_match:
            hierarchy['chapter'] = chapter_match.group(1)
            hierarchy['chapter_title'] = chapter_match.group(2).strip()
        
        # Check for section
        section_match = re.search(self.hierarchy_patterns['section'], text)
        if section_match:
            hierarchy['section'] = section_match.group(1)
            hierarchy['section_title'] = section_match.group(2).strip()
        
        # Check for article
        article_match = re.search(self.hierarchy_patterns['article'], text)
        if article_match:
            hierarchy['article'] = article_match.group(1)
        
        # Check for paragraph
        paragraph_match = re.search(self.hierarchy_patterns['paragraph'], text)
        if paragraph_match:
            hierarchy['paragraph'] = paragraph_match.group(1)
        
        return hierarchy
    
    def detect_tables(self, text: str) -> List[Tuple[str, int, int]]:
        # Detect tables constructed with box-drawing characters. Returns list of (table_text, start_pos, end_pos).
        # Recall: self.table_pattern = r'â”Œ[\s\S]*?â””'
        # This can be improved by considering more complex table structures
        tables = []
        for match in re.finditer(self.table_pattern, text, re.DOTALL):
            tables.append((match.group(0), match.start(), match.end()))
        return tables
    
    def detect_formulas(self, text: str) -> List[Tuple[str, int, int]]:
        # Detect mathematical formulas in text. Returns list of (formula_text, start_pos, end_pos).
        # Recall:ã€€self.formula_patterns = [r'[A-Za-zÎ±-Ï‰Î‘-Î©]+\s*[ï¼=]\s*[^ã€‚\n]+',ã€€r'å¼ä¸­.*?[ï¼š:].+',]
        # This can be improved by considering more complex formula structures
        formulas = []
        for pattern in self.formula_patterns:
            for match in re.finditer(pattern, text):
                formulas.append((match.group(0), match.start(), match.end()))
        return formulas
    
    def table_to_statements(self, table_text: str) -> str:
        # Use OpenAI API to convert table to natural language statements
        prompt = f"""ä½ æ˜¯ä¸€ä½å°ˆç²¾æ–¼è·æ¥­å®‰å…¨æ³•è¦çš„å°ˆå®¶ã€‚è«‹å°‡ä»¥ä¸‹è¡¨æ ¼è½‰æ›ç‚ºæ¸…æ™°ã€è©³ç´°çš„é™³è¿°å¥ï¼Œä¿ç•™æ‰€æœ‰é‡è¦ä¿¡æ¯å’Œæ•¸å€¼é—œä¿‚ã€‚

è¡¨æ ¼å…§å®¹ï¼š
{table_text}

è¦æ±‚ï¼š
1. å°‡è¡¨æ ¼ä¸­çš„æ¯ä¸€è¡Œè½‰æ›ç‚ºå®Œæ•´çš„é™³è¿°å¥
2. æ¸…æ¥šæè¿°å„æ¬„ä½ä¹‹é–“çš„é—œä¿‚
3. ä¿ç•™æ‰€æœ‰æ•¸å€¼å’Œå–®ä½
4. ä½¿ç”¨è‡ªç„¶ã€æµæš¢çš„ä¸­æ–‡è¡¨é”
5. ç¢ºä¿å°ˆæ¥­è¡“èªžçš„æº–ç¢ºæ€§

è«‹æä¾›è½‰æ›å¾Œçš„é™³è¿°å¥ï¼š"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯è·æ¥­å®‰å…¨è¡›ç”Ÿæ³•è¦å°ˆå®¶ï¼Œæ“…é•·å°‡è¤‡é›œè¡¨æ ¼è½‰æ›ç‚ºæ¸…æ™°çš„æ–‡å­—æè¿°ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=2000 ######## user-changable (especially for testing) ########
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"Error in table conversion: {e}")
            return f"[TABLE_CONVERSION_ERROR] {table_text[:100]}..."
    
    def formula_to_statements(self, formula_text: str, context: str = "") -> str:
        """
        Use OpenAI API to convert formula to natural language statements
        """
        prompt = f"""ä½ æ˜¯ä¸€ä½å°ˆç²¾æ–¼è·æ¥­å®‰å…¨æ³•è¦çš„å°ˆå®¶ã€‚è«‹å°‡ä»¥ä¸‹æ•¸å­¸å…¬å¼è½‰æ›ç‚ºæ¸…æ™°ã€è©³ç´°çš„æ–‡å­—æè¿°ã€‚

å…¬å¼å…§å®¹ï¼š
{formula_text}

ä¸Šä¸‹æ–‡ï¼š
{context[:500]}

è¦æ±‚ï¼š
1. è©³ç´°è§£é‡‹å…¬å¼ä¸­æ¯å€‹è®Šæ•¸çš„æ„ç¾©
2. èªªæ˜Žå„è®Šæ•¸ä¹‹é–“çš„æ•¸å­¸é—œä¿‚ï¼ˆæ³¨æ„ï¼šç›¸é„°å­—æ¯å¯èƒ½è¡¨ç¤ºç›¸ä¹˜ï¼‰
3. è§£é‡‹å…¬å¼çš„ç‰©ç†æˆ–å¯¦å‹™æ„ç¾©
4. åŒ…å«è®Šæ•¸çš„å–®ä½ä¿¡æ¯
5. ä½¿ç”¨è‡ªç„¶ã€æµæš¢çš„ä¸­æ–‡è¡¨é”
6. å¦‚æžœå…¬å¼ä¸­æœ‰åˆ†æ•¸ã€ä¸Šæ¨™ã€ä¸‹æ¨™ï¼Œè«‹æ¸…æ¥šèªªæ˜Ž

ç¯„ä¾‹ï¼š
è¼¸å…¥ï¼šW ï¼qCA
è¼¸å‡ºï¼šé¢¨è·é‡Wç­‰æ–¼é€Ÿåº¦å£“qã€é¢¨åŠ›ä¿‚æ•¸Cèˆ‡å—é¢¨é¢ç©Açš„ä¹˜ç©ã€‚å…¶ä¸­Wä»£è¡¨é¢¨è·é‡ï¼Œå–®ä½ç‚ºç‰›é “ï¼›qä»£è¡¨é€Ÿåº¦å£“ï¼Œå–®ä½ç‚ºç‰›é “æ¯å¹³æ–¹å…¬å°ºï¼›Cä»£è¡¨é¢¨åŠ›ä¿‚æ•¸ï¼›Aä»£è¡¨å—é¢¨é¢ç©ï¼Œå–®ä½ç‚ºå¹³æ–¹å…¬å°ºã€‚

è«‹æä¾›è½‰æ›å¾Œçš„æè¿°ï¼š"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯è·æ¥­å®‰å…¨è¡›ç”Ÿæ³•è¦å°ˆå®¶ï¼Œæ“…é•·è§£é‡‹æ•¸å­¸å…¬å¼å’ŒæŠ€è¡“è¦ç¯„ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=1500 ######## user-changable (especially for testing) ########
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"Error in formula conversion: {e}")
            return f"[FORMULA_CONVERSION_ERROR] {formula_text[:100]}..."
    
    def intelligent_chunk(self, text: str, max_tokens: int = 400) -> List[str]:
        # Intelligently chunk text while preserving sentence boundaries

        # Split by sentences (Chinese period, exclamation, question marks)
        sentences = re.split(r'([ã€‚ï¼ï¼Ÿ\n])', text)
        
        # Reconstruct sentences with their delimiters
        sentences = [''.join(sentences[i:i+2]) for i in range(0, len(sentences)-1, 2)]
        if len(sentences) % 2 == 1:
            sentences.append(sentences[-1])
        
        chunks = []
        current_chunk = ""
        current_tokens = 0
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            sentence_tokens = len(self.encoding.encode(sentence))
            
            # If adding this sentence exceeds limit and we have content, save chunk
            if current_tokens + sentence_tokens > max_tokens and current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = sentence
                current_tokens = sentence_tokens
            else:
                current_chunk += sentence
                current_tokens += sentence_tokens
        
        # Add remaining content
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def process_document(self, document: Dict[str, Any], max_chunk_tokens: int = 400) -> List[ProcessedChunk]:
        # Main processing pipeline for a single document
        text = document['text']
        filename = document['filename']
        
        # Step 1: Convert Mandarin numbers
        text = self.convert_mandarin_numbers(text)
        
        # Step 2: Detect and process tables
        tables = self.detect_tables(text)
        table_replacements = {}
        for table_text, start, end in tables:
            print(f"Processing table at position {start}...")
            statements = self.table_to_statements(table_text)
            table_replacements[(start, end)] = statements
        
        # Step 3: Detect and process formulas
        formulas = self.detect_formulas(text)
        formula_replacements = {}
        for formula_text, start, end in formulas:
            # Get context around formula (Â±200 chars)
            context_start = max(0, start - 200)
            context_end = min(len(text), end + 200)
            context = text[context_start:context_end]
            
            print(f"Processing formula at position {start}...")
            statements = self.formula_to_statements(formula_text, context)
            formula_replacements[(start, end)] = statements
        
        # Step 4: Replace tables and formulas with statements
        all_replacements = sorted(
            list(table_replacements.items()) + list(formula_replacements.items()),
            key=lambda x: x[0][0],
            reverse=True
        )
        
        processed_text = text
        for (start, end), replacement in all_replacements:
            processed_text = processed_text[:start] + replacement + processed_text[end:]
        
        # Step 5: Split into sections based on hierarchy
        lines = processed_text.split('\n')
        
        current_hierarchy = {
            'chapter': None,
            'chapter_title': None,
            'section': None,
            'section_title': None,
            'article': None
        }
        
        chunks = []
        current_content = ""
        chunk_counter = 0
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Update hierarchy
            hierarchy = self.detect_hierarchy(line)
            if hierarchy['chapter']:
                current_hierarchy['chapter'] = hierarchy['chapter']
                current_hierarchy['chapter_title'] = hierarchy['chapter_title']
            if hierarchy['section']:
                current_hierarchy['section'] = hierarchy['section']
                current_hierarchy['section_title'] = hierarchy['section_title']
            if hierarchy['article']:
                # When we hit a new article, chunk previous content
                if current_content:
                    sub_chunks = self.intelligent_chunk(current_content, max_chunk_tokens)
                    for i, sub_chunk in enumerate(sub_chunks):
                        chunk_id = f"{filename}_{chunk_counter}"
                        parent_id = f"{current_hierarchy['chapter']}_{current_hierarchy['section']}_{current_hierarchy['article']}"
                        
                        metadata = ChunkMetadata(
                            chapter=current_hierarchy['chapter'] or 'unknown',
                            section=current_hierarchy['section'] or 'unknown',
                            article=current_hierarchy['article'] or 'unknown',
                            paragraph=i + 1,
                            chunk_id=chunk_id,
                            parent_id=parent_id,
                            level=self._calculate_level(current_hierarchy),
                            content_type='text'
                        )
                        
                        chunks.append(ProcessedChunk(
                            content=sub_chunk,
                            metadata=metadata,
                            original_content=current_content if i == 0 else "",
                            tokens=len(self.encoding.encode(sub_chunk))
                        ))
                        chunk_counter += 1
                    
                    current_content = ""
                
                current_hierarchy['article'] = hierarchy['article']
            
            current_content += line + "\n"
        
        # Process remaining content
        if current_content:
            sub_chunks = self.intelligent_chunk(current_content, max_chunk_tokens)
            for i, sub_chunk in enumerate(sub_chunks):
                chunk_id = f"{filename}_{chunk_counter}"
                parent_id = f"{current_hierarchy['chapter']}_{current_hierarchy['section']}_{current_hierarchy['article']}"
                
                metadata = ChunkMetadata(
                    chapter=current_hierarchy['chapter'] or 'unknown',
                    section=current_hierarchy['section'] or 'unknown',
                    article=current_hierarchy['article'] or 'unknown',
                    paragraph=i + 1,
                    chunk_id=chunk_id,
                    parent_id=parent_id,
                    level=self._calculate_level(current_hierarchy),
                    content_type='text'
                )
                
                chunks.append(ProcessedChunk(
                    content=sub_chunk,
                    metadata=metadata,
                    original_content=current_content if i == 0 else "",
                    tokens=len(self.encoding.encode(sub_chunk))
                ))
                chunk_counter += 1
        
        return chunks
    
    def _calculate_level(self, hierarchy: Dict[str, Any]) -> int:
        # Calculate hierarchical level (0=document, 1=chapter, 2=section, 3=article, 4=paragraph)"""
        if hierarchy['article']:
            return 3
        elif hierarchy['section']:
            return 2
        elif hierarchy['chapter']:
            return 1
        else:
            return 0
    
    def process_all_documents(self, folder_path: str, output_json: str = "preprocessed_data.json"):
        # Process all PDFs in folder and save to JSON
        print("=" * 80)
        print("LEGAL DOCUMENT PREPROCESSING SYSTEM")
        print("=" * 80)
        
        # Load documents
        print(f"\nðŸ“ Loading PDFs from: {folder_path}")
        documents = self.load_pdfs_from_folder(folder_path)
        print(f"âœ“ Loaded {len(documents)} documents\n")
        
        # Process each document
        all_chunks = []
        for i, doc in enumerate(documents, 1):
            print(f"\n{'='*80}")
            print(f"Processing document {i}/{len(documents)}: {doc['filename']}")
            print(f"{'='*80}")
            
            chunks = self.process_document(doc)
            all_chunks.extend(chunks)
            
            print(f"âœ“ Generated {len(chunks)} chunks from {doc['filename']}")
        
        # Save to JSON
        print(f"\nðŸ’¾ Saving to {output_json}...")
        output_data = {
            'metadata': {
                'total_documents': len(documents),
                'total_chunks': len(all_chunks),
                'total_tokens': sum(chunk.tokens for chunk in all_chunks),
                'documents': [doc['filename'] for doc in documents]
            },
            'chunks': [
                {
                    'content': chunk.content,
                    'metadata': asdict(chunk.metadata),
                    'tokens': chunk.tokens,
                    'original_content': chunk.original_content
                }
                for chunk in all_chunks
            ]
        }
        
        with open(output_json, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ“ Preprocessing complete!")
        print(f"\nðŸ“Š Summary:")
        print(f"   - Documents processed: {len(documents)}")
        print(f"   - Total chunks: {len(all_chunks)}")
        print(f"   - Total tokens: {sum(chunk.tokens for chunk in all_chunks):,}")
        print(f"   - Average tokens per chunk: {sum(chunk.tokens for chunk in all_chunks) // len(all_chunks)}")
        
        return output_data


# ============================================================================
# USAGE EXAMPLE
# ============================================================================

if __name__ == "__main__":
    # Initialize preprocessor
    preprocessor = LegalDocumentPreprocessor(
        openai_api_key="PASTE_YOUR_OPENAI_APIKEY_HERE",
        model="gpt-4"
    )
    
    # Process all documents
    output_data = preprocessor.process_all_documents(
        folder_path="/mnt/d/__projects_main/dspproject/legalnorm", ######## user-changable ########
        output_json="preprocessed_legal_norms.json"
    )
    
    # Optional: Inspect results
    print("\n" + "="*80)
    print("SAMPLE PROCESSED CHUNKS")
    print("="*80)
    for i, chunk_data in enumerate(output_data['chunks'][:3], 1):
        print(f"\n--- Chunk {i} ---")
        print(f"Hierarchy: Chapter {chunk_data['metadata']['chapter']} > "
              f"Section {chunk_data['metadata']['section']} > "
              f"Article {chunk_data['metadata']['article']}")
        print(f"Tokens: {chunk_data['tokens']}")
        print(f"Content preview: {chunk_data['content'][:200]}...")