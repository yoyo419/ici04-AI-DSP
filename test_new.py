#!/usr/bin/env python3
"""
ğŸ§ª åŸºç¤æ¨¡å‹æ¸¬è©¦è…³æœ¬ï¼ˆåƒ…æ¸¬è©¦ Qwen2.5-3B-Instructï¼‰
ç”¨æ–¼æ¸¬è©¦åŸºç¤æ¨¡å‹ï¼ˆä¸ä½¿ç”¨ä»»ä½• LoRA adapterï¼‰åœ¨æ¸¬è©¦é›†ä¸Šçš„è¡¨ç¾
"""

import os
import sys
import argparse
from datetime import datetime
import re
import json

import torch
from datasets import load_dataset, Dataset
from unsloth import FastLanguageModel
from tqdm import tqdm

# æŠ‘åˆ¶è­¦å‘Š
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import warnings
warnings.filterwarnings('ignore')

# ============================================================
# é…ç½®
# ============================================================
class TestConfig:
    # æ¨¡å‹è·¯å¾‘
    base_model_path = "/content/Colab_Download/Qwen2.5-3B-Instruct"
    
    # æ¸¬è©¦æ•¸æ“šè·¯å¾‘
    test_data = "/content/Colab_Upload/data/test_data.json"
    
    # è¼¸å‡ºè·¯å¾‘
    output_dir = "/content/Colab/test_results_base"
    
    # ç”Ÿæˆåƒæ•¸
    max_seq_length = 2048
    max_new_tokens = 512
    temperature = 0.7
    top_p = 0.9
    do_sample = True
    
    # æ¸¬è©¦æ¨£æœ¬æ•¸ï¼ˆè¨­ç‚º None æ¸¬è©¦å…¨éƒ¨ï¼‰
    num_test_samples = None

# ============================================================
# System Promptsï¼ˆèˆ‡è¨“ç·´ä¸€è‡´ï¼‰
# ============================================================
SYSTEM_PROMPT = """ç”¨æˆ·ä¸åŠ©æ‰‹ä¹‹é—´çš„å¯¹è¯ã€‚ç”¨æˆ·æå‡ºé—®é¢˜ï¼ŒåŠ©æ‰‹è§£å†³å®ƒã€‚åŠ©æ‰‹é¦–å…ˆæ€è€ƒæ¨ç†è¿‡ç¨‹ï¼Œç„¶åæä¾›ç”¨æˆ·ç­”æ¡ˆã€‚æ¨ç†è¿‡ç¨‹å’Œç­”æ¡ˆåˆ†åˆ«åŒ…å«åœ¨ <reasoning> </reasoning> å’Œ <answer> </answer> æ ‡ç­¾ä¸­ã€‚

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”é—®é¢˜ï¼š
<reasoning>
åœ¨æ­¤è¯¦ç»†åˆ†æé—®é¢˜å¹¶å±•ç¤ºå®Œæ•´çš„æ¨ç†è¿‡ç¨‹ï¼ŒåŒ…æ‹¬æ€è€ƒæ­¥éª¤ã€ç›¸å…³çŸ¥è¯†å’Œé€»è¾‘åˆ†æã€‚
</reasoning>
<answer>
åœ¨æ­¤æä¾›ç®€æ´æ˜ç¡®çš„æœ€ç»ˆç­”æ¡ˆ(å›ç­”è‡³å°‘ä¸€å€‹é¸é …ï¼Œæ³¨æ„ï¼Œå¦‚æœç­”æ¡ˆä¸åªä¸€å€‹é¸é …ï¼Œå¦‚12345ï¼Œä¸€å®šè¦è¼¸å‡º(12345)ï¼Œä¸è¦è¼¸å‡º(1)(2)(3)(4)(5))ã€‚
</answer>"""

SYSTEM_PROMPT_BASE = """ç”¨æˆ·ä¸åŠ©æ‰‹ä¹‹é—´çš„å¯¹è¯ã€‚ç”¨æˆ·æå‡ºé—®é¢˜ï¼ŒåŠ©æ‰‹è§£å†³å®ƒã€‚ç­”æ¡ˆåŒ…å«åœ¨ <answer> </answer> æ ‡ç­¾ä¸­ã€‚

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”é—®é¢˜ï¼š
<answer>
åœ¨æ­¤æä¾›ç®€æ´æ˜ç¡®çš„æœ€ç»ˆç­”æ¡ˆ(å›ç­”è‡³å°‘ä¸€å€‹é¸é …ï¼Œæ³¨æ„ï¼Œå¦‚æœç­”æ¡ˆä¸åªä¸€å€‹é¸é …ï¼Œå¦‚12345ï¼Œä¸€å®šè¦è¼¸å‡º(12345)ï¼Œä¸è¦è¼¸å‡º(1)(2)(3)(4)(5))ã€‚
</answer>"""

EXAMPLE_TEXT = """
ä½ æ˜¯ä¸€ä¸ªè·æ¥­å®‰å…¨è¡›ç”Ÿå°ˆå®¶ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹è¦æ±‚å›ç­”ï¼š
 """

# ============================================================
# å·¥å…·å‡½æ•¸ï¼šå¾æ–‡å­—ä¸­æŠ½å–é¸é …æ•¸å­—
# ============================================================
def extract_choices_from_text(text: str):
    """
    å¾æ–‡å­—ä¸­æŠ“å‡ºæ‰€æœ‰æ‹¬è™Ÿå…§çš„æ•¸å­—é¸é …ï¼Œä¾‹å¦‚ï¼š
    (1)(2)(3)        -> "123"
    <eoa> -> "3"
    è‹¥æ‰¾ä¸åˆ°å‰‡å›å‚³ None
    """
    # æ”¯æ´å…¨å½¢/åŠå½¢æ‹¬è™Ÿ
    nums = re.findall(r"[ï¼ˆ(]([0-9]+)[ï¼‰)]", text)
    if not nums:
        return None
    return "".join(nums)

# ============================================================
# æ•¸æ“šè¼‰å…¥å‡½æ•¸
# ============================================================
def get_questions(file_path: str) -> Dataset:
    """
    è¼‰å…¥ä¸¦è™•ç†æ¸¬è©¦æ•¸æ“šé›†
    """
    # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå˜—è©¦æœ¬åœ°è·¯å¾‘
    if not os.path.exists(file_path):
        local_paths = [
            file_path.replace("/content/Colab_Upload/", "./"),
            file_path.replace("/content/Colab_Upload/", "../"),
            "./data/test_data.json",
            "../data/test_data.json"
        ]
        
        for local_path in local_paths:
            if os.path.exists(local_path):
                file_path = local_path
                print(f"âœ“ ä½¿ç”¨æœ¬åœ°è·¯å¾‘: {file_path}")
                break
        else:
            raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°è³‡æ–™æª”æ¡ˆ: {file_path}")
    
    print(f"ğŸ“‚ è¼‰å…¥è³‡æ–™é›†: {file_path}")
    
    # è¼‰å…¥æ•¸æ“š
    data = load_dataset("json", data_files=file_path)
    data = data['train']  # HuggingFace datasets çµ±ä¸€ä½¿ç”¨ 'train' key
    
    print(f"âœ“ åŸå§‹è³‡æ–™é›†å¤§å°: {len(data)}")
    
    # è™•ç†æ¯å€‹æ¨£æœ¬
    def process_sample(x: dict) -> dict:
        """è™•ç†å–®å€‹æ¨£æœ¬ï¼Œæº–å‚™ prompt æ ¼å¼"""
        return { 
            'prompt': [
                {'role': 'system', 'content': SYSTEM_PROMPT},
                {'role': 'user', 'content': f"[QUERY_ID:{x['id']}]\n[QUERY_SOURCE:{x.get('source', 'unknown')}]\n" + EXAMPLE_TEXT + x['instruction'] + x['question']}
            ],
            'answer': x['answer'],
            'id': x['id'],
            'question': x['question'],
            'instruction': x['instruction'],
            'source': x.get('source', 'unknown')
        }
    
    data = data.map(process_sample)
    print(f"âœ“ è™•ç†å¾Œè³‡æ–™é›†å¤§å°: {len(data)}")
    
    return data

# ============================================================
# æ—¥èªŒå‡½æ•¸
# ============================================================
def log_print(message: str):
    """å°å‡ºè¨Šæ¯åˆ°æ§åˆ¶å°"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print(f"[{timestamp}] {message}")

# ============================================================
# ä¸»æ¸¬è©¦å‡½æ•¸
# ============================================================
def test_model(config: TestConfig):
    """
    æ¸¬è©¦æ¨¡å‹
    """
    print()
    print("=" * 70)
    print("ğŸ§ª åŸºç¤æ¨¡å‹æ¸¬è©¦ï¼ˆQwen2.5-3B-Instructï¼‰")
    print("=" * 70)
    print()
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    os.makedirs(config.output_dir, exist_ok=True)
    
    # ============================================================
    # 1. è¼‰å…¥æ¨¡å‹
    # ============================================================
    print("=" * 70)
    print("æ­¥é©Ÿ 1: è¼‰å…¥åŸºç¤æ¨¡å‹")
    print("=" * 70)
    
    try:
        log_print(f"ğŸ“¦ è¼‰å…¥ Qwen2.5-3B-Instruct: {config.base_model_path}")
        
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name=config.base_model_path,
            max_seq_length=config.max_seq_length,
            dtype=None,  # è‡ªå‹•é¸æ“‡ dtype
            load_in_4bit=False,
        )
        log_print("âœ“ åŸºç¤æ¨¡å‹è¼‰å…¥å®Œæˆ")
        
        # è¨­ç½®ç‚ºæ¨ç†æ¨¡å¼
        FastLanguageModel.for_inference(model)
        model.eval()
        log_print("âœ“ æ¨¡å‹å·²è¨­ç½®ç‚ºæ¨ç†æ¨¡å¼")
        
    except Exception as e:
        log_print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    print()
    
    # ============================================================
    # 2. è¼‰å…¥æ¸¬è©¦æ•¸æ“š
    # ============================================================
    print("=" * 70)
    print("æ­¥é©Ÿ 2: è¼‰å…¥æ¸¬è©¦æ•¸æ“š")
    print("=" * 70)
    
    try:
        test_dataset = get_questions(file_path=config.test_data)
        
        # é™åˆ¶æ¸¬è©¦æ¨£æœ¬æ•¸
        if config.num_test_samples is not None:
            original_size = len(test_dataset)
            test_dataset = test_dataset.select(range(min(config.num_test_samples, len(test_dataset))))
            log_print(f"âœ“ æ¸¬è©¦æ¨£æœ¬æ•¸é™åˆ¶: {len(test_dataset)}/{original_size}")
        else:
            log_print(f"âœ“ æ¸¬è©¦å…¨éƒ¨æ¨£æœ¬: {len(test_dataset)}")
            
    except Exception as e:
        log_print(f"âŒ æ¸¬è©¦æ•¸æ“šè¼‰å…¥å¤±æ•—: {e}")
        sys.exit(1)
    
    print()
    
    # ============================================================
    # 3. é€²è¡Œæ¸¬è©¦
    # ============================================================
    print("=" * 70)
    print("æ­¥é©Ÿ 3: ç”Ÿæˆç­”æ¡ˆ")
    print("=" * 70)
    print()
    
    results = []
    correct_count = 0
    
    for idx in tqdm(range(len(test_dataset)), desc="ç”Ÿæˆç­”æ¡ˆ"):
        sample = test_dataset[idx]
        
        try:
            # æº–å‚™è¼¸å…¥
            messages = sample['prompt']
            input_text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
            
            # ç”Ÿæˆç­”æ¡ˆ
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=config.max_new_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    do_sample=config.do_sample,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                )
            
            # è§£ç¢¼ç”Ÿæˆçš„ç­”æ¡ˆ
            generated_ids = outputs[0][inputs['input_ids'].shape[1]:]
            predicted_answer = tokenizer.decode(generated_ids, skip_special_tokens=True)
            
            # -------- æ”¹è‰¯ç‰ˆï¼šåªæ¯”å°é¸é …æ•¸å­— --------
            ground_truth_raw = sample['answer'].strip()
            gt_choices = extract_choices_from_text(ground_truth_raw)
            pred_choices = extract_choices_from_text(predicted_answer)

            if gt_choices is not None and pred_choices is not None:
                gt_norm = "".join(sorted(gt_choices))
                pred_norm = "".join(sorted(pred_choices))
                is_correct = (gt_norm == pred_norm)
            else:
                # fallbackï¼šæ‰¾ä¸åˆ°é¸é …æ™‚ï¼Œé€€å›åŸæœ¬çš„åŒ…å«åˆ¤æ–·
                is_correct = ground_truth_raw in predicted_answer

            if is_correct:
                correct_count += 1
            
            # å„²å­˜çµæœ
            result = {
                'id': sample['id'],
                'source': sample['source'],
                'question': sample['question'],
                'instruction': sample['instruction'],
                'ground_truth': ground_truth_raw,
                'predicted_answer': predicted_answer,
                'is_correct': is_correct,
            }
            results.append(result)
            
            # å®šæœŸé¡¯ç¤ºé€²åº¦å’Œæ¨£æœ¬
            if (idx + 1) % 10 == 0 or idx < 3:
                print(f"\n{'='*70}")
                print(f"æ¨£æœ¬ {idx+1}/{len(test_dataset)}")
                print(f"ç•¶å‰æº–ç¢ºç‡: {correct_count}/{idx+1} ({100*correct_count/(idx+1):.2f}%)")
                print(f"{'='*70}")
                print(f"ğŸ“‹ ID: {sample['id']}")
                print(f"ğŸ“‚ Source: {sample['source']}")
                print(f"\nâ“ å•é¡Œ:")
                print(f"  {sample['question'][:200]}...")
                print(f"\nğŸ¤– æ¨¡å‹é æ¸¬ç­”æ¡ˆ:")
                print(f"  {predicted_answer[:500]}...")
                print(f"\nâœ… çœŸå¯¦ç­”æ¡ˆ:")
                print(f"  {ground_truth_raw[:500]}...")
                print(f"\n{'âœ“' if is_correct else 'âœ—'} {'æ­£ç¢º' if is_correct else 'éŒ¯èª¤'}")
                print()
                
        except Exception as e:
            log_print(f"âš ï¸  æ¨£æœ¬ {idx} ç”Ÿæˆå¤±æ•—: {e}")
            results.append({
                'id': sample['id'],
                'source': sample.get('source', 'unknown'),
                'question': sample['question'],
                'ground_truth': sample['answer'],
                'predicted_answer': f"[ERROR] {str(e)}",
                'is_correct': False
            })
    
    print()
    print("=" * 70)
    print("âœ“ ç­”æ¡ˆç”Ÿæˆå®Œæˆ")
    print("=" * 70)
    print()

    # ============================================================
    # 4. è¨ˆç®—ä¸¦é¡¯ç¤ºçµ±è¨ˆçµæœ
    # ============================================================
    print("=" * 70)
    print("ğŸ“Š æ¸¬è©¦çµæœçµ±è¨ˆ")
    print("=" * 70)
    
    total_samples = len(results)
    accuracy = (correct_count / total_samples * 100) if total_samples > 0 else 0
    
    log_print(f"ç¸½æ¨£æœ¬æ•¸: {total_samples}")
    log_print(f"æ­£ç¢ºæ•¸é‡: {correct_count}")
    log_print(f"éŒ¯èª¤æ•¸é‡: {total_samples - correct_count}")
    log_print(f"æº–ç¢ºç‡: {accuracy:.2f}%")
    
    print()
    
    # ============================================================
    # 5. ä¿å­˜çµæœ
    # ============================================================
    print("=" * 70)
    print("æ­¥é©Ÿ 5: ä¿å­˜çµæœ")
    print("=" * 70)
    
    # ä¿å­˜è©³ç´°çµæœï¼ˆJSONï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = os.path.join(config.output_dir, f"test_results_base_{timestamp}.json")
    
    try:
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump({
                'metadata': {
                    'model': config.base_model_path,
                    'test_data': config.test_data,
                    'timestamp': timestamp,
                    'total_samples': total_samples,
                    'correct_count': correct_count,
                    'accuracy': accuracy,
                    'config': {
                        'max_new_tokens': config.max_new_tokens,
                        'temperature': config.temperature,
                        'top_p': config.top_p
                    }
                },
                'results': results
            }, f, ensure_ascii=False, indent=2)
        
        log_print(f"âœ“ è©³ç´°çµæœå·²ä¿å­˜: {results_file}")
    except Exception as e:
        log_print(f"âŒ ä¿å­˜çµæœå¤±æ•—: {e}")
    
    # ä¿å­˜æ‘˜è¦ï¼ˆæ–‡æœ¬æ–‡ä»¶ï¼‰
    summary_file = os.path.join(config.output_dir, f"summary_base_{timestamp}.txt")
    
    try:
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("=" * 70 + "\n")
            f.write("åŸºç¤æ¨¡å‹æ¸¬è©¦æ‘˜è¦\n")
            f.write("=" * 70 + "\n\n")
            f.write(f"æ¨¡å‹: {config.base_model_path}\n")
            f.write(f"æ¸¬è©¦æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ¸¬è©¦æ•¸æ“š: {config.test_data}\n\n")
            f.write(f"ç¸½æ¨£æœ¬æ•¸: {total_samples}\n")
            f.write(f"æ­£ç¢ºæ•¸é‡: {correct_count}\n")
            f.write(f"éŒ¯èª¤æ•¸é‡: {total_samples - correct_count}\n")
            f.write(f"æº–ç¢ºç‡: {accuracy:.2f}%\n\n")
            f.write("=" * 70 + "\n")
        
        log_print(f"âœ“ æ‘˜è¦å·²ä¿å­˜: {summary_file}")
    except Exception as e:
        log_print(f"âŒ ä¿å­˜æ‘˜è¦å¤±æ•—: {e}")
    
    print()
    print("=" * 70)
    print("ğŸ‰ æ¸¬è©¦å®Œæˆï¼")
    print("=" * 70)
    print(f"âœ“ æº–ç¢ºç‡: {accuracy:.2f}% ({correct_count}/{total_samples})")
    print(f"âœ“ çµæœä¿å­˜åœ¨: {config.output_dir}")
    print("=" * 70)

# ============================================================
# ä¸»ç¨‹åº
# ============================================================
def main():
    parser = argparse.ArgumentParser(description="åŸºç¤æ¨¡å‹æ¸¬è©¦å·¥å…·ï¼ˆQwen2.5-3Bï¼‰")
    parser.add_argument(
        "--base_model", 
        type=str, 
        default="/content/Colab_Download/Qwen2.5-3B-Instruct",
        help="åŸºç¤æ¨¡å‹è·¯å¾‘"
    )
    parser.add_argument(
        "--test_data", 
        type=str, 
        default="/content/Colab_Upload/data/test_data.json",
        help="æ¸¬è©¦æ•¸æ“šè·¯å¾‘"
    )
    parser.add_argument(
        "--output_dir", 
        type=str, 
        default="/content/Colab/test_results_base",
        help="è¼¸å‡ºç›®éŒ„"
    )
    parser.add_argument(
        "--num_samples", 
        type=int, 
        default=None,
        help="æ¸¬è©¦æ¨£æœ¬æ•¸ï¼ˆNone è¡¨ç¤ºå…¨éƒ¨ï¼‰"
    )
    parser.add_argument(
        "--temperature", 
        type=float, 
        default=0.7,
        help="ç”Ÿæˆæº«åº¦"
    )
    
    args = parser.parse_args()
    
    # æ›´æ–°é…ç½®
    config = TestConfig()
    config.base_model_path = args.base_model
    config.test_data = args.test_data
    config.output_dir = args.output_dir
    config.num_test_samples = args.num_samples
    config.temperature = args.temperature
    
    # åŸ·è¡Œæ¸¬è©¦
    test_model(config)

if __name__ == "__main__":
    main()
