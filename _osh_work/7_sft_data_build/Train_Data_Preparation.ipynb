{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e06e942c2804ead8bf69e2bcbb7f250": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c6a4ac05853c4022b2001fd970d0c499",
              "IPY_MODEL_14c4dc2cbae54e358b47108d00f73246",
              "IPY_MODEL_fce2a4e3199543f1bf18801b07e00a94"
            ],
            "layout": "IPY_MODEL_dec7b9b43d844a66b73be54cbc312f6e"
          }
        },
        "c6a4ac05853c4022b2001fd970d0c499": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c5ab94a60664cac9e63e57a4c65eac6",
            "placeholder": "​",
            "style": "IPY_MODEL_057b1df14e5b48f5a3ca72ed22b9856f",
            "value": "Batches: 100%"
          }
        },
        "14c4dc2cbae54e358b47108d00f73246": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a78e649e5c34403ebb8f9d3e0f2c748e",
            "max": 65,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c9c75c04710147978515d64b6ac5b135",
            "value": 65
          }
        },
        "fce2a4e3199543f1bf18801b07e00a94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_122fd69a02df46d0b60b4e4e6cc2b3e2",
            "placeholder": "​",
            "style": "IPY_MODEL_7c50cb92eefb442d9043a53753fd25e9",
            "value": " 65/65 [01:46&lt;00:00,  2.87it/s]"
          }
        },
        "dec7b9b43d844a66b73be54cbc312f6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c5ab94a60664cac9e63e57a4c65eac6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "057b1df14e5b48f5a3ca72ed22b9856f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a78e649e5c34403ebb8f9d3e0f2c748e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9c75c04710147978515d64b6ac5b135": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "122fd69a02df46d0b60b4e4e6cc2b3e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c50cb92eefb442d9043a53753fd25e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation 1: Dataset with \"OSH situation\" inputs and \"reasoning\" output by LLM"
      ],
      "metadata": {
        "id": "hSIgc4OO0eGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**說明**：團隊已生成初版的題目與推理答案成組，接下來這個區段是說明 \"[A]生成這份題目與答案的輸入檔案是怎麼來的\" 跟 \"[B]正確答案推論如何製作，[C]要怎麼從初版做 (1)去id化 (2)法規推論優化(事故原因到法規適用到責任結論) (3)圖譜的隱性增強\""
      ],
      "metadata": {
        "id": "UnyuLfyy7lgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [A]\n",
        "# manually upload knowledge_graph_final.json"
      ],
      "metadata": {
        "id": "d12_TNSc2qPi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "rf8TI1im0BJn"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from collections import Counter, defaultdict\n",
        "from typing import Dict, List, Tuple\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import torch\n",
        "import random\n",
        "import sys\n",
        "import re\n",
        "import os\n",
        "import argparse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from sentence_transformers import SentenceTransformer, util\n",
        "    HAS_BERT = True\n",
        "except ImportError:\n",
        "    HAS_BERT = False\n",
        "    print(\"CRITICAL WARNING: sentence-transformers not installed. Semantic Pruning will fail.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sS7Q30k2TzT",
        "outputId": "3c2e23d1-9d64-4c06-de80-e77f3b325bb6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 設定 Logging 格式，看起來更像專業的實驗室工具\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - [OSKG-Lab] - %(levelname)s - %(message)s',\n",
        "    datefmt='%H:%M:%S'\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "iMk-T7cx4aj4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OSKGPreprocessor:\n",
        "    def __init__(self, json_path: str, model_name: str = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'):\n",
        "        self.json_path = json_path\n",
        "        self.model_name = model_name\n",
        "        self.G = nx.MultiDiGraph()\n",
        "        self.edge_weights = {}\n",
        "        self.node_embeddings = {} # 儲存 Tensor 格式的 Embedding\n",
        "        self.hard_negative_candidates = {} # PDF 3  困難負樣本字典\n",
        "\n",
        "        # 初始化模型 [cite: 27]\n",
        "        if HAS_BERT:\n",
        "            print(f\"Loading BERT model: {model_name}...\")\n",
        "            self.model = SentenceTransformer(model_name)\n",
        "\n",
        "        # 載入資料\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            self.raw_data = json.load(f)\n",
        "        print(f\"Loaded KG with {len(self.raw_data.get('nodes', []))} nodes.\")\n",
        "\n",
        "    def build_graph(self):\n",
        "        \"\"\"基礎圖譜建構\"\"\"\n",
        "        print(\"Building initial graph...\")\n",
        "        for node in self.raw_data['nodes']:\n",
        "            self.G.add_node(node['id'], **node)\n",
        "        for link in self.raw_data['links']:\n",
        "            self.G.add_edge(link['source'], link['target'], relation=link['relation'])\n",
        "\n",
        "    def action_b_generate_embeddings(self):\n",
        "        \"\"\"\n",
        "        動作 (B): 節點特徵初始化\n",
        "        注意：為了支援後續的語意剪枝 [cite: 12, 13]，我們必須先生成 Embedding。\n",
        "        \"\"\"\n",
        "        print(\"Executing Action B: Generating Node Embeddings...\")\n",
        "        if not HAS_BERT:\n",
        "            return\n",
        "\n",
        "        node_ids = list(self.G.nodes())\n",
        "        texts = []\n",
        "\n",
        "        # 準備文本：優先使用 full_text (法規) 或 label (違規描述)\n",
        "        for nid in node_ids:\n",
        "            node = self.G.nodes[nid]\n",
        "            # [cite: 44, 47] 提取文本邏輯\n",
        "            text = node.get('full_text', node.get('original_full_text', node.get('label', str(nid))))\n",
        "            texts.append(str(text))\n",
        "\n",
        "        # 批次編碼 [cite: 50, 51]\n",
        "        embeddings_tensor = self.model.encode(texts, convert_to_tensor=True, show_progress_bar=True, batch_size=32)\n",
        "\n",
        "        # 建立映射供快速查詢\n",
        "        self.node_embeddings = {nid: emb for nid, emb in zip(node_ids, embeddings_tensor)}\n",
        "\n",
        "        # 將 Embedding 存回圖節點屬性\n",
        "        feature_dict = {nid: emb.cpu() for nid, emb in self.node_embeddings.items()}\n",
        "        nx.set_node_attributes(self.G, feature_dict, 'x')\n",
        "\n",
        "    def action_c_semantic_pruning(self, threshold: float = 0.6, top_k: int = 3):\n",
        "        \"\"\"\n",
        "        動作 (C): 語意剪枝 (Semantic Pruning) - 依據 PDF 2 [cite: 8, 30]\n",
        "        針對 VIOLATES_LAW 進行基於 Embedding 的剪枝，解決「噪聲爆炸」問題 [cite: 2]。\n",
        "        \"\"\"\n",
        "        print(f\"Executing Action C: Semantic Pruning (Threshold={threshold}, Top-K={top_k})...\")\n",
        "        if not HAS_BERT or not self.node_embeddings:\n",
        "            print(\"Error: No embeddings found. Run action_b first.\")\n",
        "            return\n",
        "\n",
        "        # 1. 找出所有 VIOLATES_LAW 邊並分組 [cite: 34-40]\n",
        "        vio_edges = [(u, v, k) for u, v, k, attr in self.G.edges(keys=True, data=True) if attr['relation'] == 'VIOLATES_LAW']\n",
        "        vio_to_reg_map = defaultdict(list)\n",
        "        for u, v, k in vio_edges:\n",
        "            vio_to_reg_map[u].append(v)\n",
        "\n",
        "        print(f\" -> Found {len(vio_edges)} VIOLATES_LAW edges to analyze.\")\n",
        "        edges_to_remove = []\n",
        "        edges_to_add = []\n",
        "\n",
        "        # 2. 逐一處理每個 Violation 節點 [cite: 43]\n",
        "        for vio_id, reg_ids in tqdm(vio_to_reg_map.items(), desc=\"Pruning Edges\"):\n",
        "            if vio_id not in self.node_embeddings: continue\n",
        "\n",
        "            vio_emb = self.node_embeddings[vio_id] # [cite: 12]\n",
        "\n",
        "            # 收集目標 Regulations 的 Embeddings\n",
        "            valid_regs = [rid for rid in reg_ids if rid in self.node_embeddings]\n",
        "            if not valid_regs: continue\n",
        "\n",
        "            reg_embs = torch.stack([self.node_embeddings[rid] for rid in valid_regs]) # [cite: 13]\n",
        "\n",
        "            # 3. 計算 Cosine Similarity [cite: 14, 53]\n",
        "            # util.cos_sim returns (1, n_regs)\n",
        "            scores = util.cos_sim(vio_emb, reg_embs)[0]\n",
        "\n",
        "            # 4. 篩選 Top-K [cite: 54-60]\n",
        "            reg_scores = []\n",
        "            for i, rid in enumerate(valid_regs):\n",
        "                reg_scores.append((rid, scores[i].item()))\n",
        "\n",
        "            # 排序：分數高到低\n",
        "            reg_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "            top_results = reg_scores[:top_k] #\n",
        "\n",
        "            # 標記要移除的舊邊 (全部移除，稍後加回篩選過的)\n",
        "            for rid in reg_ids:\n",
        "                # 這裡需要小心，因為 NetworkX 是 MultiDiGraph，我們需要移除特定的那條邊\n",
        "                # 簡單起見，我們記錄 (u, v) 組合，之後統一移除該類型的邊\n",
        "                pass\n",
        "\n",
        "            # 5. 重建邊與賦予權重 [cite: 61]\n",
        "            for reg_id, score in top_results:\n",
        "                new_relation = 'VIOLATES_SPECIFICALLY'\n",
        "\n",
        "                # Logic from PDF 2 Page 3 [cite: 63-79]:\n",
        "                # 如果分數高於閾值 -> VIOLATES_SPECIFICALLY\n",
        "                # 如果是 Top-K 但分數略低 (這裡設個緩衝) -> IS_RELEVANT_TO\n",
        "\n",
        "                if score >= threshold: # [cite: 63]\n",
        "                    new_relation = 'VIOLATES_SPECIFICALLY'\n",
        "                    weight = score # [cite: 70]\n",
        "                else:\n",
        "                    new_relation = 'IS_RELEVANT_TO' # [cite: 77]\n",
        "                    weight = score * 0.5 # [cite: 78] 降低權重\n",
        "\n",
        "                edges_to_add.append({\n",
        "                    'source': vio_id,\n",
        "                    'target': reg_id,\n",
        "                    'relation': new_relation,\n",
        "                    'weight': weight\n",
        "                })\n",
        "\n",
        "        # 執行圖更新\n",
        "        print(\" -> Applying pruning updates to graph...\")\n",
        "        # 移除舊的 VIOLATES_LAW 邊\n",
        "        edges_to_remove = [(u, v, k) for u, v, k, attr in self.G.edges(keys=True, data=True) if attr['relation'] == 'VIOLATES_LAW']\n",
        "        self.G.remove_edges_from(edges_to_remove)\n",
        "        print(f\" -> Removed {len(edges_to_remove)} noisy edges.\")\n",
        "\n",
        "        # 加入新的精煉邊\n",
        "        for edge in edges_to_add:\n",
        "            self.G.add_edge(edge['source'], edge['target'], relation=edge['relation'], weight=edge['weight'])\n",
        "        print(f\" -> Added {len(edges_to_add)} semantic edges (VIOLATES_SPECIFICALLY / IS_RELEVANT_TO).\")\n",
        "\n",
        "    def action_d_inject_hierarchy(self):\n",
        "        \"\"\"動作 (D): 補強層級結構 (同前版)\"\"\"\n",
        "        print(\"Executing Action D: Injecting Hierarchy...\")\n",
        "        new_edges = []\n",
        "        reg_nodes = [n for n, attr in self.G.nodes(data=True) if attr.get('node_type') in ['Regulation', 'Reg']]\n",
        "        for reg_id in reg_nodes:\n",
        "            law_name = self.G.nodes[reg_id].get('law_name')\n",
        "            if law_name:\n",
        "                law_node_id = f\"LAW_{abs(hash(law_name))}\"\n",
        "                if law_node_id not in self.G:\n",
        "                    self.G.add_node(law_node_id, label=law_name, node_type='Law', law_name=law_name)\n",
        "                    # Law 節點也需要 Embedding (取平均或重新 encode)\n",
        "                    if HAS_BERT and reg_id in self.node_embeddings:\n",
        "                        self.node_embeddings[law_node_id] = self.node_embeddings[reg_id] # 暫時借用子節點特徵\n",
        "                        self.G.nodes[law_node_id]['x'] = self.node_embeddings[reg_id].cpu()\n",
        "\n",
        "                new_edges.append((reg_id, law_node_id, 'PART_OF'))\n",
        "\n",
        "        for u, v, rel in new_edges:\n",
        "            self.G.add_edge(u, v, relation=rel, weight=1.0) # PART_OF 權重設為 1\n",
        "\n",
        "    def action_prep_hard_negatives(self, top_k=5):\n",
        "        \"\"\"\n",
        "        支援 PDF 3 ：準備「困難負樣本 (Hard Negatives)」\n",
        "        找出「字面上很像但邏輯錯誤」的法規，供 KGE 訓練時的 Negative Sampling 使用。\n",
        "        \"\"\"\n",
        "        print(\"Executing Action: Preparing Hard Negative Candidates...\")\n",
        "        if not HAS_BERT: return\n",
        "\n",
        "        # 找出所有法規\n",
        "        reg_ids = [n for n, attr in self.G.nodes(data=True) if attr.get('node_type') in ['Regulation', 'Reg']]\n",
        "        if not reg_ids: return\n",
        "\n",
        "        reg_embs = torch.stack([self.node_embeddings[rid] for rid in reg_ids])\n",
        "\n",
        "        # 計算法規之間的相似度矩陣\n",
        "        sim_matrix = util.cos_sim(reg_embs, reg_embs)\n",
        "\n",
        "        candidates = {}\n",
        "        for i, rid in enumerate(reg_ids):\n",
        "            # 找出最相似的 Top-K，但排除自己\n",
        "            scores = sim_matrix[i]\n",
        "            # argsort 是由小到大，所以取最後幾個\n",
        "            top_indices = torch.argsort(scores, descending=True)[1:top_k+1]\n",
        "\n",
        "            hard_negatives = [reg_ids[idx] for idx in top_indices]\n",
        "            candidates[rid] = hard_negatives\n",
        "\n",
        "        self.hard_negative_candidates = candidates\n",
        "        print(f\" -> Generated hard negative candidates for {len(candidates)} regulations.\")\n",
        "\n",
        "    def action_a_compute_edge_weights(self):\n",
        "        \"\"\"重新計算權重 (在剪枝之後執行)\"\"\"\n",
        "        print(\"Executing Action A: Re-calculating Global Edge Weights...\")\n",
        "        relations = [attr['relation'] for u, v, k, attr in self.G.edges(keys=True, data=True)]\n",
        "        count = Counter(relations)\n",
        "        total = len(relations)\n",
        "\n",
        "        weights = {}\n",
        "        for rel, c in count.items():\n",
        "            weights[rel] = total / c\n",
        "\n",
        "        # [cite: 96] 剪枝後 VIOLATES_LAW 應該大幅減少，權重會自然上升\n",
        "        # 這裡我們將個別邊的 semantic weight 與全局 class weight 結合\n",
        "        self.class_weights = weights\n",
        "        print(\" -> Updated Class Weights:\", weights)\n",
        "\n",
        "    def export_data(self):\n",
        "        \"\"\"匯出處理後的資料物件\"\"\"\n",
        "        return {\n",
        "            'graph': self.G,\n",
        "            'node_features': torch.stack([self.G.nodes[n]['x'] for n in self.G.nodes() if 'x' in self.G.nodes[n]]),\n",
        "            'hard_negatives': self.hard_negative_candidates\n",
        "        }\n",
        "\n",
        "def inspect_pruning_results(graph, num_samples=5):\n",
        "    \"\"\"\n",
        "    [Validation] 執行人工驗證步驟\n",
        "    依據評鑑建議：隨機抽樣檢查剪枝後的邊，確認法規是否為核心法條 。\n",
        "    \"\"\"\n",
        "    logger.info(\"--- 正在執行人工抽樣驗證 (QA Check) ---\")\n",
        "\n",
        "    # 找出所有經過語意篩選的邊\n",
        "    semantic_edges = [\n",
        "        (u, v, attr) for u, v, k, attr in graph.edges(keys=True, data=True)\n",
        "        if attr.get('relation') in ['VIOLATES_SPECIFICALLY', 'IS_RELEVANT_TO']\n",
        "    ]\n",
        "\n",
        "    if not semantic_edges:\n",
        "        logger.warning(\"警告：未發現任何語意剪枝後的邊！請檢查 Threshold 設定。\")\n",
        "        return\n",
        "\n",
        "    # 隨機抽樣\n",
        "    samples = random.sample(semantic_edges, min(num_samples, len(semantic_edges)))\n",
        "\n",
        "    for i, (u, v, attr) in enumerate(samples):\n",
        "        # 取得節點文字 (優先取 label 或 full_text)\n",
        "        u_text = graph.nodes[u].get('label', str(u))[:30] + \"...\"\n",
        "        v_text = graph.nodes[v].get('label', str(v))[:30] + \"...\"\n",
        "\n",
        "        rel_type = attr['relation']\n",
        "        score = attr.get('weight', 0.0)\n",
        "\n",
        "        print(f\"Sample {i+1}:\")\n",
        "        print(f\"  [事故/違規]: {u_text}\")\n",
        "        print(f\"  --[{rel_type} (score: {score:.4f})]-->\")\n",
        "        print(f\"  [對應法規]: {v_text}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "def main():\n",
        "    # 1. 參數設定\n",
        "    parser = argparse.ArgumentParser(description=\"OSKG Preprocessor Pipeline\")\n",
        "    parser.add_argument('--input', type=str, default='knowledge_graph_final.json', help='原始知識圖譜 JSON 路徑')\n",
        "    parser.add_argument('--output', type=str, default='processed_oskg_data.pt', help='處理後資料的輸出路徑 (.pt)')\n",
        "    parser.add_argument('--threshold', type=float, default=0.7, help='語意剪枝 Cosine Similarity 閾值 [建議值: 0.7]')\n",
        "    parser.add_argument('--top_k', type=int, default=3, help='保留最相關的 K 條法規 [建議值: 3]')\n",
        "    parser.add_argument('--bert_model', type=str, default='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', help='使用的 BERT 模型')\n",
        "\n",
        "    # Colab 中避免 argparse 讀取錯誤\n",
        "    args = parser.parse_args(args=[])\n",
        "\n",
        "    # 檢查輸入檔案\n",
        "    if not os.path.exists(args.input):\n",
        "        logger.error(f\"找不到輸入檔案: {args.input}\")\n",
        "        # sys.exit(1) # In Notebook, return instead\n",
        "        return\n",
        "\n",
        "    logger.info(\"啟動職業安全知識圖譜前處理流程 (OSKG Pipeline)...\")\n",
        "    logger.info(f\"設定參數: Threshold={args.threshold} , Top-K={args.top_k} \")\n",
        "\n",
        "    # 2. 初始化處理器\n",
        "    processor = OSKGPreprocessor(json_path=args.input, model_name=args.bert_model)\n",
        "\n",
        "    # 3. 建構基礎圖譜\n",
        "    logger.info(\"Step 1: 建構基礎圖譜拓樸...\")\n",
        "    processor.build_graph()\n",
        "\n",
        "    # 4. 生成 Embeddings (Data Engineering 關鍵)\n",
        "    # [cite: 12] 計算 Violation 與 Regulation 節點的 Embedding\n",
        "    logger.info(\"Step 2: 生成 BERT Embeddings (這可能需要一點時間)...\")\n",
        "    processor.action_b_generate_embeddings()\n",
        "\n",
        "    # 5. 語意剪枝 (Semantic Pruning)\n",
        "    # [cite: 8] 解決「噪聲爆炸」問題，執行 Embedding-based Pruning\n",
        "    logger.info(f\"Step 3: 執行語意剪枝 (去除冗餘的 VIOLATES_LAW)...\")\n",
        "    processor.action_c_semantic_pruning(threshold=args.threshold, top_k=args.top_k)\n",
        "\n",
        "    # 6. 層級注入 (Hierarchy)\n",
        "    logger.info(\"Step 4: 注入法律層級結構 (Regulation -> Law)...\")\n",
        "    processor.action_d_inject_hierarchy()\n",
        "\n",
        "    # 7. 計算權重 (Weighting)\n",
        "    logger.info(\"Step 5: 重新計算全局邊權重 (Inverse Frequency)...\")\n",
        "    processor.action_a_compute_edge_weights()\n",
        "\n",
        "    # 8. 準備負採樣 (Hard Negatives)\n",
        "    # [cite: 92] 針對訓練數據不足，準備「困難負樣本」\n",
        "    logger.info(\"Step 6: 生成對抗性訓練用的困難負樣本 (Hard Negatives)...\")\n",
        "    processor.action_prep_hard_negatives(top_k=5)\n",
        "\n",
        "    # 9. 匯出與儲存\n",
        "    final_data = processor.export_data()\n",
        "\n",
        "    logger.info(f\"Step 7: 儲存處理結果至 {args.output}...\")\n",
        "    torch.save(final_data, args.output)\n",
        "\n",
        "    # 10. 人工驗證\n",
        "    #  隨機抽樣檢查，確保資料工程品質\n",
        "    inspect_pruning_results(final_data['graph'], num_samples=5)\n",
        "\n",
        "    logger.info(\"✅ 流程結束。圖譜已稀疏化並注入語意邏輯。\")"
      ],
      "metadata": {
        "id": "9wdPmwzi2Waj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764,
          "referenced_widgets": [
            "0e06e942c2804ead8bf69e2bcbb7f250",
            "c6a4ac05853c4022b2001fd970d0c499",
            "14c4dc2cbae54e358b47108d00f73246",
            "fce2a4e3199543f1bf18801b07e00a94",
            "dec7b9b43d844a66b73be54cbc312f6e",
            "8c5ab94a60664cac9e63e57a4c65eac6",
            "057b1df14e5b48f5a3ca72ed22b9856f",
            "a78e649e5c34403ebb8f9d3e0f2c748e",
            "c9c75c04710147978515d64b6ac5b135",
            "122fd69a02df46d0b60b4e4e6cc2b3e2",
            "7c50cb92eefb442d9043a53753fd25e9"
          ]
        },
        "id": "OpDYZmdn2i7Y",
        "outputId": "88cf8e0f-c7ec-4a47-8670-29b4555fb3d3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT model: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2...\n",
            "Loaded KG with 2073 nodes.\n",
            "Building initial graph...\n",
            "Executing Action B: Generating Node Embeddings...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/65 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e06e942c2804ead8bf69e2bcbb7f250"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing Action C: Semantic Pruning (Threshold=0.7, Top-K=3)...\n",
            " -> Found 45409 VIOLATES_LAW edges to analyze.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pruning Edges: 100%|██████████| 559/559 [00:00<00:00, 2141.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " -> Applying pruning updates to graph...\n",
            " -> Removed 45409 noisy edges.\n",
            " -> Added 1666 semantic edges (VIOLATES_SPECIFICALLY / IS_RELEVANT_TO).\n",
            "Executing Action D: Injecting Hierarchy...\n",
            "Executing Action A: Re-calculating Global Edge Weights...\n",
            " -> Updated Class Weights: {'INVOLVES_OBJECT': 15.787037037037036, 'OCCURS_IN': 15.787037037037036, 'HAS_INCIDENT_TYPE': 15.787037037037036, 'HAS_CAUSE': 14.666666666666666, 'ENABLED_BY': 3410.0, 'LEADS_TO': 3.198874296435272, 'IS_RELEVANT_TO': 4.267834793491865, 'IS_SUBCLASS_OF': 7.957992998833139, 'PART_OF': 18.633879781420767, 'VIOLATES_SPECIFICALLY': 100.29411764705883, 'IS_SIMILAR_TO': 189.44444444444446}\n",
            "Executing Action: Preparing Hard Negative Candidates...\n",
            " -> Generated hard negative candidates for 438 regulations.\n",
            "Sample 1:\n",
            "  [事故/違規]: 勞工未採上鎖或設置標示防止他人操作粉碎機...\n",
            "  --[IS_RELEVANT_TO (score: 0.3150)]-->\n",
            "  [對應法規]: 職業安全衛生設施規則 第57條第1項...\n",
            "--------------------------------------------------\n",
            "Sample 2:\n",
            "  [事故/違規]: 雇主未指定管理人員執行衝床之傳動系統及耐壓管連接處之檢查...\n",
            "  --[IS_RELEVANT_TO (score: 0.2643)]-->\n",
            "  [對應法規]: 職業安全衛生設施規則 第57條第1項...\n",
            "--------------------------------------------------\n",
            "Sample 3:\n",
            "  [事故/違規]: 雇主未架設施工架或設置工作台...\n",
            "  --[IS_RELEVANT_TO (score: 0.2792)]-->\n",
            "  [對應法規]: 營造安全衛生設施標準 第149條第1項...\n",
            "--------------------------------------------------\n",
            "Sample 4:\n",
            "  [事故/違規]: 雇主未採取防止移動梯滑溜或轉動之措施...\n",
            "  --[IS_RELEVANT_TO (score: 0.2683)]-->\n",
            "  [對應法規]: 職業安全衛生設施規則 第229條...\n",
            "--------------------------------------------------\n",
            "Sample 5:\n",
            "  [事故/違規]: 雇主未實施協議事項...\n",
            "  --[IS_RELEVANT_TO (score: 0.2572)]-->\n",
            "  [對應法規]: 勞工健康保護規則 第10條第1項...\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get \"processed_oskg_data.pt\"\n",
        "    # 這份是製作 \"職業安全災害情境\" 與 \"相關原因與法律推論\" 成祖 JSON 檔案的輸入檔案。"
      ],
      "metadata": {
        "id": "grwxd3OB4ss6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "fGhI-izNIyeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [B] ground truth construction\n",
        "# manually upload\n",
        "    # osh_doc_merged.json"
      ],
      "metadata": {
        "id": "uailKmIvIxv8"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path):\n",
        "    \"\"\"讀取原始 JSON 檔案\"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"錯誤：找不到檔案 {file_path}\")\n",
        "        return []\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        try:\n",
        "            data = json.load(f)\n",
        "            return data\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"錯誤：JSON 格式解析失敗，請檢查原始檔案格式。\")\n",
        "            return []"
      ],
      "metadata": {
        "id": "Mb_J5uqkJJ4E"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_law_text(text):\n",
        "    \"\"\"\n",
        "    標準化法條文字：\n",
        "    1. 去除數字與文字間的空白 (e.g., \"第 19 條\" -> \"第19條\")\n",
        "    2. 統一全形半形括號等 (視需要，目前主要處理空白)\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    # 去除 \"第\" 與 \"數字\" 之間的空白，以及 \"數字\" 與 \"條/項/款\" 之間的空白\n",
        "    # Pattern explanation: Look for '第', optional space, digits, optional space, '條'\n",
        "    normalized = re.sub(r'第\\s*(\\d+)\\s*條', r'第\\1條', text)\n",
        "    normalized = re.sub(r'第\\s*(\\d+)\\s*項', r'第\\1項', normalized)\n",
        "    normalized = re.sub(r'第\\s*(\\d+)\\s*款', r'第\\1款', normalized)\n",
        "    return normalized"
      ],
      "metadata": {
        "id": "tYKx-QHiJjVS"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_legal_citations(text):\n",
        "    \"\"\"\n",
        "    從字串中提取具體的法規引用。\n",
        "    邏輯：\n",
        "    1. 先以逗號 ',' 分割不同的防範措施區塊。\n",
        "    2. 在每個區塊中，利用 Regex 抓取 '法規名稱' + '第X條'。\n",
        "\n",
        "    Return: 一個包含所有唯一法條引用的 list (Set to List)\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "\n",
        "    text = normalize_law_text(text)\n",
        "\n",
        "    # 用於儲存提取到的法條 (使用 set 避免重複)\n",
        "    citations = set()\n",
        "\n",
        "    # 你的資料中，多個法規組合通常用逗號分隔\n",
        "    # e.g. \"營造...第19條第1項暨職安法第6條第1項, 職安教育規則...第16條...\"\n",
        "    segments = text.split(',')\n",
        "\n",
        "    # 定義 Regex\n",
        "    # 捕捉模式： (法規名稱)(第幾條)\n",
        "    # 排除常見的連接詞或非透過法規名稱開頭的雜訊\n",
        "    # [\\u4e00-\\u9fa5]+ 匹配中文法規名稱\n",
        "    # ?P<law> 命名群組\n",
        "    law_pattern = re.compile(r'(?P<law>[\\u4e00-\\u9fa5]+?)\\s*第(?P<article>\\d+)條')\n",
        "\n",
        "    for segment in segments:\n",
        "        # 在每個段落中尋找所有匹配項 (因為可能有 \"A法...暨 B法...\")\n",
        "        matches = law_pattern.finditer(segment)\n",
        "        for match in matches:\n",
        "            law_name = match.group('law')\n",
        "            article_num = match.group('article')\n",
        "\n",
        "            # 過濾掉非由法規名稱構成的誤判 (例如只寫 \"暨職業安全衛生法\")\n",
        "            # 通常法規名稱長度大於 2\n",
        "            if len(law_name) >= 2:\n",
        "                # 這裡為了處理 \"暨\" 黏在前面的問題 (e.g., \"暨職業安全衛生法\")\n",
        "                if law_name.startswith('暨'):\n",
        "                    law_name = law_name[1:]\n",
        "\n",
        "                full_citation = f\"{law_name}第{article_num}條\"\n",
        "                citations.add(full_citation)\n",
        "\n",
        "    return sorted(list(citations))"
      ],
      "metadata": {
        "id": "E9G1UWZgJkmg"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_osh_data(input_file, output_file):\n",
        "    \"\"\"主處理流程\"\"\"\n",
        "    data = load_data(input_file)\n",
        "\n",
        "    if not data:\n",
        "        print(\"警告：讀取到的資料為空，無法進行處理。請確認 'osh_doc_merged.json' 是否已上傳至正確路徑。\")\n",
        "        return\n",
        "\n",
        "    processed_data = []\n",
        "\n",
        "    print(f\"開始處理 {len(data)} 筆案件資料...\")\n",
        "\n",
        "    for idx, item in enumerate(data):\n",
        "        # 提取需要的欄位\n",
        "        description = item.get('description', '')\n",
        "        raw_regulations = item.get('preventive_regulations', '')\n",
        "\n",
        "        # 提取黃金標準 (Ground Truth)\n",
        "        gold_laws = extract_legal_citations(raw_regulations)\n",
        "\n",
        "        # 為了讓你的 LLM 實驗更好做，我們保留致災原因摘要作為輔助 (可選)\n",
        "        cause_summary = item.get('cause_summary', '')\n",
        "\n",
        "        # 建構精簡後的物件\n",
        "        processed_item = {\n",
        "            \"id\": idx,  # 給予一個 ID 方便追蹤\n",
        "            \"original_incident_type\": item.get('incident_type', ''),\n",
        "            \"description\": description,  # 輸入 (Input)\n",
        "            \"cause_summary\": cause_summary, # 輔助輸入或驗證\n",
        "            \"ground_truth_laws\": gold_laws, # 輸出標籤 (Target Labels)\n",
        "            \"raw_regulation_text\": raw_regulations # 保留原始文字以供參考\n",
        "        }\n",
        "        processed_data.append(processed_item)\n",
        "\n",
        "    # 輸出檔案\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(processed_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(f\"處理完成！已輸出至 {output_file}\")\n",
        "\n",
        "    if processed_data:\n",
        "        print(f\"範例資料 (第一筆):\")\n",
        "        print(json.dumps(processed_data[0], ensure_ascii=False, indent=2))\n",
        "    else:\n",
        "        print(\"注意：處理後的資料列表為空。\")"
      ],
      "metadata": {
        "id": "8lIOaV81JweE"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 執行區塊\n",
        "# ==========================================\n",
        "\n",
        "# 假設你的檔案名稱如下，請確保檔案在同一目錄下\n",
        "input_filename = 'osh_doc_merged.json'\n",
        "output_filename = 'osh_legal_ground_truth.json'\n",
        "\n",
        "process_osh_data(input_filename, output_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyGaQILoJm3k",
        "outputId": "5c47c816-dca7-4dd7-b405-96a8cee4f40b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "開始處理 482 筆案件資料...\n",
            "處理完成！已輸出至 osh_legal_ground_truth.json\n",
            "範例資料 (第一筆):\n",
            "{\n",
            "  \"id\": 0,\n",
            "  \"original_incident_type\": \"墜落, 滾落\",\n",
            "  \"description\": \"於104 年8 月17 日徐○○、羅○○、陳○○及何○○4 人在○○股份有限公司廠\\n房東北側屋頂從事塑膠採光浪板更換作業，當時第1 片採光浪板已完成更換及鋪設，\\n於當日17 時15 分許，罹災者於該單位廠房東北側屋頂拆除第2 片原有的塑膠採光浪\\n板之固定用螺絲釘時，不慎踏穿塑膠採光浪自高度約7.35 公尺之開口墜落至地面，因\\n頭部外傷致顱腦損傷，送至衛生福利部豐原醫院急救無效，於104 年8 月17 日18 時\\n30 分宣告不治死亡。\",\n",
            "  \"cause_summary\": \"勞工未架設安全通道與踏板、勞工未使用安全帽、雇主未規劃安全通道與防墜設施、雇主未辦理勞工安全衛生教育訓練、雇主未訂定安全衛生工作守則、雇主未置丙種職業安全衛生業務主管、雇主未指派屋頂作業主管指揮或監督\",\n",
            "  \"ground_truth_laws\": [],\n",
            "  \"raw_regulation_text\": \"抱歉，我無法從您提供的內容中提取任何法規條文名稱。請提供包含法規條文名稱的文本。\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 以下是能捕捉到 \"項\" 的版本"
      ],
      "metadata": {
        "id": "xEkMo5RVKoUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import os\n",
        "\n",
        "def load_data(file_path):\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"錯誤：找不到檔案 {file_path}\")\n",
        "        return []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        try:\n",
        "            return json.load(f)\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"錯誤：JSON 格式解析失敗。\")\n",
        "            return []"
      ],
      "metadata": {
        "id": "y6Flue9JKsxt"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_law_text(text):\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    # 去除 \"第\" 與 \"數字\" 之間的空白，以及 \"數字\" 與 \"條/項/款\" 之間的空白\n",
        "    normalized = re.sub(r'第\\s*(\\d+)\\s*條', r'第\\1條', text)\n",
        "    normalized = re.sub(r'第\\s*(\\d+)\\s*項', r'第\\1項', normalized)\n",
        "    normalized = re.sub(r'第\\s*(\\d+)\\s*款', r'第\\1款', normalized)\n",
        "    return normalized"
      ],
      "metadata": {
        "id": "eU2Say8vKuAd"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_citations_hierarchical(text):\n",
        "    \"\"\"\n",
        "    同時提取「法條層級」與「法條+項層級」的參照。\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return [], []\n",
        "\n",
        "    text = normalize_law_text(text)\n",
        "\n",
        "    citations_article_only = set()   # 僅到條\n",
        "    citations_detailed = set()       # 到項\n",
        "\n",
        "    segments = text.split(',')\n",
        "\n",
        "    # Regex: 捕捉 法規名稱 + 第X條 + (可選)第Y項\n",
        "    law_pattern = re.compile(r'(?P<law>[\\u4e00-\\u9fa5]+?)\\s*第(?P<article>\\d+)條(?:第(?P<paragraph>\\d+)項)?')\n",
        "\n",
        "    for segment in segments:\n",
        "        matches = law_pattern.finditer(segment)\n",
        "        for match in matches:\n",
        "            law_name = match.group('law')\n",
        "            article_num = match.group('article')\n",
        "            paragraph_num = match.group('paragraph')\n",
        "\n",
        "            if len(law_name) >= 2:\n",
        "                if law_name.startswith('暨'):\n",
        "                    law_name = law_name[1:]\n",
        "\n",
        "                # 1. Coarse Level\n",
        "                article_citation = f\"{law_name}第{article_num}條\"\n",
        "                citations_article_only.add(article_citation)\n",
        "\n",
        "                # 2. Detailed Level\n",
        "                if paragraph_num:\n",
        "                    detailed_citation = f\"{law_name}第{article_num}條第{paragraph_num}項\"\n",
        "                else:\n",
        "                    detailed_citation = f\"{law_name}第{article_num}條\"\n",
        "\n",
        "                citations_detailed.add(detailed_citation)\n",
        "\n",
        "    return sorted(list(citations_article_only)), sorted(list(citations_detailed))"
      ],
      "metadata": {
        "id": "o-oLiE0MKva4"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_osh_data_filtered(input_file, output_file):\n",
        "    data = load_data(input_file)\n",
        "    processed_data = []\n",
        "\n",
        "    removed_count = 0 # 統計被移除的筆數\n",
        "\n",
        "    print(f\"原始資料共 {len(data)} 筆，開始處理與過濾...\")\n",
        "\n",
        "    for idx, item in enumerate(data):\n",
        "        raw_regulations = item.get('preventive_regulations', '')\n",
        "\n",
        "        # 提取法條\n",
        "        c_article, c_detailed = extract_citations_hierarchical(raw_regulations)\n",
        "\n",
        "        # --- 關鍵修正：過濾機制 ---\n",
        "        # 如果 coarse 層級是空的，代表沒有提取到任何有效法條，直接跳過\n",
        "        if not c_article:\n",
        "            removed_count += 1\n",
        "            continue\n",
        "\n",
        "        # 準備資料\n",
        "        description_text = item.get('description', '')\n",
        "        cause_summary_text = item.get('cause_summary', '')\n",
        "\n",
        "        processed_item = {\n",
        "            \"id\": item.get('id', idx), # 如果原資料有 id 就用原來的，否則用 index\n",
        "            \"original_incident_type\": item.get('incident_type', ''),\n",
        "            \"description\": description_text,\n",
        "            \"cause_summary\": cause_summary_text,\n",
        "            \"ground_truth_coarse\": c_article,\n",
        "            \"ground_truth_fine\": c_detailed,\n",
        "            \"raw_regulation_text\": raw_regulations\n",
        "        }\n",
        "        processed_data.append(processed_item)\n",
        "\n",
        "    # 輸出結果\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(processed_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"處理完成！\")\n",
        "    print(f\"原始筆數: {len(data)}\")\n",
        "    print(f\"移除無效筆數: {removed_count}\")\n",
        "    print(f\"最終有效筆數: {len(processed_data)}\")\n",
        "    print(f\"已輸出至 {output_file}\")"
      ],
      "metadata": {
        "id": "LyAeq6O3Kw2k"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 執行區塊\n",
        "# ==========================================\n",
        "input_filename = 'osh_doc_merged.json'\n",
        "output_filename = 'osh_legal_ground_truth_cleaned.json'\n",
        "\n",
        "# 請執行此函式\n",
        "process_osh_data_filtered(input_filename, output_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRAHifbEKyk9",
        "outputId": "0a60cf28-5011-4934-a27d-58ed5cc35414"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "原始資料共 482 筆，開始處理與過濾...\n",
            "------------------------------\n",
            "處理完成！\n",
            "原始筆數: 482\n",
            "移除無效筆數: 101\n",
            "最終有效筆數: 381\n",
            "已輸出至 osh_legal_ground_truth_cleaned.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "y0MYVNIqIzbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [C]\n",
        "# related files\n",
        "    # already load knowledge_graph_final.json in [A]\n",
        "    # get \"osh_legal_ground_truth_cleaned.json\" in [B]\n",
        "    # manually upload boxe_validation_set_clean.json"
      ],
      "metadata": {
        "id": "TPFWPrbZ8Cp4"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## CONFIGURATION ##\n",
        "# Load KG\n",
        "with open('knowledge_graph_final.json', 'r') as f:\n",
        "    kg = json.load(f)\n",
        "\n",
        "# Load GT\n",
        "with open('osh_legal_ground_truth_cleaned.json', 'r') as f:\n",
        "    gt = json.load(f)"
      ],
      "metadata": {
        "id": "n-E3X4nP_Rrf"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect KG nodes\n",
        "print(\"KG Nodes sample:\")\n",
        "print(kg['nodes'][:2])\n",
        "\n",
        "# Inspect GT sample\n",
        "print(\"GT sample:\")\n",
        "print(gt[:1])\n",
        "\n",
        "# Check if we can find a link between GT description and KG nodes\n",
        "kg_incident_nodes = [n for n in kg['nodes'] if n.get('node_type') == 'Incident' or 'INC_' in n['id']]\n",
        "print(f\"Number of Incident nodes in KG: {len(kg_incident_nodes)}\")\n",
        "if kg_incident_nodes:\n",
        "    print(\"Sample Incident Node:\", kg_incident_nodes[0])\n",
        "\n",
        "# Check Law nodes\n",
        "kg_law_nodes = [n for n in kg['nodes'] if n.get('node_type') == 'Regulation' or 'REG_' in n['id']]\n",
        "print(f\"Number of Law nodes in KG: {len(kg_law_nodes)}\")\n",
        "if kg_law_nodes:\n",
        "    print(\"Sample Law Node:\", kg_law_nodes[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ6_oKpO_qYh",
        "outputId": "aa00cb69-b01e-48c2-9d30-d5446b7c3a1e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KG Nodes sample:\n",
            "[{'id': 'CAUSE_BASIC_7678f5703ca4_ATOMIC_0', 'label': '基本原因 基本原因 基本原因：', 'node_type': 'Cause_Basic_Atomic', 'parent_id': 'CAUSE_BASIC_7678f5703ca4', 'atomic_index': 0, 'embedding_text': '基本原因 基本原因 基本原因：', 'is_atomized': True, 'original_full_text': '基本原因 基本原因 基本原因： (1)未對勞工施以從事各該工作必要之一般安全衛生教育訓練。 (2)未訂定安全衛生工作守則。 (3)未置丙種職業安全衛生業務主管。 (4)勞工於屋頂從事作業未指派屋頂作業主管指揮或監督。', 'split_method': 'regex'}, {'id': 'CAUSE_BASIC_7678f5703ca4_ATOMIC_1', 'label': '未對勞工施以從事各該工作必要之一般安全衛生教育訓練。', 'node_type': 'Cause_Basic_Atomic', 'parent_id': 'CAUSE_BASIC_7678f5703ca4', 'atomic_index': 1, 'embedding_text': '未對勞工施以從事各該工作必要之一般安全衛生教育訓練。', 'is_atomized': True, 'original_full_text': '基本原因 基本原因 基本原因： (1)未對勞工施以從事各該工作必要之一般安全衛生教育訓練。 (2)未訂定安全衛生工作守則。 (3)未置丙種職業安全衛生業務主管。 (4)勞工於屋頂從事作業未指派屋頂作業主管指揮或監督。', 'split_method': 'regex'}]\n",
            "GT sample:\n",
            "[{'id': 1, 'original_incident_type': '墜落, 滾落', 'description': '104 年9 月3 日約10 時許，罹災者賴○昌與彭○德、許○福、鄭○龍等4\\n人於大肚區遊園路○段○巷○弄○號對面之屋頂進行頂棚違建拆除作業，\\n約自10 時20 分許，罹災者賴○昌為收拾乙炔焊接工具，故由靠遊園路○\\n段○巷○弄北向往左數第2 塊頂棚南側第8 個採光罩旁往北方向第12 個\\n採光罩移動，因安全母索過短無法有效使用，便將安全帶脫鉤，且屋頂頂\\n棚上未設置適當強度且寬度30 公分以上之踏板，下方亦未裝設堅固格柵\\n或安全網，罹災者賴○昌不慎踏穿第12 個採光罩自高度約22 公尺墜落至\\n地面，因周身挫傷併多發性骨折、器官損傷致休克，經送醫院急救無效，\\n於104 年9 月4 日15 時3 分宣告不治死亡。', 'cause_summary': '「勞工未架設適當強度且寬度30公分以上之踏板、勞工未使用堅固格柵或安全網、雇主未指派屋頂作業主管於現場辦理指揮與監督、雇主未使勞工接受一般安全衛生教育訓練、雇主未訂定安全衛生工作守則」', 'ground_truth_coarse': ['勞工健康保護規則第10條', '職業安全衛生教育訓練規則第16條', '職業安全衛生法第20條', '職業安全衛生法第23條', '職業安全衛生法第32條', '職業安全衛生法第34條', '職業安全衛生管理辦法第12條', '職業安全衛生管理辦法第79條'], 'ground_truth_fine': ['勞工健康保護規則第10條', '職業安全衛生教育訓練規則第16條第1項', '職業安全衛生法第20條第1項', '職業安全衛生法第23條第1項', '職業安全衛生法第32條第1項', '職業安全衛生法第34條第1項', '職業安全衛生管理辦法第12條', '職業安全衛生管理辦法第79條'], 'raw_regulation_text': '職業安全衛生教育訓練規則第16條第1項暨職業安全衛生法第32條第1項,職業安全衛生法第34條第1項,職業安全衛生管理辦法第79條暨職業安全衛生法第23條第1項,職業安全衛生管理辦法第12條之1暨職業安全衛生法第23條第1項,勞工健康保護規則第10條暨職業安全衛生法第20條第1項'}]\n",
            "Number of Incident nodes in KG: 431\n",
            "Sample Incident Node: {'id': 'INC_4becb25cffa5', 'label': '罹災者在更換塑膠採光浪板時不慎墜落7.35公尺，因顱腦損傷於送醫後不治身亡。', 'node_type': 'Incident', 'parent_id': 'INC_4becb25cffa5', 'atomic_index': 0, 'embedding_text': '罹災者在更換塑膠採光浪板時不慎墜落7.35公尺，因顱腦損傷於送醫後不治身亡。 罹災者在更換塑膠採光浪板時不慎墜落7.35公尺，因顱腦損傷於送醫後不治身亡。', 'is_atomized': True, 'full_text': '罹災者在更換塑膠採光浪板時不慎墜落7.35公尺，因顱腦損傷於送醫後不治身亡。', 'industry': '批發及零售業-金屬建材批發業（4615）', 'incident_type': '墜落, 滾落', 'incident_type_id': '1'}\n",
            "Number of Law nodes in KG: 438\n",
            "Sample Law Node: {'id': 'REG_7d9bbf249b71', 'label': '職業安全衛生教育訓練規則 第16條第1項', 'node_type': 'Regulation', 'parent_id': 'REG_7d9bbf249b71', 'atomic_index': 0, 'embedding_text': '職業安全衛生教育訓練規則 第16條第1項', 'is_atomized': True, 'law_name': '職業安全衛生教育訓練規則', 'legal_ref_id': 'law_職業安全衛生教育訓練規則_art_16_para_1', 'full_text': '1   雇主對工作場所急救人員，應使其接受急救人員之安全衛生教育訓練。但醫護人員及緊急醫療救\\n護法所定之救護技術員，不在此限。\\n2   前項教育訓練課程及時數，依附表十三之規定。', 'mapping_method': 'exact_v9', 'normalized_label': '職業安全衛生教育訓練規則第16條第1項'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import difflib\n",
        "# Prepare KG Incident texts\n",
        "kg_incidents = []\n",
        "for n in kg['nodes']:\n",
        "    if n['node_type'] == 'Incident':\n",
        "        # Use full_text or label\n",
        "        text = n.get('full_text', n.get('label', ''))\n",
        "        kg_incidents.append({'id': n['id'], 'text': text})\n",
        "\n",
        "# Prepare GT texts\n",
        "gt_incidents = []\n",
        "for item in gt:\n",
        "    gt_incidents.append({'id': item['id'], 'text': item['description']})\n",
        "\n",
        "print(f\"Total KG Incidents: {len(kg_incidents)}\")\n",
        "print(f\"Total GT Incidents: {len(gt_incidents)}\")\n",
        "\n",
        "# Try to match a few\n",
        "matched = 0\n",
        "mapping = {} # gt_id -> kg_id\n",
        "\n",
        "for g in gt_incidents[:10]: # Test first 10\n",
        "    best_ratio = 0\n",
        "    best_id = None\n",
        "    g_text = g['text']\n",
        "\n",
        "    # Simple strategy: Check if KG text is contained in GT text?\n",
        "    # Or similarity\n",
        "    for k in kg_incidents:\n",
        "        # Check containment first (likely high recall if KG is summary)\n",
        "        if k['text'] in g_text:\n",
        "             # If multiple match, take the longest?\n",
        "             # For now just take first or high similarity\n",
        "             ratio = 1.0\n",
        "             if ratio > best_ratio:\n",
        "                 best_ratio = ratio\n",
        "                 best_id = k['id']\n",
        "                 break # Exact substring match found\n",
        "\n",
        "        # Fallback to SequenceMatcher\n",
        "        s = difflib.SequenceMatcher(None, k['text'], g_text)\n",
        "        ratio = s.ratio() # This might be low if lengths differ significantly\n",
        "        # Better: use k['text'] vs relevant part of g['text']?\n",
        "        # Actually, let's just try to find if k['text'] is a substring.\n",
        "\n",
        "    if best_id:\n",
        "        mapping[g['id']] = best_id\n",
        "        matched += 1\n",
        "        print(f\"GT {g['id']} matched to {best_id}\")\n",
        "    else:\n",
        "        # Try finding one with high word overlap?\n",
        "        pass\n",
        "\n",
        "print(f\"Matched {matched}/10 in sample.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k74x3iBw_64N",
        "outputId": "6aa54078-7902-4703-8949-009c2a697e1a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total KG Incidents: 431\n",
            "Total GT Incidents: 381\n",
            "Matched 0/10 in sample.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Prepare corpus\n",
        "kg_texts = [i['text'] for i in kg_incidents]\n",
        "kg_ids = [i['id'] for i in kg_incidents]\n",
        "gt_texts = [i['text'] for i in gt_incidents]\n",
        "gt_ids = [i['id'] for i in gt_incidents]\n",
        "\n",
        "# Vectorize (Character level might be better for Chinese if no tokenizer)\n",
        "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4)) # Use char n-grams\n",
        "kg_vecs = vectorizer.fit_transform(kg_texts)\n",
        "gt_vecs = vectorizer.transform(gt_texts)\n",
        "\n",
        "# Similarity\n",
        "similarity_matrix = cosine_similarity(gt_vecs, kg_vecs)\n",
        "\n",
        "# Find best matches\n",
        "matches = []\n",
        "match_scores = []\n",
        "gt_to_kg_map = {}\n",
        "\n",
        "for i in range(len(gt_texts)):\n",
        "    best_idx = np.argmax(similarity_matrix[i])\n",
        "    score = similarity_matrix[i, best_idx]\n",
        "    gt_id = gt_ids[i]\n",
        "    kg_id = kg_ids[best_idx]\n",
        "\n",
        "    matches.append((gt_id, kg_id, score))\n",
        "    gt_to_kg_map[gt_id] = kg_id\n",
        "    if i < 5:\n",
        "        print(f\"GT {gt_id} matches KG {kg_id} with score {score:.3f}\")\n",
        "        print(f\"GT: {gt_texts[i][:50]}...\")\n",
        "        print(f\"KG: {kg_texts[best_idx]}\")\n",
        "        print(\"-\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDzZYiueACNr",
        "outputId": "9df594e4-9a84-4c53-fa76-687fc3edf001"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GT 1 matches KG INC_86e34d008e50 with score 0.214\n",
            "GT: 104 年9 月3 日約10 時許，罹災者賴○昌與彭○德、許○福、鄭○龍等4\n",
            "人於大肚區遊園路○段○...\n",
            "KG: 賴○昌於屋頂拆除作業時因安全措施不足，踏穿採光罩從22公尺高墜落，送醫不治身亡。\n",
            "--------------------\n",
            "GT 3 matches KG INC_292ec42abe68 with score 0.496\n",
            "GT: 於104 年6 月8 日1 時許，盧○○及林○○從事機械基本維護及異常\n",
            "排除等作業時，發現抽水泵及馬...\n",
            "KG: 盧○○在維護作業中被抽水泵及馬達皮帶輪碎片擊中右大腿，送醫不治身亡。\n",
            "--------------------\n",
            "GT 6 matches KG INC_08b698fad063 with score 0.613\n",
            "GT: 勞工李○○104年4月30日下午2時駕駛公務用貨車從事送貨作業，\n",
            "行經臺中市大甲區日南里中山路二段9...\n",
            "KG: 勞工李○○於104年4月30日下午2時駕駛公務用貨車發生事故。\n",
            "--------------------\n",
            "GT 7 matches KG INC_4a432d550377 with score 0.498\n",
            "GT: 於104 年10 月20 日14 時30 分許，罹災者傅○欽開預拌混凝土車，載送\n",
            "4 立方公尺之混凝...\n",
            "KG: 傅○欽駕駛預拌混凝土車於裡冷林道因土石鬆軟翻覆墜落150公尺深谷，車重超過額定載重。\n",
            "--------------------\n",
            "GT 8 matches KG INC_bed95b76352d with score 0.579\n",
            "GT: 依據目擊者林泳龍稱述：「罹災者邵○魯於104年10月30日15時50分許\n",
            "運送混凝土至臺中市太○區山...\n",
            "KG: 邵○魯於104年10月30日運送混凝土時，車輛倒退滑行15公尺後墜入山谷。\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check graph structure\n",
        "import networkx as nx\n",
        "\n",
        "G = nx.DiGraph()\n",
        "for node in kg['nodes']:\n",
        "    G.add_node(node['id'], label=node.get('label', ''), type=node.get('node_type'))\n",
        "\n",
        "for link in kg['links']:\n",
        "    G.add_edge(link['source'], link['target'], relation=link['relation'])\n",
        "\n",
        "# Pick a sample Incident Node that we matched\n",
        "sample_incident_id = 'INC_86e34d008e50' # Matches GT 1\n",
        "if sample_incident_id in G:\n",
        "    print(f\"Neighbors of {sample_incident_id}:\")\n",
        "    for n in G.successors(sample_incident_id):\n",
        "        print(f\" -> {G.nodes[n]['type']} : {G.nodes[n]['label']} (Relation: {G[sample_incident_id][n]['relation']})\")\n",
        "\n",
        "    # Check if we can reach Regulation\n",
        "    # BFS to find Regulations\n",
        "    print(\"\\nReachable Regulations (within 3 hops):\")\n",
        "    paths = nx.single_source_shortest_path(G, sample_incident_id, cutoff=3)\n",
        "    found_regs = []\n",
        "    for target, path in paths.items():\n",
        "        if G.nodes[target]['type'] == 'Regulation' or 'REG_' in target:\n",
        "             found_regs.append((target, path))\n",
        "\n",
        "    print(f\"Found {len(found_regs)} regulations.\")\n",
        "    if found_regs:\n",
        "        print(f\"Sample Path: {found_regs[0]}\")\n",
        "else:\n",
        "    print(\"Sample Incident ID not in graph?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeDcAc0uAPNP",
        "outputId": "5c9a5604-8c43-4ff1-e0ed-9ead489055b7"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neighbors of INC_86e34d008e50:\n",
            " -> Medium_Specific : 屋頂, 屋架, 樑 (Relation: INVOLVES_OBJECT)\n",
            " -> Industry : 建築工程業（4100） (Relation: OCCURS_IN)\n",
            " -> IncidentType : 墜落, 滾落 (Relation: HAS_INCIDENT_TYPE)\n",
            "\n",
            "Reachable Regulations (within 3 hops):\n",
            "Found 0 regulations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Incident: INC_86e34d008e50\n",
        "# Incoming Edge: CAUSE_DIRECT_46b946692708 -> INC_86e34d008e50\n",
        "cause_id = 'CAUSE_DIRECT_46b946692708'\n",
        "\n",
        "print(f\"Neighbors of Cause {cause_id}:\")\n",
        "# Outgoing from Cause\n",
        "for n in G.successors(cause_id):\n",
        "    print(f\" -> {G.nodes[n]['type']} : {G.nodes[n]['label']} (Relation: {G[cause_id][n]['relation']})\")\n",
        "\n",
        "# Incoming to Cause\n",
        "print(f\"Predecessors of Cause {cause_id}:\")\n",
        "for n in G.predecessors(cause_id):\n",
        "    print(f\" <- {G.nodes[n]['type']} : {G.nodes[n]['label']} (Relation: {G[n][cause_id]['relation']})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qF05mMdiAQ8S",
        "outputId": "a9985115-7a12-4dfa-88fd-53cc79e3d94c"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neighbors of Cause CAUSE_DIRECT_46b946692708:\n",
            " -> Incident : 賴○昌於屋頂拆除作業時因安全措施不足，踏穿採光罩從22公尺高墜落，送醫不治身亡。 (Relation: HAS_CAUSE)\n",
            " -> Violation : 勞工未架設適當強度且寬度30公分以上之踏板 (Relation: LEADS_TO)\n",
            " -> Violation : 勞工未使用堅固格柵或安全網 (Relation: LEADS_TO)\n",
            " -> Violation : 雇主未訂定安全衛生工作守則 (Relation: LEADS_TO)\n",
            " -> Violation : 雇主未辦理勞工安全衛生教育訓練 (Relation: LEADS_TO)\n",
            " -> Violation : 雇主未指派屋頂作業主管指揮或監督 (Relation: LEADS_TO)\n",
            " -> Incident : 勞工張○○於屋頂補漏水時未使用安全設備，踩穿採光罩墜落，送醫不治。 (Relation: HAS_CAUSE)\n",
            " -> Violation : 雇主未規劃防墜設施如堅固格柵或安全網 (Relation: LEADS_TO)\n",
            " -> Violation : 雇主未指定專人指揮或監督作業 (Relation: LEADS_TO)\n",
            " -> Violation : 雇主未訂定職業安全衛生管理計畫 (Relation: LEADS_TO)\n",
            " -> Violation : 勞工未架設安全通道與踏板 (Relation: LEADS_TO)\n",
            " -> Violation : 勞工未使用安全帽 (Relation: LEADS_TO)\n",
            " -> Violation : 雇主未定訂自動檢查計畫 (Relation: LEADS_TO)\n",
            " -> Incident : 104年6月29日，范罹災者更換屋頂浪板時踏穿採光板墜落12公尺，送醫不治。 (Relation: HAS_CAUSE)\n",
            " -> Violation : 雇主未於屋架上設置適當強度且寬度在30公分以上之踏板 (Relation: LEADS_TO)\n",
            " -> Violation : 雇主未於屋架下方適當範圍裝設堅固格柵或安全網等防墜設施 (Relation: LEADS_TO)\n",
            " -> Violation : 雇主未置職業安全衛生人員 (Relation: LEADS_TO)\n",
            " -> Violation : 雇主未規劃安全通道與防墜設施 (Relation: LEADS_TO)\n",
            " -> Violation : 雇主未實施吊掛作業危害之辨識 (Relation: LEADS_TO)\n",
            " -> Violation : 評估及控制措施 (Relation: LEADS_TO)\n",
            " -> Incident : 107年5月16日，台中市，罹災者踩穿7樓塑膠板採光罩墜落，頭胸部重傷，7月31日不治身亡。 (Relation: HAS_CAUSE)\n",
            " -> Violation : 雇主未落實承攬管理事項 (Relation: LEADS_TO)\n",
            " -> Violation : 雇主未於設計或施工規劃階段實施風險評估 (Relation: LEADS_TO)\n",
            "Predecessors of Cause CAUSE_DIRECT_46b946692708:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Cause Label\n",
        "cause_node = [n for n in kg['nodes'] if n['id'] == 'CAUSE_DIRECT_46b946692708'][0]\n",
        "print(\"Cause Label:\", cause_node.get('label'))\n",
        "print(\"Cause Full Text:\", cause_node.get('original_full_text', cause_node.get('embedding_text')))\n",
        "\n",
        "# Check Violation Labels for this cause\n",
        "violations = [n for n in G.successors(cause_node['id']) if G.nodes[n]['type'] == 'Violation']\n",
        "print(\"\\nViolations:\")\n",
        "for v in violations:\n",
        "    print(f\"- {G.nodes[v]['label']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDW7EWWhAShZ",
        "outputId": "156373c5-4538-4bc0-c363-cae166bfbf43"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cause Label: (一)直接原因：罹災者不慎踏穿採光罩自高度約22 公尺之開口墜落至地面， 造成周身挫傷併多發性骨折、器官損傷致休克不治死亡。 (二)間接原因： 不安全狀況： 於塑膠材料構築之屋頂作業時，未事先規劃安全通道，於屋架頂 棚上設置適當強度且寬度30 公分以上之踏板，於屋架下方亦未裝設 堅固格柵或安全網。 (三)基本原因： 1、未指派屋頂作業主管於現場辦理指揮、監督等工作。 2、未使勞工接受一般安全衛生教育訓練。 3、未訂定安全衛生工作守則。\n",
            "Cause Full Text: (一)直接原因：罹災者不慎踏穿採光罩自高度約22 公尺之開口墜落至地面， 造成周身挫傷併多發性骨折、器官損傷致休克不治死亡。 (二)間接原因： 不安全狀況： 於塑膠材料構築之屋頂作業時，未事先規劃安全通道，於屋架頂 棚上設置適當強度且寬度30 公分以上之踏板，於屋架下方亦未裝設 堅固格柵或安全網。 (三)基本原因： 1、未指派屋頂作業主管於現場辦理指揮、監督等工作。 2、未使勞工接受一般安全衛生教育訓練。 3、未訂定安全衛生工作守則。\n",
            "\n",
            "Violations:\n",
            "- 勞工未架設適當強度且寬度30公分以上之踏板\n",
            "- 勞工未使用堅固格柵或安全網\n",
            "- 雇主未訂定安全衛生工作守則\n",
            "- 雇主未辦理勞工安全衛生教育訓練\n",
            "- 雇主未指派屋頂作業主管指揮或監督\n",
            "- 雇主未規劃防墜設施如堅固格柵或安全網\n",
            "- 雇主未指定專人指揮或監督作業\n",
            "- 雇主未訂定職業安全衛生管理計畫\n",
            "- 勞工未架設安全通道與踏板\n",
            "- 勞工未使用安全帽\n",
            "- 雇主未定訂自動檢查計畫\n",
            "- 雇主未於屋架上設置適當強度且寬度在30公分以上之踏板\n",
            "- 雇主未於屋架下方適當範圍裝設堅固格柵或安全網等防墜設施\n",
            "- 雇主未置職業安全衛生人員\n",
            "- 雇主未規劃安全通道與防墜設施\n",
            "- 雇主未實施吊掛作業危害之辨識\n",
            "- 評估及控制措施\n",
            "- 雇主未落實承攬管理事項\n",
            "- 雇主未於設計或施工規劃階段實施風險評估\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "fkqbmaMyAZJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import difflib\n",
        "import networkx as nx\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "aZl_GimbAUV3"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load Data\n",
        "with open('knowledge_graph_final.json', 'r', encoding='utf-8') as f:\n",
        "    kg_data = json.load(f)\n",
        "\n",
        "with open('osh_legal_ground_truth_cleaned.json', 'r', encoding='utf-8') as f:\n",
        "    gt_data = json.load(f)\n",
        "\n",
        "# 2. Build Graph\n",
        "G = nx.DiGraph()\n",
        "for node in kg_data['nodes']:\n",
        "    G.add_node(node['id'], **node)\n",
        "\n",
        "for link in kg_data['links']:\n",
        "    G.add_edge(link['source'], link['target'], relation=link['relation'])"
      ],
      "metadata": {
        "id": "YvDpMoKeAYW_"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load Data\n",
        "with open('knowledge_graph_final.json', 'r', encoding='utf-8') as f:\n",
        "    kg_data = json.load(f)\n",
        "\n",
        "with open('osh_legal_ground_truth_cleaned.json', 'r', encoding='utf-8') as f:\n",
        "    gt_data = json.load(f)\n",
        "\n",
        "# 2. Build Graph\n",
        "G = nx.DiGraph()\n",
        "for node in kg_data['nodes']:\n",
        "    G.add_node(node['id'], **node)\n",
        "\n",
        "for link in kg_data['links']:\n",
        "    G.add_edge(link['source'], link['target'], relation=link['relation'])\n",
        "\n",
        "# 3. Mappings\n",
        "\n",
        "# 3a. Incident Mapping (GT ID -> KG Node ID)\n",
        "print(\"Mapping Incidents...\")\n",
        "kg_incidents = []\n",
        "for n in kg_data['nodes']:\n",
        "    if n['node_type'] == 'Incident':\n",
        "        text = n.get('full_text', n.get('label', ''))\n",
        "        kg_incidents.append({'id': n['id'], 'text': text})\n",
        "\n",
        "kg_texts = [i['text'] for i in kg_incidents]\n",
        "kg_ids = [i['id'] for i in kg_incidents]\n",
        "\n",
        "gt_texts = [item['description'] for item in gt_data]\n",
        "gt_ids = [item['id'] for item in gt_data]\n",
        "\n",
        "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))\n",
        "kg_vecs = vectorizer.fit_transform(kg_texts)\n",
        "gt_vecs = vectorizer.transform(gt_texts)\n",
        "\n",
        "similarity_matrix = cosine_similarity(gt_vecs, kg_vecs)\n",
        "\n",
        "gt_to_kg_incident = {}\n",
        "for i in range(len(gt_texts)):\n",
        "    best_idx = np.argmax(similarity_matrix[i])\n",
        "    score = similarity_matrix[i, best_idx]\n",
        "    if score > 0.1: # Low threshold as texts might vary significantly\n",
        "        gt_to_kg_incident[gt_ids[i]] = kg_ids[best_idx]\n",
        "\n",
        "print(f\"Mapped {len(gt_to_kg_incident)}/{len(gt_data)} incidents.\")\n",
        "\n",
        "# 3b. Law Mapping (Normalized String -> List of KG Node IDs)\n",
        "print(\"Mapping Laws...\")\n",
        "def normalize_law(text):\n",
        "    # Remove whitespace and specific punctuation, keep text\n",
        "    return re.sub(r'[^\\w]', '', text)\n",
        "\n",
        "kg_regulations = []\n",
        "for n in kg_data['nodes']:\n",
        "    if n['node_type'] == 'Regulation':\n",
        "        label = n.get('label', '')\n",
        "        norm = normalize_law(label)\n",
        "        kg_regulations.append({'id': n['id'], 'norm': norm, 'label': label, 'full_text': n.get('full_text', '')})\n",
        "\n",
        "# Create a lookup\n",
        "# Since matching is fuzzy (substring), we iterate during lookup\n",
        "# To speed up, we can organize by law name, but brute force on 400 nodes is fine.\n",
        "\n",
        "# 4. Generate SFT Data\n",
        "sft_data = []\n",
        "\n",
        "for case in gt_data:\n",
        "    gt_id = case['id']\n",
        "    kg_incident_id = gt_to_kg_incident.get(gt_id)\n",
        "\n",
        "    if not kg_incident_id:\n",
        "        continue # Skip if no incident match\n",
        "\n",
        "    incident_text = case['description'] # Use GT description as input\n",
        "    gt_laws = case['ground_truth_coarse'] # List of strings\n",
        "\n",
        "    # Reasoning Generation\n",
        "    reasoning_parts = []\n",
        "    reasoning_parts.append(f\"事故原因分析：\\n{incident_text}\")\n",
        "    reasoning_parts.append(\"\\n法規適用性分析：\")\n",
        "\n",
        "    # Find reachable violations/regulations from Incident\n",
        "    # Path: Incident <- Cause -> Violation -> Regulation\n",
        "    # In Graph G:\n",
        "    # Predecessors of Incident (Cause) -> Successors (Violation) -> Successors (Regulation)\n",
        "\n",
        "    reachable_regs = {} # Reg_ID -> Path Info\n",
        "\n",
        "    # Causes (Predecessors of Incident)\n",
        "    causes = list(G.predecessors(kg_incident_id))\n",
        "\n",
        "    for cause_id in causes:\n",
        "        # Violations (Successors of Cause)\n",
        "        violations = [n for n in G.successors(cause_id) if G.nodes[n].get('node_type') == 'Violation']\n",
        "\n",
        "        for vio_id in violations:\n",
        "            # Regulations (Successors of Violation)\n",
        "            regs = [n for n in G.successors(vio_id) if G.nodes[n].get('node_type') == 'Regulation']\n",
        "\n",
        "            for reg_id in regs:\n",
        "                if reg_id not in reachable_regs:\n",
        "                    reachable_regs[reg_id] = []\n",
        "                reachable_regs[reg_id].append({\n",
        "                    'cause': G.nodes[cause_id],\n",
        "                    'violation': G.nodes[vio_id],\n",
        "                    'regulation': G.nodes[reg_id]\n",
        "                })\n",
        "\n",
        "    # Match GT Laws to Reachable Regs\n",
        "    used_laws = set()\n",
        "    law_counter = 1\n",
        "\n",
        "    for law_str in gt_laws:\n",
        "        norm_gt = normalize_law(law_str)\n",
        "\n",
        "        # Find matches in reachable_regs\n",
        "        matches = []\n",
        "        for reg_id, paths in reachable_regs.items():\n",
        "            norm_kg = normalize_law(G.nodes[reg_id].get('label', ''))\n",
        "            # Check containment\n",
        "            if norm_gt in norm_kg or norm_kg in norm_gt:\n",
        "                matches.append((reg_id, paths))\n",
        "\n",
        "        if matches:\n",
        "            # Construct Graph-Based Reasoning\n",
        "            # Use the first match/path for simplicity\n",
        "            match_reg_id, paths = matches[0]\n",
        "            path = paths[0]\n",
        "            vio_label = path['violation'].get('label', '')\n",
        "            reg_label = path['regulation'].get('label', '')\n",
        "\n",
        "            reasoning_parts.append(f\"\\n{law_counter}. 違反法規：{reg_label}\")\n",
        "            reasoning_parts.append(f\"   推論路徑：本案顯示「{vio_label}」。\")\n",
        "            reasoning_parts.append(f\"   依據圖譜分析，此行為直接違反了 {reg_label}。\")\n",
        "            used_laws.add(law_str)\n",
        "        else:\n",
        "            # Fallback RAG\n",
        "            # Find the best matching KG node even if not reachable\n",
        "            best_node = None\n",
        "            for r in kg_regulations:\n",
        "                if norm_gt in r['norm'] or r['norm'] in norm_gt:\n",
        "                    best_node = r\n",
        "                    break\n",
        "\n",
        "            if best_node:\n",
        "                reasoning_parts.append(f\"\\n{law_counter}. 違反法規：{best_node['label']}\")\n",
        "                content = best_node['full_text'].replace('\\n', '')[:100] + \"...\"\n",
        "                reasoning_parts.append(f\"   法規內容：{content}\")\n",
        "                reasoning_parts.append(f\"   適用理由：本案事故情節顯示未符合此規定（圖譜中未建立直接路徑，基於法規內容推論）。\")\n",
        "            else:\n",
        "                reasoning_parts.append(f\"\\n{law_counter}. 違反法規：{law_str}\")\n",
        "                reasoning_parts.append(\"   (資料庫中未找到對應法規文本)\")\n",
        "\n",
        "        law_counter += 1\n",
        "\n",
        "    final_output = \"\\n\".join(reasoning_parts)\n",
        "\n",
        "    sft_data.append({\n",
        "        \"instruction\": \"請分析此職安事故之法律責任，並解釋法規適用理由。\",\n",
        "        \"input\": incident_text,\n",
        "        \"output\": final_output,\n",
        "        \"incident_id\": f\"GT_{gt_id}\"\n",
        "    })\n",
        "\n",
        "print(f\"Generated {len(sft_data)} SFT entries.\")\n",
        "\n",
        "# Write to file\n",
        "with open('sft_training_data_final.jsonl', 'w', encoding='utf-8') as f:\n",
        "    for entry in sft_data:\n",
        "        json.dump(entry, f, ensure_ascii=False)\n",
        "        f.write('\\n')\n",
        "\n",
        "print(\"File sft_training_data_final.jsonl saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgFdCA7AAbiJ",
        "outputId": "10dba9a6-854a-4a98-b8c4-7323d8137ad2"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapping Incidents...\n",
            "Mapped 369/381 incidents.\n",
            "Mapping Laws...\n",
            "Generated 369 SFT entries.\n",
            "File sft_training_data_final.jsonl saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "boxe_data = []\n",
        "\n",
        "# All Law IDs for negative sampling\n",
        "all_law_ids = [r['id'] for r in kg_regulations]\n",
        "\n",
        "for case in gt_data:\n",
        "    gt_id = case['id']\n",
        "    kg_incident_id = gt_to_kg_incident.get(gt_id)\n",
        "\n",
        "    if not kg_incident_id:\n",
        "        continue\n",
        "\n",
        "    gt_laws = case['ground_truth_coarse']\n",
        "    pos_law_ids = []\n",
        "\n",
        "    # Reuse mapping logic roughly\n",
        "    for law_str in gt_laws:\n",
        "        norm_gt = normalize_law(law_str)\n",
        "        # Find all matching KG nodes\n",
        "        for r in kg_regulations:\n",
        "            if norm_gt in r['norm'] or r['norm'] in norm_gt:\n",
        "                pos_law_ids.append(r['id'])\n",
        "\n",
        "    pos_law_ids = list(set(pos_law_ids))\n",
        "\n",
        "    if not pos_law_ids:\n",
        "        continue\n",
        "\n",
        "    # Negative Sampling\n",
        "    neg_law_ids = []\n",
        "    while len(neg_law_ids) < len(pos_law_ids):\n",
        "        cand = random.choice(all_law_ids)\n",
        "        if cand not in pos_law_ids and cand not in neg_law_ids:\n",
        "            neg_law_ids.append(cand)\n",
        "\n",
        "    boxe_data.append({\n",
        "        \"incident_id\": kg_incident_id,\n",
        "        \"incident_text\": case['description'],\n",
        "        \"positive_law_ids\": pos_law_ids,\n",
        "        \"negative_law_ids\": neg_law_ids,\n",
        "        \"ground_truth_text\": gt_laws\n",
        "    })\n",
        "\n",
        "print(f\"Generated {len(boxe_data)} BoxE validation entries.\")\n",
        "\n",
        "with open('boxe_validation_set_clean.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(boxe_data, f, ensure_ascii=False, indent=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWwadl6eAhFL",
        "outputId": "521ff92b-b523-4642-94a3-d3cc9ab7e34b"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 368 BoxE validation entries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Peek at the SFT file\n",
        "with open('sft_training_data_final.jsonl', 'r') as f:\n",
        "    line = f.readline()\n",
        "    print(json.loads(line)['output'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXIKMLeRAq0Q",
        "outputId": "dc0e106b-c0d6-4c0d-fe23-24cf2b6230b8"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "事故原因分析：\n",
            "104 年9 月3 日約10 時許，罹災者賴○昌與彭○德、許○福、鄭○龍等4\n",
            "人於大肚區遊園路○段○巷○弄○號對面之屋頂進行頂棚違建拆除作業，\n",
            "約自10 時20 分許，罹災者賴○昌為收拾乙炔焊接工具，故由靠遊園路○\n",
            "段○巷○弄北向往左數第2 塊頂棚南側第8 個採光罩旁往北方向第12 個\n",
            "採光罩移動，因安全母索過短無法有效使用，便將安全帶脫鉤，且屋頂頂\n",
            "棚上未設置適當強度且寬度30 公分以上之踏板，下方亦未裝設堅固格柵\n",
            "或安全網，罹災者賴○昌不慎踏穿第12 個採光罩自高度約22 公尺墜落至\n",
            "地面，因周身挫傷併多發性骨折、器官損傷致休克，經送醫院急救無效，\n",
            "於104 年9 月4 日15 時3 分宣告不治死亡。\n",
            "\n",
            "法規適用性分析：\n",
            "\n",
            "1. 違反法規：勞工健康保護規則 第10條\n",
            "   推論路徑：本案顯示「勞工未架設適當強度且寬度30公分以上之踏板」。\n",
            "   依據圖譜分析，此行為直接違反了 勞工健康保護規則 第10條。\n",
            "\n",
            "2. 違反法規：職業安全衛生教育訓練規則 第16條第1項\n",
            "   推論路徑：本案顯示「勞工未架設適當強度且寬度30公分以上之踏板」。\n",
            "   依據圖譜分析，此行為直接違反了 職業安全衛生教育訓練規則 第16條第1項。\n",
            "\n",
            "3. 違反法規：職業安全衛生法 第20條第1項\n",
            "   推論路徑：本案顯示「勞工未架設適當強度且寬度30公分以上之踏板」。\n",
            "   依據圖譜分析，此行為直接違反了 職業安全衛生法 第20條第1項。\n",
            "\n",
            "4. 違反法規：職業安全衛生法 第23條第1項\n",
            "   推論路徑：本案顯示「勞工未架設適當強度且寬度30公分以上之踏板」。\n",
            "   依據圖譜分析，此行為直接違反了 職業安全衛生法 第23條第1項。\n",
            "\n",
            "5. 違反法規：職業安全衛生法 第32條第1項\n",
            "   推論路徑：本案顯示「勞工未架設適當強度且寬度30公分以上之踏板」。\n",
            "   依據圖譜分析，此行為直接違反了 職業安全衛生法 第32條第1項。\n",
            "\n",
            "6. 違反法規：職業安全衛生法 第34條第1項\n",
            "   推論路徑：本案顯示「勞工未架設適當強度且寬度30公分以上之踏板」。\n",
            "   依據圖譜分析，此行為直接違反了 職業安全衛生法 第34條第1項。\n",
            "\n",
            "7. 違反法規：職業安全衛生管理辦法 第12條之1\n",
            "   推論路徑：本案顯示「勞工未架設適當強度且寬度30公分以上之踏板」。\n",
            "   依據圖譜分析，此行為直接違反了 職業安全衛生管理辦法 第12條之1。\n",
            "\n",
            "8. 違反法規：職業安全衛生管理辦法 第79條\n",
            "   推論路徑：本案顯示「勞工未架設適當強度且寬度30公分以上之踏板」。\n",
            "   依據圖譜分析，此行為直接違反了 職業安全衛生管理辦法 第79條。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get sft_training_data_final.jsonl"
      ],
      "metadata": {
        "id": "PPMkb4gEAr84"
      },
      "execution_count": 62,
      "outputs": []
    }
  ]
}