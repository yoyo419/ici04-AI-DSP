import json
import re
import unicodedata
from collections import defaultdict
from difflib import get_close_matches

# ==========================================
# 1. é ˜åŸŸçŸ¥è­˜èˆ‡åˆ¥ååœ°åœ– (Domain Knowledge)
# ==========================================

LAW_NAME_MAP = {
    # æ—¢æœ‰æ˜ å°„
    "å‹å·¥å®‰å…¨è¡›ç”Ÿæ³•": "è·æ¥­å®‰å…¨è¡›ç”Ÿæ³•",
    "è·æ¥­å…¨è¡›ç”Ÿæ³•": "è·æ¥­å®‰å…¨è¡›ç”Ÿæ³•",
    "å‹å·¥å®‰å…¨è¡›ç”Ÿè¨­æ–½è¦å‰‡": "è·æ¥­å®‰å…¨è¡›ç”Ÿè¨­æ–½è¦å‰‡",
    "å‹å·¥å®‰å…¨è¡›ç”Ÿçµ„ç¹”ç®¡ç†åŠè‡ªå‹•æª¢æŸ¥è¾¦æ³•": "è·æ¥­å®‰å…¨è¡›ç”Ÿç®¡ç†è¾¦æ³•",
    "å‹å·¥å¥åº·ä¿è­·è¦å‰‡": "å‹å·¥å¥åº·ä¿è­·è¦å‰‡",
    "å‹å·¥è·æ¥­ç½å®³ä¿éšªåŠä¿è­·æ³•": "å‹å·¥è·æ¥­ç½å®³ä¿éšªåŠä¿è­·æ³•",
    "ç‡Ÿé€ å®‰å…¨è¡›ç”Ÿè¨­æ–½æ¨™æº–": "ç‡Ÿé€ å®‰å…¨è¡›ç”Ÿè¨­æ–½æ¨™æº–",
    "è·æ¥­å®‰å…¨è¡›ç”Ÿæ•™è‚²è¨“ç·´è¦å‰‡": "è·æ¥­å®‰å…¨è¡›ç”Ÿæ•™è‚²è¨“ç·´è¦å‰‡",
    "å°±æ¥­æœå‹™æ³•": "å°±æ¥­æœå‹™æ³•",
    # [V9 æ–°å¢] éŒ¯å­—ä¿®æ­£
    "è·æ¥­å®‰å…¨è¡›ç”Ÿè¨­ç½®è¦å‰‡": "è·æ¥­å®‰å…¨è¡›ç”Ÿè¨­æ–½è¦å‰‡",
    "å±‹å…§ç·šè·¯è£ç½®è¦å‰‡": "å±‹å…§ç·šè·¯è£ç½®è¦å‰‡", # ç¢ºä¿ä¸€è‡´
    "é›»æ¥­æ³•": "é›»æ¥­æ³•",
}

BLACKLIST_KEYWORDS = [
    "è£½é€ æ¥­", "é£Ÿå“", "ç²‰æ¢", "éºµæ¢", "åŠ å·¥", 
    "äº‹æ•…", "æ­»äº¡", "å—å‚·", "ç½¹ç½", "åŸå› ", "åˆ†æ"
]

# ==========================================
# 2. æ ¸å¿ƒå·¥å…·é›†
# ==========================================

def parse_chinese_number(cn_str):
    """ä¸­æ–‡æ•¸å­—è½‰é˜¿æ‹‰ä¼¯æ•¸å­— (ä¿æŒ V8 çš„ç©©å®šé‚è¼¯)"""
    if not cn_str: return ""
    if cn_str.isdigit(): return cn_str
        
    cn_map = {'â—‹': 0, 'ã€‡': 0, 'é›¶': 0, 'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 
              'äº”': 5, 'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9}
    
    if len(cn_str) == 1 and cn_str in cn_map:
        return str(cn_map[cn_str])

    total = 0
    tmp = 0
    for char in cn_str:
        if char in cn_map:
            tmp = cn_map[char]
        elif char == 'å':
            if tmp == 0: tmp = 1
            total += tmp * 10
            tmp = 0
        elif char == 'ç™¾':
            if tmp == 0: tmp = 1
            total += tmp * 100
            tmp = 0
    total += tmp
    return str(total)

def text_chinese_to_arabic(text):
    pattern = re.compile(r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾â—‹ã€‡é›¶]+')
    def replace_func(match):
        return parse_chinese_number(match.group(0))
    return pattern.sub(replace_func, text)

def advanced_normalize_v9(text):
    """
    æ¨™æº–åŒ– v9ï¼šå»é™¤å¼•è™Ÿ + è£œå…¨å¾Œç¶´
    """
    if not text: return ""
    
    # 1. NFKC æ­£è¦åŒ–
    text = unicodedata.normalize('NFKC', text)
    
    # 2. è½‰å°å¯«èˆ‡å»ç©ºç™½
    text = text.lower()
    text = re.sub(r'\s+', '', text)
    
    # 3. [V9] å»é™¤é–‹é ­çµå°¾çš„æ¨™é»ç¬¦è™Ÿ (è§£æ±º ã€Œè·æ¥­å®‰å…¨... çš„å•é¡Œ)
    text = text.strip("ã€Œã€ã€ã€\"' ")

    # 4. ç§»é™¤ä»£ç¢¼é›œè¨Š
    text = re.sub(r'\(\d+\)', '', text)

    # 5. ä¸­æ–‡æ•¸å­—è½‰é˜¿æ‹‰ä¼¯
    text = text_chinese_to_arabic(text)

    # 6. æ³•è¦åç¨±æ ¡æ­£
    for alias, standard in LAW_NAME_MAP.items():
        if text.startswith(alias):
            text = text.replace(alias, standard, 1)
            break

    # 7. è™•ç†ã€Œä¹‹ã€å­—è™Ÿ
    text = re.sub(r'æ¢ä¹‹(\d+)', r'-\1æ¢', text) 
    text = re.sub(r'(\d+)ä¹‹(\d+)', r'\1-\2', text)
    text = text.replace("_", "-")

    # 8. è£œ "ç¬¬" å­—
    def add_prefix(match):
        char = match.group(1)
        num_part = match.group(2)
        if char == "ç¬¬": return match.group(0)
        return f"{char}ç¬¬{num_part}æ¢"
    text = re.sub(r'([\u4e00-\u9fa5])([\d\-]+)æ¢', add_prefix, text)

    # 9. ç§»é™¤æ¬¾ç›®
    text = re.sub(r'ç¬¬\d+æ¬¾', '', text)
    text = re.sub(r'ç¬¬\d+ç›®', '', text)
    
    # 10. [V9] è™•ç†ä¸å®Œæ•´å¾Œç¶´ (e.g., "ç¬¬59æ¢ç¬¬2") -> è¦–ç‚º "ç¬¬2é …"
    # Regex: çµå°¾æ˜¯ "ç¬¬+æ•¸å­—"ï¼Œä¸”å¾Œé¢æ²’æœ‰ä»»ä½•å–®ä½
    text = re.sub(r'(ç¬¬\d+)$', r'\1é …', text)
    
    # 11. ç§»é™¤æ‹¬è™Ÿ
    text = re.sub(r'[\(ï¼ˆ].*?[\)ï¼‰]', '', text)

    return text

# ==========================================
# 3. ç´¢å¼•å»ºæ§‹ (å« Missing Law åµæ¸¬æº–å‚™)
# ==========================================

def extract_article_from_content(content):
    if not content: return None
    norm_content = advanced_normalize_v9(content[:20])
    match = re.search(r'ç¬¬([\d\-]+)æ¢', norm_content)
    if match: return match.group(1)
    return None

def build_diagnostic_index(legal_content_path):
    print("ğŸ—ï¸ æ­£åœ¨å»ºæ§‹ v9 è¨ºæ–·å‹ç´¢å¼•...")
    
    with open(legal_content_path, 'r', encoding='utf-8') as f:
        legal_data = json.load(f)

    index_full = {}    
    index_article = {} 
    existing_laws = set() # [V9] ç”¨ä¾†è¨˜éŒ„è³‡æ–™åº«è£¡åˆ°åº•æœ‰å“ªäº›æ³•
    
    article_aggregator = defaultdict(list)
    article_meta = {}

    for entry in legal_data:
        raw_law = str(entry.get('law_name', ''))
        
        # æ­£è¦åŒ–æ³•è¦åç¨±ä¸¦å­˜å…¥é›†åˆ
        norm_law_name = advanced_normalize_v9(raw_law)
        # ç§»é™¤å¯èƒ½çš„ "ç¬¬xæ¢" å¾Œç¶´ï¼Œåªç•™æ³•å
        norm_law_name = re.sub(r'ç¬¬[\d\-]+æ¢.*', '', norm_law_name)
        if norm_law_name:
            existing_laws.add(norm_law_name)

        raw_art = str(entry.get('article', '')) 
        raw_para = entry.get('paragraph', '')
        para_str = str(raw_para) if raw_para not in [0, "0", None, "None", ""] else ""

        # Content Sniffing
        sniffed_art = extract_article_from_content(entry.get('content', ''))
        
        target_arts = set()
        target_arts.add(raw_art)
        if sniffed_art and sniffed_art != raw_art:
            target_arts.add(sniffed_art)

        for art in target_arts:
            base_key = f"{raw_law}ç¬¬{art}æ¢"
            
            if para_str:
                full_key = f"{base_key}ç¬¬{para_str}é …"
                norm_full = advanced_normalize_v9(full_key)
                index_full[norm_full] = entry
                
                if para_str == "1":
                    norm_base = advanced_normalize_v9(base_key)
                    if norm_base not in index_full:
                        index_full[norm_base] = entry
            else:
                norm_base = advanced_normalize_v9(base_key)
                index_full[norm_base] = entry

            for k in entry.get('match_keys', []):
                index_full[advanced_normalize_v9(k)] = entry

            if raw_law and art:
                norm_art_key = advanced_normalize_v9(base_key)
                prefix = f"[ç¬¬{para_str}é …] " if para_str else ""
                article_aggregator[norm_art_key].append(prefix + entry.get('content', ''))
                
                if norm_art_key not in article_meta:
                    node = entry.copy()
                    node['paragraph'] = "AGGREGATED"
                    node['node_id'] = f"{norm_art_key}_AGGREGATED"
                    article_meta[norm_art_key] = node

    for key, contents in article_aggregator.items():
        node = article_meta[key]
        node['content'] = "\n".join(contents)
        node['is_aggregated'] = True
        index_article[key] = node

    print(f"ğŸ“š è³‡æ–™åº«æ”¶éŒ„æ³•è¦æ•¸: {len(existing_laws)}")
    return index_full, index_article, existing_laws

# ==========================================
# 4. ä¸»åŸ·è¡Œæµç¨‹
# ==========================================

def execute_mapping_v9(kg_file, legal_content_file, output_file):
    index_full, index_article, existing_laws = build_diagnostic_index(legal_content_file)
    all_index_keys = list(index_full.keys()) + list(index_article.keys())
    
    with open(kg_file, 'r', encoding='utf-8') as f:
        kg = json.load(f)
        
    mapped_count = 0
    total_reg = 0
    
    # éŒ¯èª¤åˆ†é¡çµ±è¨ˆ
    missing_law_logs = defaultdict(int) # æ³•è¦ä¸å­˜åœ¨
    missing_article_logs = []           # æ³•è¦å­˜åœ¨ä½†æ¢æ–‡å°ä¸ä¸Š
    
    nodes = kg.get('nodes', [])
    
    print("ğŸš€ é–‹å§‹ V9 Mapping (æœ€çµ‚è¨ºæ–·ç‰ˆ)...")

    for node in nodes:
        label = str(node.get('label', '')).strip()
        norm_label = advanced_normalize_v9(label)
        
        # Filter
        is_valid = True
        for bad in BLACKLIST_KEYWORDS:
            if bad in label: is_valid = False
        if "ç¬¬" not in norm_label or "æ¢" not in norm_label: is_valid = False
        if len(label) > 60: is_valid = False
        
        if not is_valid: continue

        total_reg += 1
        target_node = None
        match_method = "unknown"

        # ç­–ç•¥ 1: Exact Match
        if norm_label in index_full:
            target_node = index_full[norm_label]
            match_method = "exact_v9"
            
        # ç­–ç•¥ 2: Article Rollup
        if not target_node:
            match = re.match(r'(.*?ç¬¬[\d\-]+æ¢)', norm_label)
            if match:
                rollup_key = match.group(1)
                if rollup_key in index_article:
                    target_node = index_article[rollup_key]
                    match_method = "rollup_v9"
                    
        # ç­–ç•¥ 3: Fuzzy Match
        if not target_node:
            law_match = re.match(r'(.*?)ç¬¬', norm_label)
            if law_match:
                current_law = law_match.group(1)
                candidate_keys = [k for k in all_index_keys if k.startswith(current_law)]
                matches = get_close_matches(norm_label, candidate_keys, n=1, cutoff=0.85)
                if matches:
                    best_match = matches[0]
                    target_node = index_full.get(best_match) or index_article.get(best_match)
                    match_method = f"fuzzy_v9 ({best_match})"

        if target_node:
            node['legal_ref_id'] = target_node.get('node_id')
            node['full_text'] = target_node.get('content')
            node['mapping_method'] = match_method
            node['normalized_label'] = norm_label
            mapped_count += 1
        else:
            # --- V9 è¨ºæ–·é‚è¼¯ ---
            # å˜—è©¦æå–æ³•è¦åç¨±
            match_law = re.match(r'(.*?)ç¬¬', norm_label)
            if match_law:
                law_name = match_law.group(1)
                # æª¢æŸ¥è©²æ³•è¦æ˜¯å¦å­˜åœ¨æ–¼ index ä¸­
                # æˆ‘å€‘ç”¨ fuzzy check ç¢ºä¿ä¸æ˜¯å› ç‚ºå°éŒ¯å­— (e.g. è·å®‰æ³• vs è·æ¥­å®‰å…¨è¡›ç”Ÿæ³• å·²ç¶“åœ¨ map è™•ç†éï¼Œé€™è£¡æ¯”å° normalized name)
                
                # æª¢æŸ¥ existing_laws è£¡æœ‰æ²’æœ‰é€™å€‹æ³•
                # é€™è£¡åšä¸€å€‹ç°¡å–®çš„ substring check æˆ– exact check
                is_law_exist = False
                for exist_law in existing_laws:
                    if law_name in exist_law or exist_law in law_name:
                        is_law_exist = True
                        break
                
                if not is_law_exist:
                    missing_law_logs[law_name] += 1
                else:
                    missing_article_logs.append(f"{label} (Law Found, Article Missing)")
            else:
                 missing_article_logs.append(f"{label} (Parse Error)")

    print(f"ğŸ“Š V9 æœ€çµ‚çµ±è¨ˆçµæœ:")
    print(f"    - æœ‰æ•ˆæ³•è¦ç¯€é»: {total_reg}")
    print(f"    - æˆåŠŸåŒ¹é…: {mapped_count}")
    print(f"    - æˆåŠŸç‡: {mapped_count/total_reg*100:.2f}%")
    
    print("\nğŸ” æœªåŒ¹é…åŸå› è¨ºæ–·:")
    if missing_law_logs:
        print(f"    ğŸ”´ [åš´é‡] è³‡æ–™åº«å®Œå…¨ç¼ºå¤±ä»¥ä¸‹æ³•è¦ (è«‹è£œå…… legal_content.json):")
        for law, count in missing_law_logs.items():
            print(f"       - {law}: {count} å€‹ç¯€é»å—å½±éŸ¿")
            
    if missing_article_logs:
        print(f"    ğŸŸ  [è­¦å‘Š] æ³•è¦å­˜åœ¨ä½†æ¢æ–‡åŒ¹é…å¤±æ•— (å‰ 15 å€‹):")
        for log in missing_article_logs[:15]:
            print(f"       - {log}")

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(kg, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    execute_mapping_v9(
        'knowledge_graph_connected.json', 
        'legal_content.json', 
        'knowledge_graph_final.json'
    )