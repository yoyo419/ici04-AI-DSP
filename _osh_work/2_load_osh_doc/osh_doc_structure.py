# Legal Reasoning Project, NCCU (2025)
# osh_doc_structure.py: Process occupational safety incident documenting PDFs into structured JSON

# Note: This code cannot perfectly extract all incidents due to the variability in document formatting.
# However, it ensures that each extracted incident contains almost complete elements.
# Those not well extracted will be manually revised in subsequent updates.
# ç½å®³é¡å‹è·Ÿåª’ä»‹ç‰©åˆ†é¡çš„åƒè€ƒè³‡æ–™ï¼šhttps://mse.ntust.edu.tw/var/file/19/1019/img/790/850509961.pdf

import os
import re
import json
import fitz  # PyMuPDF
from typing import Dict, List, Optional
from openai import OpenAI
from pathlib import Path

class IncidentPDFProcessor:
    def __init__(self, api_key: str):
        """
        åˆå§‹åŒ–è™•ç†å™¨
        
        Args:
            api_key: OpenAI APIå¯†é‘°
        """
        self.client = OpenAI(api_key=api_key)
        
        # å¾æ–‡ä»¶è¼‰å…¥åˆ†é¡å®šç¾©
        self.incident_types = self._load_incident_types()
        self.medium_types = self._load_medium_types()
        
    def _load_incident_types(self) -> Dict:
        """è¼‰å…¥ç½å®³é¡å‹åˆ†é¡"""
        return {
            "1": "å¢œè½, æ»¾è½",
            "2": "è·Œå€’",
            "3": "è¡æ’",
            "4": "ç‰©é«”é£›è½",
            "5": "ç‰©é«”å€’å¡Œ, å´©å¡Œ",
            "6": "è¢«æ’",
            "7": "è¢«å¤¾, è¢«æ²",
            "8": "è¢«åˆ‡, å‰², æ“¦å‚·",
            "9": "è¸©è¸",
            "10": "æººæ–ƒ",
            "11": "èˆ‡é«˜æº«, ä½æº«æ¥è§¸",
            "12": "èˆ‡æœ‰å®³ç‰©ç­‰ä¹‹æ¥è§¸",
            "13": "æ„Ÿé›»",
            "14": "çˆ†ç‚¸",
            "15": "ç‰©é«”ç ´è£‚",
            "16": "ç«ç½",
            "17": "ä¸ç•¶å‹•ä½œ",
            "18": "å…¶ä»–",
            "19": "ç„¡æ³•æ­¸é¡è€…",
            "21": "å…¬è·¯äº¤é€šäº‹æ•…",
            "22": "éµè·¯äº¤é€šäº‹æ•…",
            "23": "èˆ¹èˆ¶, èˆªç©ºç­‰äº¤é€šäº‹æ•…",
            "29": "å…¶ä»–äº¤é€šäº‹æ•…"
        }
    
    def _load_medium_types(self) -> Dict:
        """è¼‰å…¥åª’ä»‹ç‰©åˆ†é¡"""
        return {
            "general": {
                "1": "å‹•åŠ›æ©Ÿæ¢°",
                "2": "è£å¸é‹æ¬æ©Ÿæ¢°",
                "3": "å…¶ä»–è¨­å‚™",
                "4": "ç‡Ÿå»ºç‰©åŠæ–½å·¥è¨­å‚™",
                "5": "ç‰©è³ªææ–™",
                "6": "è²¨ç‰©",
                "7": "ç’°å¢ƒ",
                "9": "å…¶ä»–é¡"
            },
            "normal": {
                "11": "åŸå‹•æ©Ÿ", "12": "å‹•åŠ›å‚³å°è£ç½®", "13": "æœ¨æåŠ å·¥ç”¨æ©Ÿæ¢°",
                "14": "ç‡Ÿé€ ç”¨æ©Ÿæ¢°", "15": "ä¸€èˆ¬å‹•åŠ›æ©Ÿæ¢°", "21": "èµ·é‡æ©Ÿæ¢°",
                "22": "å‹•åŠ›é‹æ¬æ©Ÿæ¢°", "23": "äº¤é€šå·¥å…·", "31": "å£“åŠ›å®¹å™¨é¡",
                "32": "åŒ–å­¸è¨­å‚™", "33": "ç†”æ¥è¨­å‚™", "34": "çˆçª¯ç­‰",
                "35": "é›»æ°£è¨­å‚™", "36": "äººåŠ›æ©Ÿæ¢°å·¥å…·", "37": "ç”¨å…·",
                "39": "å…¶ä»–è¨­å‚™", "41": "ç‡Ÿå»ºç‰©åŠæ–½å·¥è¨­å‚™", "51": "å±éšªç‰©, æœ‰å®³ç‰©",
                "52": "ææ–™", "61": "é‹æ¬ç‰©é«”", "71": "ç’°å¢ƒ",
                "91": "å…¶ä»–åª’ä»‹ç‰©", "92": "ç„¡åª’ä»‹ç‰©", "99": "ä¸èƒ½åˆ†é¡"
            },
            "specific": {
                "111": "åŸå‹•æ©Ÿ", "121": "å‚³å‹•è»¸", "122": "å‚³å‹•è¼ª", "123": "é½’è¼ª",
                "129": "å…¶ä»–", "131": "åœ“é‹¸", "132": "å¸¶é‹¸", "133": "é‰‹é¢é‹¸",
                "139": "å…¶ä»–", "141": "ç‰½å¼•æ©Ÿé¡è¨­å‚™", "142": "å‹•åŠ›éŸé¡è¨­å‚™",
                "143": "æ‰“æ¨æ©Ÿ, æ‹”æ¨æ©Ÿ", "149": "å…¶ä»–", "151": "è»ŠåºŠ",
                "152": "é‘½åºŠ", "153": "ç ”ç£¨åºŠ", "154": "æ²–åºŠ, å‰ªåºŠ",
                "155": "é›å£“éš", "156": "é›¢å¿ƒæ©Ÿ", "157": "æ··åˆæ©Ÿ, ç²‰ç¢æ©Ÿ",
                "158": "è¼¥ç­’æ©Ÿ", "159": "å…¶ä»–", "211": "èµ·é‡æ©Ÿ",
                "212": "ç§»å‹•å¼èµ·é‡æ©Ÿ", "213": "äººå­—è‡‚èµ·é‡æ©Ÿ", "214": "å‡é™æ©Ÿ, æå‡æ©Ÿ",
                "215": "èˆ¹èˆ¶è£å¸è£ç½®", "216": "åŠç± ", "217": "æ©Ÿæ¢°é‹æã€ç´¢é“æ©Ÿæ¢°ã€é›†æè£ç½®",
                "218": "å›ºå®šå¼èµ·é‡æ©Ÿ", "219": "å…¶ä»–", "221": "å¡è»Š",
                "222": "å †é«˜æ©Ÿ", "223": "äº‹æ¥­å…§, è»Œé“è¨­", "224": "è¼¸é€å¸¶",
                "229": "å…¶ä»–", "231": "æ±½è»Š, å…¬å…±æ±½è»Š", "232": "ç«è»Š",
                "233": "å…¶ä»–", "311": "é‹çˆ", "312": "å£“åŠ›å®¹å™¨",
                "319": "å…¶ä»–", "321": "åŒ–å­¸è¨­å‚™", "331": "æ°£é«”ç†”æ¥",
                "332": "é›»å¼§ç†”æ¥", "339": "å…¶ä»–", "341": "çˆçª¯ç­‰",
                "351": "è¼¸é…é›»ç·šè·¯", "352": "é›»åŠ›è¨­å‚™", "353": "å…¶ä»–",
                "361": "äººåŠ›èµ·é‡æ©Ÿ", "362": "äººåŠ›é‹æ¬æ©Ÿ", "363": "äººåŠ›æ©Ÿæ¢°",
                "364": "æ‰‹å·¥å…·", "371": "æ¢¯å­ç­‰", "372": "åŠæ›é‰¤å…·",
                "379": "å…¶ä»–", "391": "å…¶ä»–è¨­å‚™", "411": "æ–½å·¥æ¶",
                "412": "æ”¯æ’æ¶", "413": "æ¨“æ¢¯, æ¢¯é“", "414": "é–‹å£éƒ¨ä»½",
                "415": "å±‹é ‚, å±‹æ¶, æ¨‘", "416": "å·¥ä½œå°, è¸æ¿", "417": "é€šè·¯",
                "418": "ç‡Ÿå»ºç‰©", "419": "å…¶ä»–", "511": "çˆ†ç‚¸æ€§ç‰©è³ª",
                "512": "å¼•ç«æ€§ç‰©è³ª", "513": "å¯ç‡ƒæ€§æ°£é«”", "514": "æœ‰å®³ç‰©",
                "515": "è¼»å°„ç·š", "519": "å…¶ä»–", "521": "é‡‘å±¬ææ–™",
                "522": "æœ¨æ, ç«¹æ", "523": "çŸ³é ­, ç ‚, å°çŸ³å­", "529": "å…¶ä»–",
                "611": "å·²åŒ…è£è²¨ç‰©", "612": "æœªåŒ…è£æ©Ÿæ¢°", "711": "åœŸç ‚, å²©çŸ³",
                "712": "ç«‹æœ¨", "713": "æ°´", "714": "ç‰¹æ®Šç’°å¢ƒ",
                "715": "é«˜ä½æº«ç’°å¢ƒ", "719": "å…¶ä»–", "911": "å…¶ä»–åª’ä»‹ç‰©",
                "999": "ä¸èƒ½åˆ†é¡"
            }
        }
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """
        å¾PDFæå–æ–‡å­—å…§å®¹
        
        Args:
            pdf_path: PDFæª”æ¡ˆè·¯å¾‘
            
        Returns:
            æå–çš„æ–‡å­—å…§å®¹
        """
        doc = fitz.open(pdf_path)
        full_text = ""
        
        for page_num in range(len(doc)):
            page = doc[page_num]
            full_text += page.get_text()
        
        doc.close()
        return full_text
    
    def extract_sections(self, text: str) -> List[Dict[str, str]]:
        """
        å¾æ–‡å­—ä¸­æå–å„å€‹äº‹ä»¶æ®µè½
        ä½¿ç”¨å¤šé‡ç­–ç•¥ç¢ºä¿æœ€å¤§åŒ–äº‹ä»¶æå–ç‡
        """
        incidents = []
        
        print("  ç­–ç•¥ 1ï¼šä½¿ç”¨æ¨™æº–æ ¼å¼åˆ†å‰²ï¼ˆä¸€ã€è¡Œæ¥­ç¨®é¡ï¼‰")
        # ç­–ç•¥1ï¼šæ¨™æº–çš„ã€Œä¸€ã€è¡Œæ¥­ç¨®é¡ã€åˆ†å‰²
        pattern1 = r'(?:ä¸€ã€|1ã€|ä¸€ï¼Œ)\s*è¡Œæ¥­[ç¨®ç±»]é¡[ï¼š:]\s*([^\n]+).*?(?=(?:ä¸€ã€|1ã€|ä¸€ï¼Œ)\s*è¡Œæ¥­|$)'
        matches1 = list(re.finditer(pattern1, text, re.DOTALL))
        print(f"    æ‰¾åˆ° {len(matches1)} å€‹äº‹ä»¶")
        
        if len(matches1) >= 5:  # å¦‚æœæ‰¾åˆ°è¶³å¤ å¤šçš„äº‹ä»¶ï¼Œä½¿ç”¨é€™å€‹ç­–ç•¥
            for i, match in enumerate(matches1, 1):
                incident_text = match.group(0)
                incident_data = self._parse_incident_text(incident_text)
                if incident_data and len(incident_data) >= 3:
                    incidents.append(incident_data)
            return incidents
        
        print("  ç­–ç•¥ 2ï¼šä½¿ç”¨å¯¬é¬†æ ¼å¼åˆ†å‰²ï¼ˆåŒ…å«è®Šé«”ï¼‰")
        # ç­–ç•¥2ï¼šæ›´å¯¬é¬†çš„åˆ†å‰²ï¼ŒåŒ…å«å„ç¨®å¯èƒ½çš„æ ¼å¼è®Šé«”
        pattern2 = r'ä¸€[ã€ï¼,:ï¼š]\s*è¡Œæ¥­[ç¨®ç±»]é¡[ï¼š:ã€ï¼,\s]*[^\n]+.*?(?=ä¸€[ã€ï¼,:ï¼š]\s*è¡Œæ¥­|$)'
        matches2 = list(re.finditer(pattern2, text, re.DOTALL | re.IGNORECASE))
        print(f"    æ‰¾åˆ° {len(matches2)} å€‹äº‹ä»¶")
        
        if len(matches2) >= 5:
            for i, match in enumerate(matches2, 1):
                incident_text = match.group(0)
                incident_data = self._parse_incident_text(incident_text)
                if incident_data and len(incident_data) >= 3:
                    incidents.append(incident_data)
            return incidents
        
        print("  ç­–ç•¥ 3ï¼šåŸºæ–¼ç« ç¯€æ¨™é¡Œåˆ†å‰²")
        # ç­–ç•¥3ï¼šå°‹æ‰¾åŒ…å«ã€Œå¾äº‹...ä½œæ¥­ã€çš„æ¨™é¡Œä½œç‚ºåˆ†ç•Œé»
        pattern3 = r'å¾äº‹.{2,50}ä½œæ¥­.*?ç½å®³.*?(?=å¾äº‹.{2,50}ä½œæ¥­|$)'
        matches3 = list(re.finditer(pattern3, text, re.DOTALL))
        print(f"    æ‰¾åˆ° {len(matches3)} å€‹æ½›åœ¨äº‹ä»¶å€å¡Š")

        # ç”¨æ–¼ç­–ç•¥å…§å»é‡
        temp_signatures = set()

        for i, match in enumerate(matches3, 1):
            # å‘å‰æ“´å±•ï¼Œå°‹æ‰¾ã€Œä¸€ã€è¡Œæ¥­ç¨®é¡ã€
            start_pos = max(0, match.start() - 500)
            extended_text = text[start_pos:match.end()]
            
            # æª¢æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„æ¬„ä½
            if 'è¡Œæ¥­' in extended_text and 'ç½å®³' in extended_text:
                incident_data = self._parse_incident_text(extended_text)
                if incident_data and len(incident_data) >= 3:
                    # ç­–ç•¥å…§å»é‡
                    temp_sig = incident_data.get('description', '')[:100]
                    temp_sig = re.sub(r'[\sã€ã€‚ï¼Œ]', '', temp_sig)
                    if temp_sig not in temp_signatures:
                        temp_signatures.add(temp_sig)
                        incidents.append(incident_data)

        if len(incidents) >= 5:
            return incidents
        
        print("  ç­–ç•¥ 4ï¼šå›ºå®šé•·åº¦åˆ†å‰²ï¼ˆæœ€å¾Œæ‰‹æ®µï¼‰")
        # ç­–ç•¥4ï¼šå¦‚æœå‰é¢éƒ½å¤±æ•—ï¼Œä½¿ç”¨å›ºå®šé•·åº¦åˆ†å‰²
        # å¹³å‡æ¯å€‹äº‹ä»¶ç´„ 2000-3000 å­—
        chunk_size = 2500
        overlap = 500
        
        for i in range(0, len(text), chunk_size - overlap):
            chunk = text[i:i + chunk_size]
            if 'è¡Œæ¥­' in chunk:  # ç¢ºä¿åŒ…å«åŸºæœ¬è³‡è¨Š
                incident_data = self._parse_incident_text(chunk)
                if incident_data and len(incident_data) >= 3:
                    # æ›´åš´æ ¼çš„å»é‡æª¢æŸ¥
                    new_sig = re.sub(r'[\sã€ã€‚ï¼Œ]', '', 
                                incident_data.get('industry', '')[:30] + 
                                incident_data.get('description', '')[:80])
                    
                    # æª¢æŸ¥æ˜¯å¦èˆ‡å·²æœ‰äº‹ä»¶é‡è¤‡
                    is_duplicate = False
                    for existing in incidents:
                        exist_sig = re.sub(r'[\sã€ã€‚ï¼Œ]', '', 
                                        existing.get('industry', '')[:30] + 
                                        existing.get('description', '')[:80])
                        if new_sig and exist_sig and new_sig == exist_sig:
                            is_duplicate = True
                            break
                    
                    if not is_duplicate:
                        incidents.append(incident_data)
        
        # å»é‡ï¼šæ ¹æ“šé—œéµæ¬„ä½åˆ¤æ–·æ˜¯å¦ç‚ºé‡è¤‡äº‹ä»¶
        print(f"  å»é‡å‰æ‰¾åˆ° {len(incidents)} å€‹äº‹ä»¶")
        unique_incidents = []
        seen_signatures = set()

        for incident in incidents:
            # å»ºç«‹äº‹ä»¶çš„å”¯ä¸€ç°½åï¼ˆä½¿ç”¨å‰50å­—çš„é—œéµæ¬„ä½ï¼‰
            signature_parts = [
                incident.get('industry', '')[:50],
                incident.get('incident', '')[:30],
                incident.get('description', '')[:100]
            ]
            signature = '|'.join(signature_parts).lower().strip()
            
            # ç§»é™¤ç©ºç™½å’Œæ¨™é»ç¬¦è™Ÿï¼Œä½¿æ¯”å°æ›´æº–ç¢º
            signature = re.sub(r'[\sã€ã€‚ï¼Œï¼ï¼Ÿï¼›ï¼š]', '', signature)
            
            if signature and signature not in seen_signatures:
                seen_signatures.add(signature)
                unique_incidents.append(incident)
            else:
                if signature:
                    print(f"    âš ï¸  åµæ¸¬åˆ°é‡è¤‡äº‹ä»¶ï¼Œå·²è·³é")

        print(f"  å»é‡å¾Œå‰©é¤˜ {len(unique_incidents)} å€‹æœ‰æ•ˆäº‹ä»¶")
        return unique_incidents
    
    def _parse_incident_text(self, text: str) -> Dict[str, str]:
        """è§£æå–®å€‹äº‹ä»¶æ–‡å­—ï¼Œä½¿ç”¨è¶…ç´šå¯¬é¬†çš„åŒ¹é…è¦å‰‡"""
        data = {}
        
        # è¶…ç´šå¯¬é¬†çš„æ­£å‰‡è¡¨é”å¼æ¨¡å¼
        patterns = {
            'industry': [
                r'[ä¸€1ï¼‘][ã€ï¼,:ï¼šï¼Œã€‚]\s*è¡Œæ¥­[ç¨®ç±»]é¡[ï¼š:ã€ï¼,ï¼Œã€‚\s]*([^\n]{2,50})',
                r'è¡Œæ¥­[ç¨®ç±»]é¡[ï¼š:ã€ï¼,ï¼Œã€‚\s]*([^\n]{2,50})',
                r'ä¸€[^äºŒ]*?è¡Œæ¥­[^ï¼š:]*?[ï¼š:]([^\n]+)',
            ],
            'incident': [
                r'[äºŒ2ï¼’][ã€ï¼,:ï¼šï¼Œã€‚]\s*ç½å®³[é¡ç±»]å‹[ï¼š:ã€ï¼,ï¼Œã€‚\s]*([^\n]+)',
                r'ç½å®³[é¡ç±»]å‹[ï¼š:ã€ï¼,ï¼Œã€‚\s]*([^\n]+)',
                r'äºŒ[^ä¸‰]*?ç½å®³[^ï¼š:]*?[ï¼š:]([^\n]+)',
            ],
            'medium_type': [
                r'[ä¸‰3ï¼“][ã€ï¼,:ï¼šï¼Œã€‚]\s*åª’\s*ä»‹\s*ç‰©[ï¼š:ã€ï¼,ï¼Œã€‚\s]*([^\n]+)',
                r'åª’\s*ä»‹\s*ç‰©[ï¼š:ã€ï¼,ï¼Œã€‚\s]*([^\n]+)',
                r'ä¸‰[^å››äº”]*?åª’[^ï¼š:]{0,5}[ï¼š:]([^\n]+)',
            ],
            'description': [
                r'[äº”5ï¼•][ã€ï¼,:ï¼šï¼Œã€‚]\s*ç½å®³ç™¼ç”Ÿç¶“é[ï¼š:ã€ï¼,ï¼Œã€‚\s]*(.+?)(?=[å…­ä¸ƒå…«6ï¼—ï¼˜][ã€ï¼,:ï¼šï¼Œã€‚]|$)',
                r'ç½å®³ç™¼ç”Ÿç¶“é[ï¼š:ã€ï¼,ï¼Œã€‚\s]*(.+?)(?=ç½å®³[åŸå› é˜²]|å…­[ã€ï¼]|$)',
                r'äº”[^å…­ä¸ƒå…«]*?ç¶“é[^ï¼š:]{0,10}[ï¼š:](.+?)(?=å…­[ã€ï¼]|ç½å®³åŸå› |$)',
            ],
            'cause_analysis': [
                r'[å…­6ï¼–][ã€ï¼,:ï¼šï¼Œã€‚]\s*ç½å®³åŸå› åˆ†æ[ï¼š:ã€ï¼,ï¼Œã€‚\s]*(.+?)(?=[ä¸ƒå…«7ï¼˜][ã€ï¼,:ï¼šï¼Œã€‚]|$)',
                r'ç½å®³åŸå› åˆ†æ[ï¼š:ã€ï¼,ï¼Œã€‚\s]*(.+?)(?=ç½å®³é˜²|ä¸ƒ[ã€ï¼]|$)',
                r'å…­[^ä¸ƒå…«]*?åŸå› [^ï¼š:]{0,10}[ï¼š:](.+?)(?=ä¸ƒ[ã€ï¼]|ç½å®³é˜²|$)',
            ],
            'preventive_measures': [
                # åŒ¹é…åˆ°ã€Œå…«ã€ã€æˆ–æª”æ¡ˆçµå°¾ï¼Œä½†ä¸åŒ¹é…ã€Œç¾å ´ã€ï¼ˆå› ç‚ºå¯èƒ½åœ¨å°ç­–å…§å®¹ä¸­å‡ºç¾ï¼‰
                r'[ä¸ƒ7ï¼—][ã€ï¼,:ï¼šï¼Œã€‚]\s*ç½å®³é˜²[æ­¢]?å°ç­–[ï¼š:ã€ï¼,ï¼Œã€‚\s]*(.+?)(?=å…«[ã€ï¼,:ï¼šï¼Œã€‚]\s*(?:ç½å®³ç¤ºæ„åœ–|ç¾å ´ç¤ºæ„åœ–|ç…§ç‰‡)|$)',
                r'ç½å®³é˜²[æ­¢]?å°ç­–[ï¼š:ã€ï¼,ï¼Œã€‚\s]*(.+?)(?=å…«[ã€ï¼,:ï¼šï¼Œã€‚]\s*(?:ç½å®³ç¤ºæ„åœ–|ç¾å ´ç¤ºæ„åœ–|ç…§ç‰‡)|$)',
                r'ä¸ƒ[^å…«ä¹å]*?å°ç­–[^ï¼š:]{0,10}[ï¼š:](.+?)(?=å…«[ã€ï¼,:ï¼šï¼Œã€‚]\s*(?:ç½å®³ç¤ºæ„åœ–|ç¾å ´ç¤ºæ„åœ–|ç…§ç‰‡)|$)',
                # å‚™ç”¨ï¼šåŒ¹é…åˆ°æª”æ¡ˆçµå°¾
                r'[ä¸ƒ7ï¼—][ã€ï¼,:ï¼šï¼Œã€‚]\s*ç½å®³é˜²[æ­¢]?å°ç­–[ï¼š:ã€ï¼,ï¼Œã€‚\s]*(.+)',
            ]
        }
        
        # å°æ¯å€‹æ¬„ä½å˜—è©¦å¤šå€‹æ¨¡å¼
        for key, pattern_list in patterns.items():
            found = False
            for pattern in pattern_list:
                match = re.search(pattern, text, re.DOTALL)
                if match:
                    content = match.group(1).strip()
                    # æ¸…ç†å…§å®¹ï¼šç§»é™¤éå¤šçš„ç©ºç™½å’Œæ›è¡Œï¼ˆä½†ä¿ç•™åŸºæœ¬æ ¼å¼ï¼‰
                    content = re.sub(r'\n\s*\n\s*\n+', '\n\n', content)  # å¤šå€‹æ›è¡Œè®Šæˆå…©å€‹
                    content = re.sub(r'[ \t]+', ' ', content)  # å¤šå€‹ç©ºæ ¼è®Šæˆä¸€å€‹
                    
                    # å° preventive_measures ä¸åšé•·åº¦é™åˆ¶
                    if key != 'preventive_measures':
                        content = content[:5000]  # å…¶ä»–æ¬„ä½é™åˆ¶é•·åº¦
                    else:
                        content = content[:50000]  # preventive_measures å…è¨±æ›´é•·ï¼ˆç´„50KBï¼‰
                    
                    data[key] = content
                    found = True
                    break
            
            if not found and key not in ['medium_type']:  # medium_type å¯ä»¥æ²’æœ‰
                print(f"      âš  æœªæ‰¾åˆ°æ¬„ä½ '{key}'")
        
        # æª¢æŸ¥æ˜¯å¦è‡³å°‘æœ‰ 3 å€‹å¿…è¦æ¬„ä½
        required_fields = ['industry', 'incident', 'description']
        found_required = sum(1 for field in required_fields if field in data and data[field])
        
        if found_required >= 2:  # é™ä½é–€æª»ï¼šè‡³å°‘2å€‹å¿…è¦æ¬„ä½
            return data
        else:
            return None
    
    def extract_basic_fields(self, incident_text: str) -> Dict:
        """
        æå–åŸºæœ¬æ¬„ä½ï¼ˆä¸éœ€è¦AIçš„æ¬„ä½ï¼‰
        
        Args:
            incident_text: å–®ä¸€äº‹æ•…çš„æ–‡æœ¬
            
        Returns:
            åŒ…å«åŸºæœ¬æ¬„ä½çš„å­—å…¸
        """
        result = {}
        
        # æå–è¡Œæ¥­ç¨®é¡
        industry_match = re.search(r'ä¸€ã€è¡Œæ¥­ç¨®é¡[ï¼š:]\s*(.+?)(?=\n|äºŒã€)', incident_text)
        if industry_match:
            result['industry'] = industry_match.group(1).strip()
        
        # æå–ç½å®³é¡å‹
        incident_match = re.search(r'äºŒã€ç½å®³é¡å‹[ï¼š:]\s*(.+?)(?=\n|ä¸‰ã€)', incident_text)
        if incident_match:
            result['incident'] = incident_match.group(1).strip()
        
        # æå–åª’ä»‹ç‰©
        medium_match = re.search(r'ä¸‰ã€åª’ä»‹ç‰©[ï¼š:]\s*(.+?)(?=\n|å››ã€)', incident_text)
        if medium_match:
            result['medium_type'] = medium_match.group(1).strip()
        
        # æå–ç½å®³ç™¼ç”Ÿç¶“é
        desc_match = re.search(r'äº”ã€ç½å®³ç™¼ç”Ÿç¶“é[ï¼š:]\s*(.+?)(?=å…­ã€|$)', incident_text, re.DOTALL)
        if desc_match:
            result['description'] = desc_match.group(1).strip()
        
        # æå–ç½å®³åŸå› åˆ†æ
        cause_match = re.search(r'å…­ã€ç½å®³åŸå› åˆ†æ[ï¼š:]\s*(.+?)(?=ä¸ƒã€|$)', incident_text, re.DOTALL)
        if cause_match:
            result['cause_analysis'] = cause_match.group(1).strip()
        
        # æå–ç½å®³é˜²æ­¢å°ç­–
        prevent_match = re.search(r'ä¸ƒã€ç½å®³é˜²æ­¢å°ç­–[ï¼š:]\s*(.+?)(?=å…«ã€|$)', incident_text, re.DOTALL)
        if prevent_match:
            result['preventive_measures'] = prevent_match.group(1).strip()
        
        return result
    
    def classify_incident_type(self, incident: str) -> tuple:
        """
        ä½¿ç”¨OpenAI APIåˆ†é¡ç½å®³é¡å‹
        
        Args:
            incident: ç½å®³é¡å‹æ–‡å­—
            
        Returns:
            (ç½å®³é¡å‹, ç½å®³é¡å‹ID)
        """
        if not incident or incident.strip() == "":
            return "", ""
            
        prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹ç½å®³é¡å‹åˆ†é¡è¡¨ï¼Œåˆ¤æ–·ã€Œ{incident}ã€æœ€ç¬¦åˆå“ªä¸€å€‹é¡åˆ¥ã€‚

ç½å®³é¡å‹åˆ†é¡è¡¨ï¼š
{json.dumps(self.incident_types, ensure_ascii=False, indent=2)}

é‡è¦æŒ‡ç¤ºï¼š
1. ä»”ç´°æ¯”å°è¼¸å…¥çš„ç½å®³é¡å‹èˆ‡åˆ†é¡è¡¨ä¸­çš„æè¿°
2. é¸æ“‡æœ€ç›¸ç¬¦çš„é¡åˆ¥
3. åªå›å‚³JSONæ ¼å¼ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—
4. JSONæ ¼å¼ï¼š{{"type": "é¡å‹åç¨±", "id": "ç·¨è™Ÿ"}}

ç¯„ä¾‹ï¼š
è¼¸å…¥ï¼šå¢œè½
è¼¸å‡ºï¼š{{"type": "å¢œè½, æ»¾è½", "id": "1"}}"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯è·æ¥­å®‰å…¨å°ˆå®¶ï¼Œå°ˆé–€è² è²¬ç½å®³åˆ†é¡ã€‚è«‹åš´æ ¼æŒ‰ç…§æŒ‡ç¤ºå›å‚³JSONæ ¼å¼ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=100,
                temperature=0
            )
            
            content = response.choices[0].message.content.strip()
            # æ¸…ç†å¯èƒ½çš„ markdown æ ¼å¼
            content = content.replace('```json', '').replace('```', '').strip()
            
            result = json.loads(content)
            return result.get('type', ''), result.get('id', '')
        except Exception as e:
            print(f"åˆ†é¡ç½å®³é¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            print(f"è¼¸å…¥: {incident}")
            return "", ""
    
    def classify_medium_type(self, medium: str) -> tuple:
        """
        ä½¿ç”¨OpenAI APIåˆ†é¡åª’ä»‹ç‰©
        
        Args:
            medium: åª’ä»‹ç‰©æ–‡å­—
            
        Returns:
            (å¤§é¡åˆ¥, å¤§é¡åˆ¥ID, é¡åˆ¥, é¡åˆ¥ID, é …ç›®, é …ç›®ID)
        """
        if not medium or medium.strip() == "":
            return "", "", "", "", "", ""
            
        prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹åª’ä»‹ç‰©åˆ†é¡è¡¨ï¼Œåˆ¤æ–·ã€Œ{medium}ã€æœ€ç¬¦åˆå“ªä¸€å€‹é¡åˆ¥ã€‚

åª’ä»‹ç‰©å¤§é¡åˆ¥ï¼š
{json.dumps(self.medium_types['general'], ensure_ascii=False, indent=2)}

åª’ä»‹ç‰©é¡åˆ¥ï¼š
{json.dumps(self.medium_types['normal'], ensure_ascii=False, indent=2)}

åª’ä»‹ç‰©é …ç›®ï¼š
{json.dumps(self.medium_types['specific'], ensure_ascii=False, indent=2)}

é‡è¦æŒ‡ç¤ºï¼š
1. éœ€è¦æ‰¾å‡ºä¸‰å€‹å±¤ç´šçš„åˆ†é¡ï¼šå¤§é¡åˆ¥ã€é¡åˆ¥ã€é …ç›®
2. å¤§é¡åˆ¥IDæ˜¯å€‹ä½æ•¸ï¼ˆ1-9ï¼‰
3. é¡åˆ¥IDæ˜¯åä½æ•¸ï¼ˆ11-99ï¼‰
4. é …ç›®IDæ˜¯ç™¾ä½æ•¸ï¼ˆ111-999ï¼‰
5. åªå›å‚³JSONæ ¼å¼ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—

JSONæ ¼å¼ï¼š
{{
    "general": "å¤§é¡åˆ¥åç¨±",
    "general_id": "å¤§é¡åˆ¥ID",
    "normal": "é¡åˆ¥åç¨±",
    "normal_id": "é¡åˆ¥ID",
    "specific": "é …ç›®åç¨±",
    "specific_id": "é …ç›®ID"
}}

ç¯„ä¾‹ï¼š
è¼¸å…¥ï¼šå›ºå®šå¼èµ·é‡æ©Ÿ
è¼¸å‡ºï¼š
{{
    "general": "è£å¸é‹æ¬æ©Ÿæ¢°",
    "general_id": "2",
    "normal": "èµ·é‡æ©Ÿæ¢°",
    "normal_id": "21",
    "specific": "å›ºå®šå¼èµ·é‡æ©Ÿ",
    "specific_id": "218"
}}"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯è·æ¥­å®‰å…¨å°ˆå®¶ï¼Œå°ˆé–€è² è²¬åª’ä»‹ç‰©åˆ†é¡ã€‚è«‹åš´æ ¼æŒ‰ç…§æŒ‡ç¤ºå›å‚³JSONæ ¼å¼ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=200,
                temperature=0
            )
            
            content = response.choices[0].message.content.strip()
            # æ¸…ç†å¯èƒ½çš„ markdown æ ¼å¼
            content = content.replace('```json', '').replace('```', '').strip()
            
            result = json.loads(content)
            return (
                result.get('general', ''), result.get('general_id', ''),
                result.get('normal', ''), result.get('normal_id', ''),
                result.get('specific', ''), result.get('specific_id', '')
            )
        except Exception as e:
            print(f"åˆ†é¡åª’ä»‹ç‰©æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            print(f"è¼¸å…¥: {medium}")
            return "", "", "", "", "", ""
    
    def generate_description_summary(self, description: str) -> str:
        """
        ä½¿ç”¨OpenAI APIç”Ÿæˆç½å®³ç™¼ç”Ÿç¶“éæ‘˜è¦
        
        Args:
            description: ç½å®³ç™¼ç”Ÿç¶“éå®Œæ•´æè¿°
            
        Returns:
            æ‘˜è¦æ–‡å­—ï¼ˆ60-100å­—ï¼‰
        """
        if not description or description.strip() == "":
            return ""
            
        prompt = f"""è«‹å°‡ä»¥ä¸‹ç½å®³ç™¼ç”Ÿç¶“éæ¿ƒç¸®æˆ60-100å­—çš„æ‘˜è¦ã€‚

ç½å®³ç™¼ç”Ÿç¶“éï¼š
{description}

é‡è¦æŒ‡ç¤ºï¼š
1. æ‘˜è¦å¿…é ˆç°¡æ½”æ˜ç¢ºï¼ŒåŒ…å«é—œéµè³‡è¨Š
2. å­—æ•¸æ§åˆ¶åœ¨60-100å­—ä¹‹é–“
3. åªå›å‚³æ‘˜è¦æ–‡å­—ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–èªªæ˜
4. ä¸è¦ä½¿ç”¨å¼•è™Ÿæˆ–å…¶ä»–æ ¼å¼

ç¯„ä¾‹ï¼š
è¼¸å…¥ï¼šæ“šâ—‹â—‹æœ‰é™å…¬å¸æ‰€åƒ±å‹å·¥æ—â—‹â—‹ç¨±ï¼š103å¹´5æœˆ15æ—¥ç´„12æ™‚37åˆ†è¨±ï¼Œç§»å‹•å¼èµ·é‡æ©Ÿæ“ä½œæ‰‹é™³â—‹â—‹(ç½¹ç½è€…)æ“ä½œå±¥å¸¶ç§»å‹•å¼èµ·é‡æ©Ÿï¼Œå¾äº‹æ©‹å¢©å…¨å¥—ç®¡åŸºæ¨å·¥ç¨‹ä¹‹æŒ–æ˜ä½œæ¥­æ™‚ï¼Œçªç„¶è½åˆ°ç¢°ä¸€è²ï¼Œçœ‹åˆ°è©²èµ·é‡æ©Ÿä¹‹æ¡æ¶èˆ‡åŠè»Šå·¦é‚Šé€£çµè™•æ–·è£‚ï¼Œä¸”æ¡æ¶å‘å³å‚¾å£“åˆ°é§•é§›å®¤ï¼Œè‡´é§•é§›å®¤å…§èµ·é‡æ©Ÿæ“ä½œæ‰‹é™³â—‹â—‹ç•¶å ´æ­»äº¡ã€‚
è¼¸å‡ºï¼šé™³ç½¹ç½è€…æ“ä½œå±¥å¸¶ç§»å‹•å¼èµ·é‡æ©Ÿé€²è¡Œæ©‹å¢©å…¨å¥—ç®¡åŸºæ¨å·¥ç¨‹ä¹‹æŒ–æ˜ä½œæ¥­æ™‚ï¼Œæ¡æ¶èˆ‡åŠè»Šå·¦å´é€£çµè™•æ–·è£‚ï¼Œå°è‡´æ¡æ¶å‚¾å£“é§•é§›å®¤ï¼Œé™³ç•¶å ´æ­»äº¡ã€‚
é‡è¦é—œéµè³‡è¨Šï¼šå±¥å¸¶ç§»å‹•å¼èµ·é‡æ©Ÿ (åª’ä»‹ç‰©)ã€æ©‹å¢©å…¨å¥—ç®¡åŸºæ¨å·¥ç¨‹ (é‡è¦äº‹ä»¶ç´°ç¯€)ã€æ¡æ¶èˆ‡åŠè»Šå·¦å´é€£çµè™•æ–·è£‚ (é‡è¦äº‹ä»¶ç´°ç¯€)
"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯è·æ¥­å®‰å…¨æ–‡ä»¶æ’°å¯«å°ˆå®¶ï¼Œæ“…é•·æ¿ƒç¸®ç½å®³å ±å‘Šã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=150,
                temperature=0.3
            )
            
            summary = response.choices[0].message.content.strip()
            # ç§»é™¤å¯èƒ½çš„å¼•è™Ÿ
            summary = summary.strip('"\'')
            return summary
        except Exception as e:
            print(f"ç”Ÿæˆæ‘˜è¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return ""
    
    def generate_cause_summary(self, cause_analysis: str) -> str:
        """
        ä½¿ç”¨OpenAI APIç”Ÿæˆç½å®³åŸå› æ‘˜è¦
        
        Args:
            cause_analysis: ç½å®³åŸå› åˆ†æå®Œæ•´å…§å®¹
            
        Returns:
            çµæ§‹åŒ–çš„åŸå› æ‘˜è¦
        """
        if not cause_analysis or cause_analysis.strip() == "":
            return ""
            
        prompt = f"""è«‹åˆ†æä»¥ä¸‹ç½å®³åŸå› ï¼Œä¸¦ä»¥çµæ§‹åŒ–æ–¹å¼æ‘˜è¦ã€‚

ç½å®³åŸå› åˆ†æï¼š
{cause_analysis}

é‡è¦æŒ‡ç¤ºï¼š
1. å¿…é ˆæ˜ç¢ºæŒ‡å‡ºä¸»é«”ï¼ˆå‹å·¥æˆ–é›‡ä¸»ï¼‰
2. èªªæ˜å…·é«”è¡Œç‚ºï¼ˆæœªæ¶è¨­ã€æœªä½¿ç”¨ã€æœªç¦æ­¢ã€æœªè¾¦ç†ã€æœªè¨‚å®šç­‰ï¼‰
3. åŒ…å«ç›¸é—œè¨­å‚™ã€è¦å‰‡æˆ–æ´»å‹•
4. ç”¨ã€Œã€ã€åˆ†éš”å„é …åŸå› 
5. åªå›å‚³æ‘˜è¦æ–‡å­—ï¼Œä¸è¦æœ‰å…¶ä»–èªªæ˜
6. å¦‚æœåŸæ–‡æ²’æœ‰æ˜ç¢ºæåˆ°ä¸»é«”ï¼Œè«‹æ ¹æ“šè¡Œç‚ºé¡å‹è°æ˜åˆ¤æ–·

è¡Œç‚ºé¡å‹åˆ¤æ–·åŸå‰‡ï¼š
- æœªæ¶è¨­è¨­å‚™ã€æœªä½¿ç”¨é˜²è­·å…· â†’ é€šå¸¸æ˜¯å‹å·¥
- æœªç¦æ­¢å±éšªè¡Œç‚ºã€æœªè¾¦ç†è¨“ç·´ã€æœªè¨‚å®šè¦å‰‡ â†’ é€šå¸¸æ˜¯é›‡ä¸»

ç¯„ä¾‹æ ¼å¼ï¼š
ã€Œå‹å·¥æœªæ¶è¨­æ–½å·¥æ¶èˆ‡å·¥ä½œè‡ºã€å‹å·¥æœªä½¿ç”¨å®‰å…¨å¸¶èˆ‡å®‰å…¨å¸½ç­‰é˜²è­·å·¥å…·ã€é›‡ä¸»æœªç¦æ­¢å‹å·¥æ­è¼‰å †é«˜æ©Ÿé™¤ä¹˜åå¸­å¤–çš„ä½ç½®ã€é›‡ä¸»æœªè¾¦ç†å‹å·¥å®‰å…¨è¡›ç”Ÿæ•™è‚²è¨“ç·´ã€é›‡ä¸»æœªè¨‚å®šé©åˆä¹‹å®‰å…¨è¡›ç”Ÿå·¥ä½œå®ˆå‰‡ã€

è«‹ç›´æ¥å›å‚³æ‘˜è¦ï¼š"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯è·æ¥­å®‰å…¨åˆ†æå°ˆå®¶ï¼Œæ“…é•·è­˜åˆ¥ç½å®³æˆå› ä¸¦æ­¸ç´è²¬ä»»ä¸»é«”ã€‚è«‹åš´æ ¼æŒ‰ç…§æŒ‡ç¤ºæ ¼å¼è¼¸å‡ºã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=300,
                temperature=0.3
            )
            
            summary = response.choices[0].message.content.strip()
            # ç§»é™¤å¯èƒ½çš„å¼•è™Ÿ
            summary = summary.strip('"\'')
            return summary
        except Exception as e:
            print(f"ç”ŸæˆåŸå› æ‘˜è¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return ""
    
    def extract_regulations(self, preventive_measures: str) -> str:
        """
        ä½¿ç”¨OpenAI APIæå–æ³•è¦æ¢æ–‡
        
        Args:
            preventive_measures: ç½å®³é˜²æ­¢å°ç­–å®Œæ•´å…§å®¹
            
        Returns:
            ä»¥é€—è™Ÿåˆ†éš”çš„æ³•è¦æ¢æ–‡åˆ—è¡¨
        """
        if not preventive_measures or preventive_measures.strip() == "":
            return ""
            
        prompt = f"""è«‹å¾ä»¥ä¸‹ç½å®³é˜²æ­¢å°ç­–ä¸­æå–æ‰€æœ‰æ³•è¦æ¢æ–‡åç¨±ã€‚

ç½å®³é˜²æ­¢å°ç­–ï¼š
{preventive_measures}

é‡è¦æŒ‡ç¤ºï¼š
1. æå–æ‰€æœ‰å®Œæ•´çš„æ³•è¦æ¢æ–‡åç¨±ï¼ˆåŒ…å«æ³•å¾‹åç¨±å’Œæ¢æ–‡ç·¨è™Ÿï¼‰
2. ç”¨åŠå½¢é€—è™ŸåŠ ç©ºæ ¼ ", " åˆ†éš”å„æ³•è¦
3. ä¿æŒåŸæ–‡çš„æ³•è¦åç¨±æ ¼å¼
4. åªå›å‚³æ³•è¦åˆ—è¡¨ï¼Œä¸è¦æœ‰å…¶ä»–èªªæ˜æ–‡å­—
5. å¦‚æœæœ‰ã€Œæš¨ã€é€£æ¥å¤šå€‹æ³•æ¢ï¼Œè«‹ä¿æŒå®Œæ•´

ç¯„ä¾‹æ ¼å¼ï¼š
ã€Œå‹å·¥å®‰å…¨è¡›ç”Ÿæ³•ç¬¬25æ¢ç¬¬1é …, å‹å·¥å®‰å…¨è¡›ç”Ÿæ³•ç¬¬14æ¢ç¬¬2é …, å‹å·¥å®‰å…¨è¡›ç”Ÿçµ„ç¹”ç®¡ç†åŠè‡ªå‹•æª¢æŸ¥è¾¦æ³•ç¬¬12æ¢ä¹‹1ç¬¬2é …æš¨å‹å·¥å®‰å…¨è¡›ç”Ÿæ³•ç¬¬14æ¢ç¬¬3é …ã€

è«‹ç›´æ¥å›å‚³æ³•è¦åˆ—è¡¨ï¼š"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯æ³•è¦æ–‡ä»¶è™•ç†å°ˆå®¶ï¼Œæ“…é•·æå–å’Œæ•´ç†æ³•è¦æ¢æ–‡ã€‚è«‹åš´æ ¼æŒ‰ç…§æŒ‡ç¤ºæ ¼å¼è¼¸å‡ºã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=500,
                temperature=0
            )
            
            regulations = response.choices[0].message.content.strip()
            # ç§»é™¤å¯èƒ½çš„å¼•è™Ÿ
            regulations = regulations.strip('"\'')
            # ç§»é™¤æ‰€æœ‰ç©ºæ ¼ï¼ˆåŒ…å«å…¨å½¢å’ŒåŠå½¢ç©ºæ ¼ï¼‰
            regulations = regulations.replace(' ', '').replace('ã€€', '')
            return regulations
        except Exception as e:
            print(f"æå–æ³•è¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return ""
    
    def process_pdf(self, pdf_path: str, output_path: Optional[str] = None) -> List[Dict]:
        """
        è™•ç†å®Œæ•´çš„PDFæª”æ¡ˆ
        
        Args:
            pdf_path: PDFæª”æ¡ˆè·¯å¾‘
            output_path: è¼¸å‡ºJSONæª”æ¡ˆè·¯å¾‘ï¼ˆå¯é¸ï¼‰
            
        Returns:
            æ‰€æœ‰äº‹æ•…æ¡ˆä¾‹çš„åˆ—è¡¨
        """
        print(f"æ­£åœ¨è®€å–PDFæª”æ¡ˆ: {pdf_path}")
        text = self.extract_text_from_pdf(pdf_path)
        
        print("æ­£åœ¨åˆ†å‰²äº‹æ•…æ¡ˆä¾‹...")
        incidents = self.extract_sections(text)
        print(f"æ‰¾åˆ° {len(incidents)} å€‹äº‹æ•…æ¡ˆä¾‹")
        
        results = []
        processed_signatures = set()

        for i, incident_data in enumerate(incidents, 1):
            print(f"\nè™•ç†ç¬¬ {i}/{len(incidents)} å€‹äº‹æ•…æ¡ˆä¾‹...")
            
            # å»ºç«‹ç°½åæª¢æŸ¥æ˜¯å¦é‡è¤‡
            check_sig = (
                incident_data.get('industry', '')[:30] + '|' +
                incident_data.get('incident', '')[:20] + '|' +
                incident_data.get('description', '')[:80]
            )
            check_sig = re.sub(r'[\sã€ã€‚ï¼Œï¼ï¼Ÿï¼›ï¼š]', '', check_sig)
            
            if check_sig in processed_signatures:
                print(f"  âš ï¸  åµæ¸¬åˆ°é‡è¤‡äº‹ä»¶ï¼ˆæœ€çµ‚æª¢æŸ¥ï¼‰ï¼Œè·³éè™•ç†")
                continue
            
            processed_signatures.add(check_sig)
            
            print(f"  é–‹å§‹è™•ç†äº‹ä»¶ï¼š{incident_data.get('industry', 'æœªçŸ¥')[:20]}...")
            print(f"  å·²å–å¾—æ¬„ä½ï¼š{list(incident_data.keys())}")
            result = self.process_incident_data(incident_data)
            if result:
                results.append(result)
        
        # å„²å­˜çµæœ
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"\nçµæœå·²å„²å­˜è‡³: {output_path}")
        
        return results

    def process_folder(self, folder_path: str, output_folder: Optional[str] = None):
        """
        è™•ç†è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰ PDF æª”æ¡ˆ
        
        Args:
            folder_path: åŒ…å« PDF æª”æ¡ˆçš„è³‡æ–™å¤¾è·¯å¾‘
            output_folder: è¼¸å‡º JSON æª”æ¡ˆçš„è³‡æ–™å¤¾è·¯å¾‘ï¼ˆå¯é¸ï¼Œé è¨­ç‚ºèˆ‡ PDF ç›¸åŒä½ç½®ï¼‰
        """
        from pathlib import Path
        
        folder = Path(folder_path)
        
        if not folder.exists():
            print(f"âŒ éŒ¯èª¤ï¼šè³‡æ–™å¤¾ä¸å­˜åœ¨ - {folder_path}")
            return
        
        if not folder.is_dir():
            print(f"âŒ éŒ¯èª¤ï¼šè·¯å¾‘ä¸æ˜¯è³‡æ–™å¤¾ - {folder_path}")
            return
        
        # æ‰¾å‡ºæ‰€æœ‰ PDF æª”æ¡ˆ
        pdf_files = list(folder.glob("*.pdf"))
        
        if not pdf_files:
            print(f"âŒ åœ¨ {folder_path} ä¸­æ²’æœ‰æ‰¾åˆ° PDF æª”æ¡ˆ")
            return
        
        print(f"ğŸ“ æ‰¾åˆ° {len(pdf_files)} å€‹ PDF æª”æ¡ˆ")
        print("=" * 60)
        
        # è¨­å®šè¼¸å‡ºè³‡æ–™å¤¾
        if output_folder:
            output_dir = Path(output_folder)
            output_dir.mkdir(parents=True, exist_ok=True)
        else:
            output_dir = folder
        
        # çµ±è¨ˆè³‡è¨Š
        total_incidents = 0
        successful_files = 0
        failed_files = []
        
        # è™•ç†æ¯å€‹ PDF æª”æ¡ˆ
        for idx, pdf_file in enumerate(pdf_files, 1):
            print(f"\n{'='*60}")
            print(f"ğŸ“„ [{idx}/{len(pdf_files)}] è™•ç†æª”æ¡ˆ: {pdf_file.name}")
            print(f"{'='*60}")
            
            try:
                # ç”Ÿæˆè¼¸å‡ºæª”æ¡ˆåç¨±
                output_file = output_dir / f"{pdf_file.stem}_çµæ§‹åŒ–è³‡æ–™.json"
                
                # è™•ç†å–®ä¸€ PDF
                results = self.process_pdf(str(pdf_file), str(output_file))
                
                if results:
                    total_incidents += len(results)
                    successful_files += 1
                    print(f"âœ… æˆåŠŸè™•ç†ï¼š{pdf_file.name}")
                    print(f"   æå–äº‹ä»¶æ•¸ï¼š{len(results)} å€‹")
                    print(f"   è¼¸å‡ºæª”æ¡ˆï¼š{output_file.name}")
                else:
                    failed_files.append(pdf_file.name)
                    print(f"âš ï¸  æœªèƒ½æå–ä»»ä½•äº‹ä»¶ï¼š{pdf_file.name}")
            
            except Exception as e:
                failed_files.append(pdf_file.name)
                print(f"âŒ è™•ç†å¤±æ•—ï¼š{pdf_file.name}")
                print(f"   éŒ¯èª¤è¨Šæ¯ï¼š{str(e)}")
        
        # è¼¸å‡ºç¸½çµ
        print(f"\n{'='*60}")
        print(f"ğŸ“Š è™•ç†å®Œæˆçµ±è¨ˆ")
        print(f"{'='*60}")
        print(f"ç¸½æª”æ¡ˆæ•¸ï¼š{len(pdf_files)}")
        print(f"æˆåŠŸè™•ç†ï¼š{successful_files}")
        print(f"å¤±æ•—æª”æ¡ˆï¼š{len(failed_files)}")
        print(f"ç¸½äº‹ä»¶æ•¸ï¼š{total_incidents}")
        
        if failed_files:
            print(f"\nâŒ å¤±æ•—çš„æª”æ¡ˆæ¸…å–®ï¼š")
            for filename in failed_files:
                print(f"   - {filename}")
        
        print(f"\nğŸ“ æ‰€æœ‰è¼¸å‡ºæª”æ¡ˆä½æ–¼ï¼š{output_dir}")
    
    def process_incident_data(self, incident_data: Dict[str, str]) -> Dict:
        """
        è™•ç†å·²è§£æçš„äº‹ä»¶è³‡æ–™ï¼ˆå¾ extract_sections ä¾†çš„ï¼‰
        
        Args:
            incident_data: å·²è§£æçš„äº‹ä»¶è³‡æ–™å­—å…¸
            
        Returns:
            å®Œæ•´çš„çµæ§‹åŒ– JSON è³‡æ–™
        """
        result = {}
        
        # ç›´æ¥ä½¿ç”¨å·²è§£æçš„åŸºæœ¬æ¬„ä½
        result['industry'] = incident_data.get('industry', '')
        result['incident'] = incident_data.get('incident', '')
        result['medium_type'] = incident_data.get('medium_type', '')
        result['description'] = incident_data.get('description', '')
        result['cause_analysis'] = incident_data.get('cause_analysis', '')
        result['preventive_measures'] = incident_data.get('preventive_measures', '')
        
        # ä½¿ç”¨ OpenAI åˆ†é¡ç½å®³é¡å‹
        if result['incident']:
            print("  æ­£åœ¨åˆ†é¡ç½å®³é¡å‹...")
            incident_type, incident_type_id = self.classify_incident_type(result['incident'])
            result['incident_type'] = incident_type
            result['incident_type_id'] = incident_type_id
        
        # ä½¿ç”¨ OpenAI åˆ†é¡åª’ä»‹ç‰©
        if result['medium_type']:
            print("  æ­£åœ¨åˆ†é¡åª’ä»‹ç‰©...")
            general, general_id, normal, normal_id, specific, specific_id = \
                self.classify_medium_type(result['medium_type'])
            result['medium_type_general'] = general
            result['medium_type_general_id'] = general_id
            result['medium_type_normal'] = normal
            result['medium_type_normal_id'] = normal_id
            result['medium_type_specific'] = specific
            result['medium_type_specific_id'] = specific_id
        else:
            # æ²’æœ‰åª’ä»‹ç‰©è³‡æ–™æ™‚ï¼Œè¨­å®šç©ºå­—ä¸²
            result['medium_type_general'] = ''
            result['medium_type_general_id'] = ''
            result['medium_type_normal'] = ''
            result['medium_type_normal_id'] = ''
            result['medium_type_specific'] = ''
            result['medium_type_specific_id'] = ''
        
        # ç”Ÿæˆæè¿°æ‘˜è¦
        if result['description']:
            print("  æ­£åœ¨ç”Ÿæˆç½å®³ç™¼ç”Ÿç¶“éæ‘˜è¦...")
            result['description_summary'] = self.generate_description_summary(result['description'])
        
        # ç”ŸæˆåŸå› æ‘˜è¦
        if result['cause_analysis']:
            print("  æ­£åœ¨ç”Ÿæˆç½å®³åŸå› æ‘˜è¦...")
            result['cause_summary'] = self.generate_cause_summary(result['cause_analysis'])
        
        # æå–æ³•è¦
        if result['preventive_measures']:
            print("  æ­£åœ¨æå–æ³•è¦æ¢æ–‡...")
            result['preventive_regulations'] = self.extract_regulations(result['preventive_measures'])
        
        return result

def main():
    """ä¸»ç¨‹å¼"""
    from pathlib import Path
    
    # ================== è¨­å®šå€ ==================
    # è¨­å®š OpenAI API å¯†é‘°
    API_KEY = "sk-YOUR_API_KEY"

    # é¸æ“‡è™•ç†æ¨¡å¼ï¼š'folder' æˆ– 'single'
    MODE = 'folder'
    # MODE = 'single' 

    # è³‡æ–™å¤¾æ¨¡å¼è¨­å®š
    FOLDER_PATH = "./osh_case_folder_1"
    OUTPUT_FOLDER = "./extraction_output"  # None è¡¨ç¤ºè¼¸å‡ºåˆ°èˆ‡ PDF ç›¸åŒä½ç½®ï¼Œä¹Ÿå¯æŒ‡å®šè·¯å¾‘
    
    # å–®ä¸€æª”æ¡ˆæ¨¡å¼è¨­å®š
    SINGLE_PDF_PATH = "./osh_case_folder_3/109å¹´å¾äº‹é›»æ¢¯ç³»çµ±æ›´æ–°ä½œæ¥­ç™¼ç”Ÿæ„Ÿé›»ç½å®³è‡´æ­»é‡å¤§è·æ¥­ç½å®³æ¡ˆä¾‹.pdf"
    # ============================================
    
    print("ğŸš€ è·æ¥­ç½å®³ PDF è³‡è¨ŠæŠ½å–ç³»çµ±")
    print("=" * 60)
    
    # å»ºç«‹è™•ç†å™¨
    processor = IncidentPDFProcessor(API_KEY)
    
    if MODE == 'folder':
        # è³‡æ–™å¤¾æ¨¡å¼
        print(f"ğŸ“‚ æ¨¡å¼ï¼šæ‰¹æ¬¡è™•ç†è³‡æ–™å¤¾")
        print(f"ğŸ“ è¼¸å…¥è³‡æ–™å¤¾ï¼š{FOLDER_PATH}")
        if OUTPUT_FOLDER:
            print(f"ğŸ“ è¼¸å‡ºè³‡æ–™å¤¾ï¼š{OUTPUT_FOLDER}")
        else:
            print(f"ğŸ“ è¼¸å‡ºè³‡æ–™å¤¾ï¼šèˆ‡ PDF æª”æ¡ˆç›¸åŒä½ç½®")
        print("=" * 60)
        
        processor.process_folder(FOLDER_PATH, OUTPUT_FOLDER)
    
    elif MODE == 'single':
        # å–®ä¸€æª”æ¡ˆæ¨¡å¼
        print(f"ğŸ“„ æ¨¡å¼ï¼šè™•ç†å–®ä¸€æª”æ¡ˆ")
        pdf_path = Path(SINGLE_PDF_PATH)
        output_path = pdf_path.parent / f"{pdf_path.stem}_çµæ§‹åŒ–è³‡æ–™.json"
        
        print(f"ğŸ“„ è¼¸å…¥æª”æ¡ˆï¼š{pdf_path.name}")
        print(f"ğŸ“„ è¼¸å‡ºæª”æ¡ˆï¼š{output_path.name}")
        print("=" * 60)
        
        results = processor.process_pdf(str(pdf_path), str(output_path))
        
        if results:
            print(f"\nâœ… è™•ç†å®Œæˆï¼å…±è™•ç† {len(results)} å€‹äº‹æ•…æ¡ˆä¾‹")
        else:
            print(f"\nâš ï¸  æœªèƒ½æå–ä»»ä½•äº‹ä»¶")
    
    else:
        print(f"âŒ éŒ¯èª¤ï¼šç„¡æ•ˆçš„æ¨¡å¼ '{MODE}'ï¼Œè«‹è¨­å®šç‚º 'folder' æˆ– 'single'")


if __name__ == "__main__":
    main()