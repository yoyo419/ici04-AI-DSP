{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Qwen + RAG Runner\n\nUses your existing `legal_content.json` (unchanged) as the retrieval corpus by reading each node's `node_id` and `rag_text`, retrieves top-k relevant passages, injects them into the prompt, and saves results to a CSV.\n\n**You only need to edit the PATHS cell.**"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "!pip -q install -U transformers accelerate bitsandbytes pandas tqdm"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# ===== PATHS (EDIT ME) =====\nEVAL_SET_PATH = \"/content/drive/MyDrive/your_project/data/eval_set.jsonl\"      # <- TODO\nLEGAL_CONTENT_JSON = \"/content/drive/MyDrive/your_project/legal_content.json\"  # <- TODO (your existing file)\nOUTPUT_DIR = \"/content/drive/MyDrive/your_project/outputs\"                    # <- TODO\n\nMODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n\n# Retrieval\nTOP_K = 5\nMAX_CONTEXT_CHARS = 6000  # keep prompt size manageable\n\n# Generation (keep temperature=0 for reproducibility)\nTEMPERATURE = 0.0\nTOP_P = 1.0\nMAX_NEW_TOKENS = 256"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import os, json, re, time, datetime\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def load_eval_set(jsonl_path: str):\n    items = []\n    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            items.append(json.loads(line))\n    for it in items:\n        assert \"qid\" in it and \"question\" in it\n    return items\n\ndef load_legal_docs(legal_json_path: str):\n    \"\"\"\n    Reads your existing legal_content.json as-is.\n    It must be a JSON list. Each node should include:\n      - node_id\n      - rag_text\n    Returns a list of {doc_id, text}.\n    \"\"\"\n    with open(legal_json_path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n    docs = []\n    for node in data:\n        doc_id = (node.get(\"node_id\") or \"\").strip()\n        text = (node.get(\"rag_text\") or \"\").strip()\n        if doc_id and text:\n            docs.append({\"doc_id\": doc_id, \"text\": text})\n    return docs\n\ndef score_overlap(query: str, text: str) -> int:\n    # Simple char overlap (language-agnostic, no extra packages)\n    q = set(query)\n    return sum(1 for ch in q if ch in text)\n\ndef retrieve(query: str, docs, top_k: int):\n    scored = []\n    for d in docs:\n        s = score_overlap(query, d[\"text\"])\n        if s > 0:\n            scored.append((s, d))\n    scored.sort(key=lambda x: x[0], reverse=True)\n    return [d for _, d in scored[:top_k]]\n\ndef format_mcq_prompt_with_context(item: dict, context: str) -> str:\n    q = item[\"question\"].strip()\n    choices = item.get(\"choices\", {})\n    choices_str = \"\\n\".join([f\"{k}. {v}\" for k,v in choices.items()])\n\n    return (\n        \"你是一位職安衛法規助理。請根據【參考資料】與題目選出最正確的選項，只輸出選項字母(A/B/C/D)。\\n\\n\"\n        f\"【參考資料】\\n{context}\\n\\n\"\n        f\"題目：{q}\\n\"\n        f\"{choices_str}\\n\\n\"\n        \"答案：\"\n    )\n\nANSWER_RE = re.compile(r\"\\b([ABCD])\\b\", re.IGNORECASE)\n\ndef parse_choice(text: str):\n    m = ANSWER_RE.search(text.strip())\n    return m.group(1).upper() if m else None"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# ===== Load model =====\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=\"auto\",\n    load_in_4bit=True,\n)\nmodel.eval()"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "eval_items = load_eval_set(EVAL_SET_PATH)\ndocs = load_legal_docs(LEGAL_CONTENT_JSON)\nprint(\"eval items:\", len(eval_items))\nprint(\"legal docs:\", len(docs))\nprint(\"example doc_id:\", docs[0][\"doc_id\"])"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def generate_one(prompt: str):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    with torch.no_grad():\n        out = model.generate(\n            **inputs,\n            max_new_tokens=MAX_NEW_TOKENS,\n            do_sample=(TEMPERATURE > 0),\n            temperature=TEMPERATURE if TEMPERATURE > 0 else None,\n            top_p=TOP_P,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n    text = tokenizer.decode(out[0], skip_special_tokens=True)\n    if \"答案：\" in text:\n        tail = text.split(\"答案：\", 1)[-1].strip()\n    else:\n        tail = text.strip()\n    return text, tail\n\nrun_ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nrun_id = f\"rag_{run_ts}\"\nos.makedirs(os.path.join(OUTPUT_DIR, \"rag\"), exist_ok=True)\n\nrows = []\nfor item in tqdm(eval_items, desc=\"RAG eval\"):\n    qid = item[\"qid\"]\n    gold = item.get(\"answer\")\n    question_text = item[\"question\"]\n\n    retrieved = retrieve(question_text, docs, top_k=TOP_K)\n    retrieved_ids = [d[\"doc_id\"] for d in retrieved]\n    context = \"\\n\\n\".join([d[\"text\"] for d in retrieved])\n\n    if len(context) > MAX_CONTEXT_CHARS:\n        context = context[:MAX_CONTEXT_CHARS] + \"\\n...(truncated)...\"\n\n    prompt = format_mcq_prompt_with_context(item, context)\n\n    t0 = time.time()\n    raw, tail = generate_one(prompt)\n    latency_ms = int((time.time() - t0) * 1000)\n\n    pred = parse_choice(tail)\n    correct = int(pred == gold) if (pred is not None and gold is not None) else None\n\n    rows.append({\n        \"run_id\": run_id,\n        \"method\": \"rag\",\n        \"model_name\": MODEL_NAME,\n        \"qid\": qid,\n        \"gold_choice\": gold,\n        \"parsed_choice\": pred,\n        \"correct\": correct,\n        \"raw_output\": raw,\n        \"latency_ms\": latency_ms,\n        \"temperature\": TEMPERATURE,\n        \"top_p\": TOP_P,\n        \"max_new_tokens\": MAX_NEW_TOKENS,\n        \"retrieved_k\": TOP_K,\n        \"retrieved_ids\": \";\".join(retrieved_ids) if retrieved_ids else None,\n        \"context_chars\": len(context),\n    })\n\ndf = pd.DataFrame(rows)\nout_path = os.path.join(OUTPUT_DIR, \"rag\", f\"{run_id}.csv\")\ndf.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\nprint(\"Saved:\", out_path)\nprint(\"Accuracy:\", df[\"correct\"].mean())"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}