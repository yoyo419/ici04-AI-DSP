{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Vanilla Qwen Runner (no RAG)\n\nRuns the fixed evaluation set (`eval_set.jsonl`) through a vanilla Qwen model and saves results to a CSV.\n\n**You only need to edit the PATHS cell.**"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# ===== Install (Colab) =====\n!pip -q install -U transformers accelerate bitsandbytes pandas tqdm"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# ===== PATHS (EDIT ME) =====\nEVAL_SET_PATH = \"/content/drive/MyDrive/your_project/data/eval_set.jsonl\"  # <- TODO\nOUTPUT_DIR = \"/content/drive/MyDrive/your_project/outputs\"                # <- TODO\n\n# Model (edit if you want)\nMODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n\n# Generation (keep temperature=0 for reproducibility)\nTEMPERATURE = 0.0\nTOP_P = 1.0\nMAX_NEW_TOKENS = 256"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import os, json, re, time, datetime\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def load_eval_set(jsonl_path: str):\n    items = []\n    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            items.append(json.loads(line))\n    # Basic validation\n    for it in items:\n        assert \"qid\" in it and \"question\" in it, \"Each line must include qid and question\"\n        if \"choices\" in it:\n            assert isinstance(it[\"choices\"], dict), \"choices must be a dict like {A:...,B:...,C:...,D:...}\"\n    return items\n\ndef format_mcq_prompt(item: dict) -> str:\n    q = item[\"question\"].strip()\n    choices = item.get(\"choices\", {})\n    if choices:\n        choices_str = \"\\n\".join([f\"{k}. {v}\" for k,v in choices.items()])\n        return (\n            \"你是一位職安衛法規助理。請根據題目選出最正確的選項，只輸出選項字母(A/B/C/D)。\\n\\n\"\n            f\"題目：{q}\\n\"\n            f\"{choices_str}\\n\\n\"\n            \"答案：\"\n        )\n    else:\n        # non-MCQ (optional)\n        return f\"請回答：{q}\"\n\nANSWER_RE = re.compile(r\"\\b([ABCD])\\b\", re.IGNORECASE)\n\ndef parse_choice(text: str):\n    # Robust: find first A/B/C/D\n    m = ANSWER_RE.search(text.strip())\n    if not m:\n        return None\n    return m.group(1).upper()"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# ===== Load model =====\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=\"auto\",\n    load_in_4bit=True,   # works on Colab T4/L4/A100; if error, set to False and remove bitsandbytes\n)\nmodel.eval()"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "eval_items = load_eval_set(EVAL_SET_PATH)\nlen(eval_items), eval_items[0].keys()"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def generate_one(prompt: str):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    with torch.no_grad():\n        out = model.generate(\n            **inputs,\n            max_new_tokens=MAX_NEW_TOKENS,\n            do_sample=(TEMPERATURE > 0),\n            temperature=TEMPERATURE if TEMPERATURE > 0 else None,\n            top_p=TOP_P,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n    text = tokenizer.decode(out[0], skip_special_tokens=True)\n    # Keep only the tail after \"答案：\" if present\n    if \"答案：\" in text:\n        text_tail = text.split(\"答案：\", 1)[-1].strip()\n    else:\n        text_tail = text.strip()\n    return text, text_tail\n\nrun_ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nrun_id = f\"vanilla_{run_ts}\"\nos.makedirs(os.path.join(OUTPUT_DIR, \"vanilla\"), exist_ok=True)\n\nrows = []\nfor item in tqdm(eval_items, desc=\"Vanilla eval\"):\n    qid = item[\"qid\"]\n    gold = item.get(\"answer\")\n    prompt = format_mcq_prompt(item)\n    t0 = time.time()\n    raw, tail = generate_one(prompt)\n    latency_ms = int((time.time() - t0) * 1000)\n    pred = parse_choice(tail)\n    correct = int(pred == gold) if (pred is not None and gold is not None) else None\n    rows.append({\n        \"run_id\": run_id,\n        \"method\": \"vanilla\",\n        \"model_name\": MODEL_NAME,\n        \"qid\": qid,\n        \"gold_choice\": gold,\n        \"parsed_choice\": pred,\n        \"correct\": correct,\n        \"raw_output\": raw,\n        \"latency_ms\": latency_ms,\n        # keep generation params for reproducibility\n        \"temperature\": TEMPERATURE,\n        \"top_p\": TOP_P,\n        \"max_new_tokens\": MAX_NEW_TOKENS,\n        # rag fields kept for schema alignment\n        \"retrieved_k\": None,\n        \"retrieved_ids\": None,\n        \"context_chars\": None,\n    })\n\ndf = pd.DataFrame(rows)\nout_path = os.path.join(OUTPUT_DIR, \"vanilla\", f\"{run_id}.csv\")\ndf.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\nout_path, df[\"correct\"].mean()"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}